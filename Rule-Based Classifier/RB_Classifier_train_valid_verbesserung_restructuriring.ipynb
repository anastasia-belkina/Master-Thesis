{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee484548-0f4d-4182-a70f-aef0f0cd5b96",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d02f264-f222-4ffb-be3b-f0097f988b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import ast\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b106f5-091e-4100-af31-f7677c66f1de",
   "metadata": {},
   "source": [
    "# Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1049d0-bd96-4662-988d-0db76fe1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "#column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"senses\", \"dependencies\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"]\n",
    "column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"dependencies\"]\n",
    "\n",
    "#df_train_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "#df_valid_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "df_train_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/datasets_preprocessed/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_valid_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/datasets_preprocessed/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "# Remove leading and trailing spaces in the \"Sentence\" column\n",
    "df_train_preprocessed['Sentence'] = df_train_preprocessed['Sentence'].str.strip()\n",
    "df_valid_preprocessed['Sentence'] = df_valid_preprocessed['Sentence'].str.strip()\n",
    "\n",
    "# Delete columns \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"\n",
    "#df_train_ready = df_train_preprocessed.drop(columns = [\"senses\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "#df_valid_ready = df_valid_preprocessed.drop(columns = [\"senses\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "\n",
    "df_train_ready = df_train_preprocessed\n",
    "df_valid_ready = df_valid_preprocessed\n",
    "\n",
    "# Shuffle the data\n",
    "#df_train_ready = df_train_preprocessed.sample(frac=1).reset_index(drop=True)\n",
    "#df_valid_ready = df_valid_preprocessed.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32502fac-70ca-4c2c-83be-a0f206d6b765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a. m. Initial eyewitness accounts of such inci...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('a.', 'X'), ('m.', 'NOUN'), ('Initial', 'ADJ...</td>\n",
       "      <td>[('British', 'NORP'), ('Cox’s', 'PERSON')]</td>\n",
       "      <td>[('a.', 10, 'dep'), ('m.', 10, 'nsubj'), ('Ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shortly after the beginning of the attack, the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Shortly', 'ADV'), ('after', 'ADP'), ('the',...</td>\n",
       "      <td>[('Talibans', 'NORP'), ('Zabihullah Mujahid', ...</td>\n",
       "      <td>[('Shortly', 4, 'advmod'), ('after', 4, 'case'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Judge Pryor initially supported Judge Moore bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('Judge', 'NOUN'), ('Pryor', 'PROPN'), ('init...</td>\n",
       "      <td>[('Pryor', 'PERSON'), ('Moore', 'PERSON')]</td>\n",
       "      <td>[('Judge', 4, 'nsubj'), ('Pryor', 1, 'flat'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump also expects to receive a major new fina...</td>\n",
       "      <td>3</td>\n",
       "      <td>[('Trump', 'PROPN'), ('also', 'ADV'), ('expect...</td>\n",
       "      <td>[('Trump', 'PERSON'), ('the United States', 'G...</td>\n",
       "      <td>[('Trump', 3, 'nsubj'), ('also', 3, 'advmod'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just decentralisation.Mr Purcell praised the C...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('just', 'ADV'), ('decentralisation', 'NOUN')...</td>\n",
       "      <td>[('Purcell', 'PERSON'), ('Coalition', 'ORG')]</td>\n",
       "      <td>[('just', 2, 'advmod'), ('decentralisation', 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>The White House has confirmed that the U.S. wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('The', 'DET'), ('White', 'ADJ'), ('House', '...</td>\n",
       "      <td>[('The White House', 'ORG'), ('U.S.', 'GPE'), ...</td>\n",
       "      <td>[('The', 3, 'det'), ('White', 3, 'amod'), ('Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>If deterrence were to fail, and an invasion we...</td>\n",
       "      <td>3</td>\n",
       "      <td>[('If', 'SCONJ'), ('deterrence', 'NOUN'), ('we...</td>\n",
       "      <td>[('North Korea', 'GPE'), ('U.S.', 'GPE'), ('So...</td>\n",
       "      <td>[('If', 5, 'mark'), ('deterrence', 5, 'nsubj')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>Watson strove to hit her way out of the doldru...</td>\n",
       "      <td>4</td>\n",
       "      <td>[('Watson', 'PROPN'), ('strove', 'VERB'), ('to...</td>\n",
       "      <td>[('Watson', 'PERSON'), ('Cadantu', 'PERSON'), ...</td>\n",
       "      <td>[('Watson', 2, 'nsubj'), ('strove', 0, 'root')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>Sergi Roberto attempted a repeat of his heroic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('Sergi', 'PROPN'), ('Roberto', 'PROPN'), ('a...</td>\n",
       "      <td>[('Sergi Roberto', 'PERSON'), ('PSG', 'ORG'), ...</td>\n",
       "      <td>[('Sergi', 3, 'nsubj'), ('Roberto', 1, 'flat')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>Where Trump stands Support for Israel is an ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Where', 'ADV'), ('Trump', 'PROPN'), ('stand...</td>\n",
       "      <td>[('Trump', 'PERSON'), ('Israel', 'GPE'), ('Rep...</td>\n",
       "      <td>[('Where', 3, 'advmod'), ('Trump', 3, 'nsubj')...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5032 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  Label  \\\n",
       "0     a. m. Initial eyewitness accounts of such inci...      0   \n",
       "1     Shortly after the beginning of the attack, the...      1   \n",
       "2     Judge Pryor initially supported Judge Moore bu...      0   \n",
       "3     Trump also expects to receive a major new fina...      3   \n",
       "4     just decentralisation.Mr Purcell praised the C...      1   \n",
       "...                                                 ...    ...   \n",
       "5027  The White House has confirmed that the U.S. wi...      1   \n",
       "5028  If deterrence were to fail, and an invasion we...      3   \n",
       "5029  Watson strove to hit her way out of the doldru...      4   \n",
       "5030  Sergi Roberto attempted a repeat of his heroic...      0   \n",
       "5031  Where Trump stands Support for Israel is an ea...      1   \n",
       "\n",
       "                                             tokens_pos  \\\n",
       "0     [('a.', 'X'), ('m.', 'NOUN'), ('Initial', 'ADJ...   \n",
       "1     [('Shortly', 'ADV'), ('after', 'ADP'), ('the',...   \n",
       "2     [('Judge', 'NOUN'), ('Pryor', 'PROPN'), ('init...   \n",
       "3     [('Trump', 'PROPN'), ('also', 'ADV'), ('expect...   \n",
       "4     [('just', 'ADV'), ('decentralisation', 'NOUN')...   \n",
       "...                                                 ...   \n",
       "5027  [('The', 'DET'), ('White', 'ADJ'), ('House', '...   \n",
       "5028  [('If', 'SCONJ'), ('deterrence', 'NOUN'), ('we...   \n",
       "5029  [('Watson', 'PROPN'), ('strove', 'VERB'), ('to...   \n",
       "5030  [('Sergi', 'PROPN'), ('Roberto', 'PROPN'), ('a...   \n",
       "5031  [('Where', 'ADV'), ('Trump', 'PROPN'), ('stand...   \n",
       "\n",
       "                                               entities  \\\n",
       "0            [('British', 'NORP'), ('Cox’s', 'PERSON')]   \n",
       "1     [('Talibans', 'NORP'), ('Zabihullah Mujahid', ...   \n",
       "2            [('Pryor', 'PERSON'), ('Moore', 'PERSON')]   \n",
       "3     [('Trump', 'PERSON'), ('the United States', 'G...   \n",
       "4         [('Purcell', 'PERSON'), ('Coalition', 'ORG')]   \n",
       "...                                                 ...   \n",
       "5027  [('The White House', 'ORG'), ('U.S.', 'GPE'), ...   \n",
       "5028  [('North Korea', 'GPE'), ('U.S.', 'GPE'), ('So...   \n",
       "5029  [('Watson', 'PERSON'), ('Cadantu', 'PERSON'), ...   \n",
       "5030  [('Sergi Roberto', 'PERSON'), ('PSG', 'ORG'), ...   \n",
       "5031  [('Trump', 'PERSON'), ('Israel', 'GPE'), ('Rep...   \n",
       "\n",
       "                                           dependencies  \n",
       "0     [('a.', 10, 'dep'), ('m.', 10, 'nsubj'), ('Ini...  \n",
       "1     [('Shortly', 4, 'advmod'), ('after', 4, 'case'...  \n",
       "2     [('Judge', 4, 'nsubj'), ('Pryor', 1, 'flat'), ...  \n",
       "3     [('Trump', 3, 'nsubj'), ('also', 3, 'advmod'),...  \n",
       "4     [('just', 2, 'advmod'), ('decentralisation', 0...  \n",
       "...                                                 ...  \n",
       "5027  [('The', 3, 'det'), ('White', 3, 'amod'), ('Ho...  \n",
       "5028  [('If', 5, 'mark'), ('deterrence', 5, 'nsubj')...  \n",
       "5029  [('Watson', 2, 'nsubj'), ('strove', 0, 'root')...  \n",
       "5030  [('Sergi', 3, 'nsubj'), ('Roberto', 1, 'flat')...  \n",
       "5031  [('Where', 3, 'advmod'), ('Trump', 3, 'nsubj')...  \n",
       "\n",
       "[5032 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade7978-08a0-4c84-be4c-be6ed20f1d04",
   "metadata": {},
   "source": [
    "# Merging Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aaba528-c606-4797-aeee-52437715a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {2: 1, 3: 2, 4: 2}\n",
    "\n",
    "# 0 - neutral, 1 - positive, 2 - negative\n",
    "\n",
    "df_train_ready_merged = df_train_ready\n",
    "df_valid_ready_merged = df_valid_ready\n",
    "\n",
    "# Apply the mapping to the 'Label' column\n",
    "df_train_ready_merged['Label'] = df_train_ready_merged['Label'].replace(label_mapping)\n",
    "df_valid_ready_merged['Label'] = df_valid_ready_merged['Label'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a1cf9-5a57-493a-8c3a-7802f92ffba6",
   "metadata": {},
   "source": [
    "# Turning strings back to lists and tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e79989-3230-4c6b-ba9c-17d47543c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(dependencies_str):\n",
    "    # Check if it's a string and if it appears to be in the list of tuples format\n",
    "    if isinstance(dependencies_str, str) and dependencies_str.startswith(\"[\") and dependencies_str.endswith(\"]\"):\n",
    "        try:\n",
    "            # Convert string representation of list back to actual list of tuples\n",
    "            return ast.literal_eval(dependencies_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error parsing: {dependencies_str}\")\n",
    "            raise e\n",
    "    elif isinstance(dependencies_str, list):\n",
    "        # If it's already a list, return as is\n",
    "        return dependencies_str\n",
    "    else:\n",
    "        # If it's another unexpected type, return as is or handle appropriately\n",
    "        return dependencies_str\n",
    "\n",
    "def get_sense(tokens):\n",
    "    #print(tokens)\n",
    "    senses = []\n",
    "    for item in tokens:\n",
    "        #print(item)\n",
    "        if isinstance(item, tuple) and len(item) == 2:\n",
    "            token, pos = item\n",
    "            sense = lesk([token], token)\n",
    "            senses.append((token, sense))\n",
    "        else:\n",
    "            # Handle cases where the token doesn't meet the expected structure\n",
    "            print(f\"Unexpected format: {item}\")\n",
    "            senses.append((item, None))\n",
    "    return senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6e53857-0db5-4237-b8c7-e742c1fd955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['dependencies'] = df_train_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_valid_ready_merged['dependencies'] = df_valid_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_train_ready_merged['tokens_pos'] = df_train_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_valid_ready_merged['tokens_pos'] = df_valid_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_train_ready_merged['entities'] = df_train_ready_merged['entities'].apply(convert_to_list)\n",
    "df_valid_ready_merged['entities'] = df_valid_ready_merged['entities'].apply(convert_to_list)\n",
    "#df_train_ready_merged['senses'] = df_train_ready_merged['tokens_pos'].apply(get_sense)\n",
    "#df_valid_ready_merged['senses'] = df_valid_ready_merged['tokens_pos'].apply(get_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79357fd-7599-4283-9460-848a8ab4880b",
   "metadata": {},
   "source": [
    "# Making a small set for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610909d0-e497-46f1-b4ed-8a69c46b5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ready_merged_small = df_train_ready_merged.head(10)\n",
    "df_valid_ready_merged_small = df_valid_ready_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5898c-ac22-4229-9a87-3a89b6c3912e",
   "metadata": {},
   "source": [
    "# Following the Modified Algorithm of Blame/Praise Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291d5c-c2b0-4bba-ab48-4fc2faf21902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Older versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b404f2-ab1c-4e08-ae6f-bb832282354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Step 2: Process root verbs\\n    for i, root in enumerate(roots):\\n        root_verb, root_index = root\\n        #print(root_verb, root_index)\\n        # Check if the root verb is valid (foreseeability and not coercion)\\n        for token, pos in tokens_pos:\\n            #print(token, pos)\\n            if token == root_verb and pos == \\'VERB\\':\\n                #print(\"Found VERB\")\\n                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\\n                    #print(\"F and not C\") \\n                    root_verbs.append((root_verb, i+1))\\n        # Check for any conj attached to this root verb and add it if valid\\n        for j, conj in enumerate(dependencies):\\n            conj_word, conj_head, conj_rel = conj\\n            if conj_head == root_index and conj_rel == \\'conj\\':\\n                for token_conj, pos_conj in tokens_pos:\\n                    if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                            root_verbs.append((conj_word, j+1))\\n    \\n    #print(root_verbs)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    # Step 2: Process root verbs\n",
    "    for i, root in enumerate(roots):\n",
    "        root_verb, root_index = root\n",
    "        #print(root_verb, root_index)\n",
    "        # Check if the root verb is valid (foreseeability and not coercion)\n",
    "        for token, pos in tokens_pos:\n",
    "            #print(token, pos)\n",
    "            if token == root_verb and pos == 'VERB':\n",
    "                #print(\"Found VERB\")\n",
    "                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\n",
    "                    #print(\"F and not C\") \n",
    "                    root_verbs.append((root_verb, i+1))\n",
    "        # Check for any conj attached to this root verb and add it if valid\n",
    "        for j, conj in enumerate(dependencies):\n",
    "            conj_word, conj_head, conj_rel = conj\n",
    "            if conj_head == root_index and conj_rel == 'conj':\n",
    "                for token_conj, pos_conj in tokens_pos:\n",
    "                    if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                            root_verbs.append((conj_word, j+1))\n",
    "    \n",
    "    #print(root_verbs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0d40cf-2090-4b6a-bcaf-7278232587ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    \\n    # Lists to store categorized verbs\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for i, dep in enumerate(dependencies):\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, i+1))\\n                # Check for any conj attached to this root verb and add it if valid\\n                for j, conj in enumerate(dependencies):\\n                    conj_word, conj_head, conj_rel = conj\\n                    if conj_head == i+1 and conj_rel == \\'conj\\':\\n                        for token_conj, pos_conj in tokens_pos:\\n                            if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                    root_verbs.append((conj_word, j+1))\\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, i+1))\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for j, conj in enumerate(dependencies):\n",
    "                    conj_word, conj_head, conj_rel = conj\n",
    "                    if conj_head == i+1 and conj_rel == 'conj':\n",
    "                        for token_conj, pos_conj in tokens_pos:\n",
    "                            if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                    root_verbs.append((conj_word, j+1))\n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b412f5aa-2f1f-4e37-ad7d-0e12e665ebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# TRYING WITH COUNTER\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                #print(\"Update Counter I\")\\n                counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any conj attached to this root verb and add it if valid\\n                for conj in dependencies:\\n                    if len(conj) == 3:\\n                        conj_word, conj_head, conj_rel = conj\\n                        counter_j = counter_j +1\\n                        #print(conj_word, counter_j)\\n                        if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if conj_head == counter_i and conj_rel == \\'conj\\':\\n                            for token_conj, pos_conj in tokens_pos:\\n                                if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                        root_verbs.append((conj_word, counter_j))\\n                    \\n                    \\n        \\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(roots)\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TRYING WITH COUNTER\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                #print(\"Update Counter I\")\n",
    "                counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for conj in dependencies:\n",
    "                    if len(conj) == 3:\n",
    "                        conj_word, conj_head, conj_rel = conj\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(conj_word, counter_j)\n",
    "                        if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if conj_head == counter_i and conj_rel == 'conj':\n",
    "                            for token_conj, pos_conj in tokens_pos:\n",
    "                                if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                        root_verbs.append((conj_word, counter_j))\n",
    "                    \n",
    "                    \n",
    "        \n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(roots)\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc6632ed-0032-44b0-9830-875c49495651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# TRYING WITH all tags in one for cycle\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n       \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    counter_x = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            \\n            if roots:\\n                if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                    #print(\"Update Counter I\")\\n                    counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any relater words attached to this root verb and add it if valid\\n                for related in dependencies:\\n                    if len(related) == 3:\\n                        related_word, related_head, related_rel = related\\n                        counter_j = counter_j +1\\n                        #print(related_word, counter_j)\\n                        if related_rel == \\'punct\\' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if related_head == counter_i and related_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\', \\'conj\\']:\\n                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\\n                            for token_related, pos_related in tokens_pos:\\n                                if token_related == related_word and \\'VERB\\' in pos_related:\\n                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\\n                                        \\n                                        if related_rel == \\'conj\\':\\n                                            root_verbs.append((related_word, counter_j))\\n                                            \\n                                        elif related_rel == \\'xcomp\\':\\n                                            xcomp_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    xcomp_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'ccomp\\':\\n                                            ccomp_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    ccomp_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'parataxis\\':\\n                                            parataxis_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    parataxis_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'advcl\\':\\n                                            advcl_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    advcl_verbs.append((conj_word, counter_x))\\n    \\n\\n    #print(\"NEW ROW\")\\n    \\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        #print(roots, \" - roots\")\\n        #print(root_verbs, \" - root_verbs\")\\n        #print(xcomp_verbs, \" - xcomp_verbs\")\\n        #print(ccomp_verbs, \" - ccomp_verbs\")\\n        #print(parataxis_verbs, \" - parataxis_verbs\")\\n        #print(advcl_verbs, \" - advcl_verbs\")\\n        #print()\\n        return \\'related\\'\\n    else:\\n        return 0\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# TRYING WITH all tags in one for cycle\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "       \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    counter_x = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            \n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    #print(\"Update Counter I\")\n",
    "                    counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any relater words attached to this root verb and add it if valid\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(related_word, counter_j)\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if related_head == counter_i and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl', 'conj']:\n",
    "                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\n",
    "                            for token_related, pos_related in tokens_pos:\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\n",
    "                                        \n",
    "                                        if related_rel == 'conj':\n",
    "                                            root_verbs.append((related_word, counter_j))\n",
    "                                            \n",
    "                                        elif related_rel == 'xcomp':\n",
    "                                            xcomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    xcomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'ccomp':\n",
    "                                            ccomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    ccomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'parataxis':\n",
    "                                            parataxis_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    parataxis_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'advcl':\n",
    "                                            advcl_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    advcl_verbs.append((conj_word, counter_x))\n",
    "    \n",
    "\n",
    "    #print(\"NEW ROW\")\n",
    "    \n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb86b91-082e-4234-bb2d-3b244ede8e74",
   "metadata": {},
   "source": [
    "## Current Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850365df-dec0-4b8f-a92e-6631d0347707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to check if a verb belongs to Foreseeability or Coercion groups\n",
    "\n",
    "def is_foreseeability_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of foreseeability-related verb classes.\n",
    "    foreseeability_classes = {'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lexname = synset.lexname().split('.')[1]  # Extracts the lexical category (i.e., type of action)\n",
    "        if lexname in foreseeability_classes:  # Checks if the lexical category is in the foreseeability class\n",
    "            return True  # Returns True if the verb matches any foreseeability category\n",
    "    return False  # If no match is found, returns False\n",
    "\n",
    "\n",
    "def is_coercion_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of coercion-related VerbNet classes.\n",
    "    coercion_classes = {'urge-58.1', 'force-59', 'forbid-67'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lemma = synset.lemmas()[0]  # Gets the first lemma for each synset\n",
    "        vn_classes = lemma.key().split('%')[0]  # Extracts the lemma key\n",
    "        vn_class_ids = vn.classids(vn_classes)  # Fetches the VerbNet classes for the lemma\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):  # Checks for a match in coercion classes\n",
    "            return True  # If a match is found in coercion classes, return True\n",
    "    return False  # If no match is found, return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ea980d-d800-4637-a05c-79f56cda0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_one_function(row):\n",
    "    # This is the function to find valid verbs (root, xcomp, ccomp, parataxis, advcl and their conjunctions)\n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence\n",
    "    \n",
    "    counter_i = 0  # Counter for tracking the index of words in the dependency structure\n",
    "    counter_j = 0  # Counter for tracking the index during nested loops\n",
    "    counter_x = 0  # Counter used to track conj words\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "\n",
    "    # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    roots = []  # For root verbs\n",
    "    root_verbs = []  # For valid root verbs (that pass foreseeability and coercion checks)\n",
    "\n",
    "    # (word, own index, head index)\n",
    "    xcomp_verbs = []  # For xcomp verbs\n",
    "    ccomp_verbs = []  # For ccomp verbs\n",
    "    parataxis_verbs = []  # For parataxis verbs\n",
    "    advcl_verbs = []  # For advcl verbs\n",
    "    \n",
    "    # Iterate through dependencies to identify roots and their related verbs\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep  # Unpacking the dependency tuple (word, head, relation)\n",
    "            counter_i += 1  # Increment the index counter for this word\n",
    "            \n",
    "            # If an end of the sentence has been found, reset the counter for punctuation\n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    counter_i = 0  # Reset counter when punctuation is found after root\n",
    "                \n",
    "            # Check if the current word is the root of the sentence\n",
    "            if deprel == 'root':\n",
    "                roots.append((word, counter_i, counter_i))  # Add the root verb and its index\n",
    "                for token, pos in tokens_pos:  # Iterate through POS tokens to find the root as a verb\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            root_verbs.append((word, counter_i, counter_i))  # Add root verb if it passes foreseeability and coercion checks\n",
    "                # looking for related conj\n",
    "                counter_j = 0\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                        counter_j += 1  # Increment the index for the related word\n",
    "                        # Reset the counter for punctuation after root - end of the sentence\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            counter_j = 0\n",
    "                        # Look for conj attached to the verb\n",
    "                        if related_head == counter_i and related_rel in ['conj']:\n",
    "                            roots.append((related_word, counter_j, counter_i))  # Add the root verb and its index\n",
    "                            for token_related, pos_related in tokens_pos:  # Find if the related word is a verb\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        root_verbs.append((related_word, counter_j, counter_i))  # Conj relation to root\n",
    "\n",
    "\n",
    "    \n",
    "    # Find related verbs (xcomp, ccomp, etc.) for root verbs and their conj\n",
    "\n",
    "    for verb in roots:\n",
    "        word, index, head_index = verb\n",
    "        #counter_i += 1\n",
    "\n",
    "        counter_j = 0\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                counter_j += 1  # Increment the index for the related word\n",
    "                \n",
    "                # Reset the counter for punctuation after root - end of the sentence\n",
    "                if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                    counter_j = 0\n",
    "                \n",
    "                # Look for xcomp, ccomp, parataxis or advcl relations attached to the root and its conj\n",
    "                if related_head == index and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found some related word: \", related_rel)\n",
    "                    for token_related, pos_related in tokens_pos:  # Find if the related word is a verb\n",
    "                        if token_related == related_word and 'VERB' in pos_related:\n",
    "                            if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                #print(\"related word passed all checks: \", related_rel)\n",
    "                                # Depending on the relation type, add the related verb to the appropriate list\n",
    "                                #if related_rel == 'conj':\n",
    "                                   # root_verbs.append((related_word, counter_j))  # Conj relation to root\n",
    "                                if related_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append((related_word, counter_j, index))  # xcomp relation to root\n",
    "                                    # Handle conj for xcomp verbs\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                counter_x = 0  # Reset counter for punctuation\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                # Check if the conj word is a valid verb\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            xcomp_verbs.append((conj_word, counter_x, index))  # Conj for xcomp\n",
    "                                \n",
    "                                # Handle ccomp, parataxis, advcl similarly for related verbs and their conjunctions\n",
    "                                elif related_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append((related_word, counter_j, index))  # ccomp relation\n",
    "                                    # Handle conj for ccomp\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            ccomp_verbs.append((conj_word, counter_x, index))  # Conj for ccomp\n",
    "                                \n",
    "                                elif related_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append((related_word, counter_j, index))  # parataxis relation\n",
    "                                    # Handle conj for parataxis\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            parataxis_verbs.append((conj_word, counter_x, index))  # Conj for parataxis\n",
    "                                \n",
    "                                elif related_rel == 'advcl':\n",
    "                                    advcl_verbs.append((related_word, counter_j, index))  # advcl relation\n",
    "                                    # Handle conj for advcl\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            advcl_verbs.append((conj_word, counter_x, index))  # Conj for advcl\n",
    "    #print(\"NEW ROW\")\n",
    "    \n",
    "    # Return the lists of related verbs\n",
    "    #if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "    return roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30dbe389-4669-4a18-9135-f9128335f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_agent_validity(related_word, row, tokens_pos):\n",
    "    \n",
    "    entities = row['entities']\n",
    "    valid_ent_labels = [\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]\n",
    "    valid_additional_words = [\"person\", \"man\", \"woman\", \"police\", \"administration\", \"immigrants\", \"president\", \"minister\", \"senator\", \n",
    "                              \"representative\", \"governor\", \"mayor\", \"council\", \"secretary\", \"ambassador\", \"chancellor\", \"parliamentary\", \"mr.\", \"ms.\", \"mrs.\"]\n",
    "\n",
    "    self = False\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    for entity, label in entities: \n",
    "        if entity in related_word and label in valid_ent_labels: \n",
    "            agent_is_valid = True # is it in NER list?  \n",
    "    if not agent_is_valid and 'PRON' in [pos for token, pos in tokens_pos if token == related_word]: \n",
    "        agent_is_valid = True # is it a promoun?\n",
    "        #if related_word.lower() == \"i\" or related_word.lower() == \"we\":\n",
    "            #self = True\n",
    "    if not agent_is_valid and related_word.lower() in valid_additional_words: agent_is_valid = True # is it from the list of additional words?\n",
    "\n",
    "    return agent_is_valid, self\n",
    "\n",
    "\n",
    "\n",
    "def check_causative_verb(verb):\n",
    "    # Check if the verb is in the CAUSE class or has CAUSETO relation in WordNet\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def define_polarity(verb, obj):\n",
    "    # Function to define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "    # 0 - others, 1 - positive, 2 - negative\n",
    "\n",
    "    result = 0\n",
    "    \n",
    "    # Create a simple context for WSD\n",
    "    context = f\"{verb} {obj}\"\n",
    "    \n",
    "    # Word Sense Disambiguation for the verb and object\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    # Calculate polarity using SentiWordNet\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    # AFINN score\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    # Subjectivity Lexicon score\n",
    "    tokens = context.split()\n",
    "    subj_pos = sum([1 for token in tokens if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in tokens if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    # Determine final polarity\n",
    "    if pos_score > neg_score:\n",
    "        return 1  # Positive/Praise\n",
    "    elif neg_score > pos_score:\n",
    "        return 2  # Negative/Blame\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "        \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def adjust_sentiment_for_negation(row, polarity, verb):\n",
    "    word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    dependencies = row['dependencies']\n",
    "\n",
    "    for related in dependencies:\n",
    "        if len(related) == 3:\n",
    "            related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "            if related_head == index and related_rel in ['advmod'] and (related_word == 'not' or related_word == 'n’t'):\n",
    "                if polarity == 0:\n",
    "                    return 0\n",
    "                if polarity == 1:\n",
    "                    return 2\n",
    "                if polarity == 2:\n",
    "                    return 1\n",
    "    \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f01c3852-ef74-496c-aeb5-95f01c929ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs):\n",
    "    \n",
    "     # This is the function to decide on Agent Causality, find the object, decide on Polarity and classify the row \n",
    "    \n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence  \n",
    "\n",
    "    self = False\n",
    "    result = None\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    # Сonnection 1: nsubj / nsubj:pass - root_verb - obj / iobj / obl - by priority\n",
    "    for verb in root_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "        \n",
    "\n",
    "    \n",
    "    # Сonnection 2: nsubj / nsubj:pass - root_verb - xcomp - obj / iobj / obl - by priority\n",
    "    for verb in xcomp_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "\n",
    "    # Сonnection 3: nsubj / nsubj:pass - parataxis_verbs - (xcomp) - obj / iobj / obl - by priority    \n",
    "    for verb in parataxis_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with parataxis\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "\n",
    "\n",
    "\n",
    "    # Сonnection 4: nsubj / nsubj:pass - advcl_verbs - (xcomp) - obj / iobj / obl - by priority   \n",
    "    for verb in advcl_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with advcl\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "\n",
    "    \n",
    "    \n",
    "    # Checking for other possible connections\n",
    "    # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    # (word, own index, head index)\n",
    "    \n",
    "    # АГЕНТ МБ ПРИСОЕДИНЕН К ROOT, А ОБЪЕКТ К ВСПОМОГАТЕЛЬНОМУ ТЕГУ\n",
    "    # ИСКАТЬ СУБЪЕКТ И К СЕБЕ И К ROOT ПРИСОЕДИНЕННОМУ\n",
    "    # ОБЪЕКТ ИЩЕМ ПРИСОЕДИНЕННЫЙ К СЕБЕ\n",
    "\n",
    "    # Сonnection 5: nsubj - ccomp_verbs - (xcomp) - obj / iobj / obl - by priority   \n",
    "    for verb in ccomp_verbs: \n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                #else:\n",
    "                    #if related_head == head_index and related_rel in ['nsubj']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        #agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with advcl\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # Сonnection 6: nsubj:pass - ccomp_verbs - (xcomp) - obl:agent / obl - by priority   \n",
    "    for verb in ccomp_verbs: \n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['obl:agent', 'obl']: # is that an agent?\n",
    "                    #print(\"Marvel with ccomp passive happened\")\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                #else:\n",
    "                   #if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                       #agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            #print(\"Even Agent is valid\")\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                        \n",
    "                    if related_head == index and related_rel in ['nsubj:pass']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33e4e0cf-d699-42a2-968b-063783d3a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_verbs(row):\n",
    "    \n",
    "    # This is the main function to process each row of data and classify the row \n",
    "\n",
    "    # 1 - Find all the related verbs in categories in dependency column 'root', 'xcomp', 'ccomp', 'parataxis', 'advcl', 'conj' (is a verb check - foreseeability check - coercion check)\n",
    "    roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs = step_one_function(row)\n",
    "    \n",
    "    # 2 - If at least one of the lists is not empty - can proceed\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        \n",
    "        # 3 - Take a final decision about the label (0 - others, 1 - positive, 2 - negative)\n",
    "        return step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs)\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "314989cf-1c6a-4635-b61b-e4ec36054046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_14200\\4059228178.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_14200\\4059228178.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the small dataset\n",
    "df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c233a97e-f4ff-434f-8f97-5764a84500c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a. m. Initial eyewitness accounts of such inci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(a., X), (m., NOUN), (Initial, ADJ), (eyewitn...</td>\n",
       "      <td>[(British, NORP), (Cox’s, PERSON)]</td>\n",
       "      <td>[(a., 10, dep), (m., 10, nsubj), (Initial, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shortly after the beginning of the attack, the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Shortly, ADV), (after, ADP), (the, DET), (be...</td>\n",
       "      <td>[(Talibans, NORP), (Zabihullah Mujahid, PERSON)]</td>\n",
       "      <td>[(Shortly, 4, advmod), (after, 4, case), (the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Judge Pryor initially supported Judge Moore bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Judge, NOUN), (Pryor, PROPN), (initially, AD...</td>\n",
       "      <td>[(Pryor, PERSON), (Moore, PERSON)]</td>\n",
       "      <td>[(Judge, 4, nsubj), (Pryor, 1, flat), (initial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump also expects to receive a major new fina...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Trump, PROPN), (also, ADV), (expects, VERB),...</td>\n",
       "      <td>[(Trump, PERSON), (the United States, GPE), (t...</td>\n",
       "      <td>[(Trump, 3, nsubj), (also, 3, advmod), (expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just decentralisation.Mr Purcell praised the C...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(just, ADV), (decentralisation, NOUN), (., PU...</td>\n",
       "      <td>[(Purcell, PERSON), (Coalition, ORG)]</td>\n",
       "      <td>[(just, 2, advmod), (decentralisation, 0, root...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In December, an Oklahoma City jury found Danie...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[(In, ADP), (December, PROPN), (,, PUNCT), (an...</td>\n",
       "      <td>[(December, DATE), (Oklahoma City, GPE), (Dani...</td>\n",
       "      <td>[(In, 2, case), (December, 8, obl), (,, 8, pun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Her brother, Declan, provided The Post with a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Her, PRON), (brother, NOUN), (,, PUNCT), (De...</td>\n",
       "      <td>[(Declan, PERSON), (The Post, ORG), (Friday, D...</td>\n",
       "      <td>[(Her, 2, nmod:poss), (brother, 6, nsubj), (,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>House Speaker Paul Ryan ( ) said Monday that t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(House, PROPN), (Speaker, PROPN), (Paul, PROP...</td>\n",
       "      <td>[(House, ORG), (Paul Ryan, PERSON), (Monday, D...</td>\n",
       "      <td>[(House, 2, compound), (Speaker, 7, nsubj), (P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Volvo Connected Bicycle Helmet Three Swedish c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Volvo, PROPN), (Connected, VERB), (Bicycle, ...</td>\n",
       "      <td>[(Volvo, ORG), (Three, CARDINAL), (Swedish, NO...</td>\n",
       "      <td>[(Volvo, 2, compound), (Connected, 7, amod), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Donald A. Henderson, a leader of one of ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Dr., PROPN), (Donald, PROPN), (A., PROPN), (...</td>\n",
       "      <td>[(Donald A. Henderson, PERSON), (one, CARDINAL...</td>\n",
       "      <td>[(Dr., 23, nsubj), (Donald, 1, flat), (A., 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  Final_Result  \\\n",
       "0  a. m. Initial eyewitness accounts of such inci...      0             0   \n",
       "1  Shortly after the beginning of the attack, the...      1             0   \n",
       "2  Judge Pryor initially supported Judge Moore bu...      0             0   \n",
       "3  Trump also expects to receive a major new fina...      2             1   \n",
       "4  just decentralisation.Mr Purcell praised the C...      1             1   \n",
       "5  In December, an Oklahoma City jury found Danie...      2             0   \n",
       "6  Her brother, Declan, provided The Post with a ...      0             0   \n",
       "7  House Speaker Paul Ryan ( ) said Monday that t...      0             0   \n",
       "8  Volvo Connected Bicycle Helmet Three Swedish c...      0             0   \n",
       "9  Dr. Donald A. Henderson, a leader of one of ma...      0             0   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(a., X), (m., NOUN), (Initial, ADJ), (eyewitn...   \n",
       "1  [(Shortly, ADV), (after, ADP), (the, DET), (be...   \n",
       "2  [(Judge, NOUN), (Pryor, PROPN), (initially, AD...   \n",
       "3  [(Trump, PROPN), (also, ADV), (expects, VERB),...   \n",
       "4  [(just, ADV), (decentralisation, NOUN), (., PU...   \n",
       "5  [(In, ADP), (December, PROPN), (,, PUNCT), (an...   \n",
       "6  [(Her, PRON), (brother, NOUN), (,, PUNCT), (De...   \n",
       "7  [(House, PROPN), (Speaker, PROPN), (Paul, PROP...   \n",
       "8  [(Volvo, PROPN), (Connected, VERB), (Bicycle, ...   \n",
       "9  [(Dr., PROPN), (Donald, PROPN), (A., PROPN), (...   \n",
       "\n",
       "                                            entities  \\\n",
       "0                 [(British, NORP), (Cox’s, PERSON)]   \n",
       "1   [(Talibans, NORP), (Zabihullah Mujahid, PERSON)]   \n",
       "2                 [(Pryor, PERSON), (Moore, PERSON)]   \n",
       "3  [(Trump, PERSON), (the United States, GPE), (t...   \n",
       "4              [(Purcell, PERSON), (Coalition, ORG)]   \n",
       "5  [(December, DATE), (Oklahoma City, GPE), (Dani...   \n",
       "6  [(Declan, PERSON), (The Post, ORG), (Friday, D...   \n",
       "7  [(House, ORG), (Paul Ryan, PERSON), (Monday, D...   \n",
       "8  [(Volvo, ORG), (Three, CARDINAL), (Swedish, NO...   \n",
       "9  [(Donald A. Henderson, PERSON), (one, CARDINAL...   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(a., 10, dep), (m., 10, nsubj), (Initial, 5, ...  \n",
       "1  [(Shortly, 4, advmod), (after, 4, case), (the,...  \n",
       "2  [(Judge, 4, nsubj), (Pryor, 1, flat), (initial...  \n",
       "3  [(Trump, 3, nsubj), (also, 3, advmod), (expect...  \n",
       "4  [(just, 2, advmod), (decentralisation, 0, root...  \n",
       "5  [(In, 2, case), (December, 8, obl), (,, 8, pun...  \n",
       "6  [(Her, 2, nmod:poss), (brother, 6, nsubj), (,,...  \n",
       "7  [(House, 2, compound), (Speaker, 7, nsubj), (P...  \n",
       "8  [(Volvo, 2, compound), (Connected, 7, amod), (...  \n",
       "9  [(Dr., 23, nsubj), (Donald, 1, flat), (A., 1, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33375c99-60a7-4f95-a5d1-87d16c49c0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the dataset\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged = df_train_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "915deb00-52fe-4572-b9e7-5a7f68e35cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0    3701\n",
       "2     738\n",
       "1     593\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11d1c91f-3ad7-4025-a5fe-671a593c81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0    416\n",
       "2     84\n",
       "1     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc10ca-614a-4f59-b8bd-2ecd6d0c916f",
   "metadata": {},
   "source": [
    "## Save to Excel random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca906596-78b7-4b4b-8426-eccb6403ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 20 random rows from each DataFrame\n",
    "#df_train_sample = df_train_ready_merged.sample(n=200, random_state=42)\n",
    "#df_valid_sample = df_valid_ready_merged.sample(n=200, random_state=42)\n",
    "\n",
    "# Save them to an Excel file with different sheets\n",
    "#with pd.ExcelWriter('sampled_data_2.xlsx') as writer:\n",
    "    #df_train_sample.to_excel(writer, sheet_name='Train_Sample', index=False)\n",
    "    #df_valid_sample.to_excel(writer, sheet_name='Valid_Sample', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80232b5-0c37-466e-9c42-8022250cea6e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427d92c-2bc0-43d1-9e4a-e7ac7590985c",
   "metadata": {},
   "source": [
    "## Map values in Final_Result column to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d74c33e-c339-4f34-9040-a776c93b3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_15268\\908398405.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_15268\\908398405.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {\"self - 1\": 1, \"self - 2\": 2}\n",
    "\n",
    "# 0 - neutral, 1 - praise, 2 - blame\n",
    "\n",
    "# Apply the mapping to the 'Final_Result' column\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18124c1a-5036-4314-b0fa-5da17bead4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0    3701\n",
       "2     738\n",
       "1     593\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef282da6-6eb9-4226-a64e-22840708b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0    416\n",
       "2     84\n",
       "1     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ee14185-795a-4684-92e8-06358fb39bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval = df_train_ready_merged[['Sentence', 'Label', 'Final_Result']]\n",
    "df_valid_eval = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e40a5-4319-4f03-921d-cd7c3953eb84",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a296a3-fa30-49fa-a7f1-701772bc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_train = df_train_eval['Label']\n",
    "y_pred_train = df_train_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b674ec-f8c7-4c94-a771-73660a1670f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.566375       0.459475          0.535049\n",
      "1  Precision       0.566375       0.512161               NaN\n",
      "2     Recall       0.566375       0.454808               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69      2733\n",
      "           1       0.37      0.28      0.32       798\n",
      "           2       0.57      0.28      0.37      1501\n",
      "\n",
      "    accuracy                           0.57      5032\n",
      "   macro avg       0.51      0.45      0.46      5032\n",
      "weighted avg       0.55      0.57      0.54      5032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "f1_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "f1_weighted = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_train, y_pred_train, average='micro')\n",
    "precision_macro = precision_score(y_true_train, y_pred_train, average='macro')\n",
    "recall_micro = recall_score(y_true_train, y_pred_train, average='micro')\n",
    "recall_macro = recall_score(y_true_train, y_pred_train, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46642122-b89b-49fc-9463-56e7b5c447c8",
   "metadata": {},
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8fa5b22-60a0-4dcb-b334-223b84179568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_valid = df_valid_eval['Label']\n",
    "y_pred_valid = df_valid_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dfaea35-0810-4955-870e-75195f16a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.574545       0.466072          0.541533\n",
      "1  Precision       0.574545       0.527924               NaN\n",
      "2     Recall       0.574545       0.456872               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69       305\n",
      "           1       0.44      0.28      0.34        78\n",
      "           2       0.55      0.28      0.37       167\n",
      "\n",
      "    accuracy                           0.57       550\n",
      "   macro avg       0.53      0.46      0.47       550\n",
      "weighted avg       0.56      0.57      0.54       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_valid, y_pred_valid, average='micro')\n",
    "f1_macro = f1_score(y_true_valid, y_pred_valid, average='macro')\n",
    "f1_weighted = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_valid, y_pred_valid, average='micro')\n",
    "precision_macro = precision_score(y_true_valid, y_pred_valid, average='macro')\n",
    "recall_micro = recall_score(y_true_valid, y_pred_valid, average='micro')\n",
    "recall_macro = recall_score(y_true_valid, y_pred_valid, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd1bf9-773f-4c8c-a589-923d2d3789a8",
   "metadata": {},
   "source": [
    "# I have labeled 200 + 100 rows myself and now will make report from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13ccda0-0a42-49b7-a418-ed2725456ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = pd.read_excel('sampled_data_2.xlsx', sheet_name='Train_Sample').head(200)\n",
    "df_valid_sample = pd.read_excel('sampled_data_2.xlsx', sheet_name='Valid_Sample').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7cdbc78-352c-4416-add0-125333faccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>my_label</th>\n",
       "      <th>Label_they</th>\n",
       "      <th>Final_Result_RBC</th>\n",
       "      <th>all_the_same</th>\n",
       "      <th>my_they_same</th>\n",
       "      <th>they_RBC_same</th>\n",
       "      <th>my_RBC_same</th>\n",
       "      <th>all_different</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They say: I have a job, I must support my fami...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[('They', 'PRON'), ('say', 'VERB'), (':', 'PUN...</td>\n",
       "      <td>[('hundreds', 'CARDINAL'), ('the Middle East',...</td>\n",
       "      <td>[('They', 2, 'nsubj'), ('say', 0, 'root'), (':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Sterling hasn’t missed a Yankees game sin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[('John', 'PROPN'), ('Sterling', 'PROPN'), ('h...</td>\n",
       "      <td>[('John Sterling', 'PERSON'), ('Yankees', 'ORG...</td>\n",
       "      <td>[('John', 5, 'nsubj'), ('Sterling', 1, 'flat')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phyllis Schlafly’s obituaries were windows on ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[('Phyllis', 'PROPN'), ('Schlafly', 'PROPN'), ...</td>\n",
       "      <td>[('Phyllis Schlafly’s', 'PERSON'), ('American'...</td>\n",
       "      <td>[('Phyllis', 4, 'nmod:poss'), ('Schlafly', 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andrew Flintoff's father says the all-rounder ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[('Andrew', 'PROPN'), ('Flintoff', 'PROPN'), (...</td>\n",
       "      <td>[(\"Andrew Flintoff's\", 'PERSON'), ('Duncan Fle...</td>\n",
       "      <td>[('Andrew', 4, 'nmod:poss'), ('Flintoff', 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boehners attack line is that Obama is a consta...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[('Boehners', 'PROPN'), ('attack', 'NOUN'), ('...</td>\n",
       "      <td>[('Boehners', 'PERSON'), ('Obama', 'PERSON'), ...</td>\n",
       "      <td>[('Boehners', 2, 'compound'), ('attack', 3, 'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  my_label  Label_they  \\\n",
       "0  They say: I have a job, I must support my fami...       0.0         1.0   \n",
       "1  John Sterling hasn’t missed a Yankees game sin...       0.0         1.0   \n",
       "2  Phyllis Schlafly’s obituaries were windows on ...       0.0         0.0   \n",
       "3  Andrew Flintoff's father says the all-rounder ...       2.0         2.0   \n",
       "4  Boehners attack line is that Obama is a consta...       2.0         2.0   \n",
       "\n",
       "   Final_Result_RBC  all_the_same my_they_same they_RBC_same my_RBC_same  \\\n",
       "0               2.0           0.0            0             0           0   \n",
       "1               0.0           0.0            0             0           1   \n",
       "2               0.0           1.0            1             1           1   \n",
       "3               0.0           0.0            1             0           0   \n",
       "4               0.0           0.0            1             0           0   \n",
       "\n",
       "   all_different                                         tokens_pos  \\\n",
       "0            1.0  [('They', 'PRON'), ('say', 'VERB'), (':', 'PUN...   \n",
       "1            0.0  [('John', 'PROPN'), ('Sterling', 'PROPN'), ('h...   \n",
       "2            0.0  [('Phyllis', 'PROPN'), ('Schlafly', 'PROPN'), ...   \n",
       "3            0.0  [('Andrew', 'PROPN'), ('Flintoff', 'PROPN'), (...   \n",
       "4            0.0  [('Boehners', 'PROPN'), ('attack', 'NOUN'), ('...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [('hundreds', 'CARDINAL'), ('the Middle East',...   \n",
       "1  [('John Sterling', 'PERSON'), ('Yankees', 'ORG...   \n",
       "2  [('Phyllis Schlafly’s', 'PERSON'), ('American'...   \n",
       "3  [(\"Andrew Flintoff's\", 'PERSON'), ('Duncan Fle...   \n",
       "4  [('Boehners', 'PERSON'), ('Obama', 'PERSON'), ...   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [('They', 2, 'nsubj'), ('say', 0, 'root'), (':...  \n",
       "1  [('John', 5, 'nsubj'), ('Sterling', 1, 'flat')...  \n",
       "2  [('Phyllis', 4, 'nmod:poss'), ('Schlafly', 1, ...  \n",
       "3  [('Andrew', 4, 'nmod:poss'), ('Flintoff', 1, '...  \n",
       "4  [('Boehners', 2, 'compound'), ('attack', 3, 'c...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72288f1-cd02-4a53-b1ff-4c0c0d194130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>my_label</th>\n",
       "      <th>Label_they</th>\n",
       "      <th>Final_Result_RBC</th>\n",
       "      <th>all_the_same</th>\n",
       "      <th>my_they_same</th>\n",
       "      <th>they_RBC_same</th>\n",
       "      <th>my_RBC_same</th>\n",
       "      <th>all_different</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Kate McKinnon of “Saturday Night Live” provide...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>If I were advising Bush, I’d have told him to ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Mr. Sanders has started running commercials in...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The United States on Saturday announced it is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The European criticisms are increasingly aimed...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  my_label  Label_they  \\\n",
       "95  Kate McKinnon of “Saturday Night Live” provide...       0.0         0.0   \n",
       "96  If I were advising Bush, I’d have told him to ...       0.0         0.0   \n",
       "97  Mr. Sanders has started running commercials in...       2.0         2.0   \n",
       "98  The United States on Saturday announced it is ...       0.0         1.0   \n",
       "99  The European criticisms are increasingly aimed...       2.0         2.0   \n",
       "\n",
       "    Final_Result_RBC  all_the_same my_they_same they_RBC_same my_RBC_same  \\\n",
       "95               0.0           1.0            1             1           1   \n",
       "96               2.0           0.0            1             0           0   \n",
       "97               0.0           0.0            1             0           0   \n",
       "98               2.0           0.0            0             0           0   \n",
       "99               0.0           0.0            1             0           0   \n",
       "\n",
       "    all_different  \n",
       "95            0.0  \n",
       "96            0.0  \n",
       "97            0.0  \n",
       "98            1.0  \n",
       "99            0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901d089-b928-472e-9374-85f5b386cf6f",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564affd1-3847-46d8-aa02-30fc20487d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_train = df_train_sample['my_label']\n",
    "y_pred_train = df_train_sample['Final_Result_RBC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b6211f-43ec-4ce0-9ab6-52218c5a0048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score          0.715       0.573864          0.706733\n",
      "1  Precision          0.715       0.593491               NaN\n",
      "2     Recall          0.715       0.560133               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.84      0.81       139\n",
      "         1.0       0.43      0.39      0.41        23\n",
      "         2.0       0.57      0.45      0.50        38\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.59      0.56      0.57       200\n",
      "weighted avg       0.70      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "f1_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "f1_weighted = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_train, y_pred_train, average='micro')\n",
    "precision_macro = precision_score(y_true_train, y_pred_train, average='macro')\n",
    "recall_micro = recall_score(y_true_train, y_pred_train, average='micro')\n",
    "recall_macro = recall_score(y_true_train, y_pred_train, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde8543-f2cf-48e4-a6d8-915597cda260",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8a0a58-d965-4a16-8e6b-4f982ffd30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_valid = df_valid_sample['my_label']\n",
    "y_pred_valid = df_valid_sample['Final_Result_RBC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0e3a4b-c7fd-45b7-b003-f2eae3def230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score           0.71       0.543760          0.699235\n",
      "1  Precision           0.71       0.563714               NaN\n",
      "2     Recall           0.71       0.529455               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.84      0.81        73\n",
      "         1.0       0.64      0.54      0.58        13\n",
      "         2.0       0.27      0.21      0.24        14\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.56      0.53      0.54       100\n",
      "weighted avg       0.69      0.71      0.70       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_valid, y_pred_valid, average='micro')\n",
    "f1_macro = f1_score(y_true_valid, y_pred_valid, average='macro')\n",
    "f1_weighted = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_valid, y_pred_valid, average='micro')\n",
    "precision_macro = precision_score(y_true_valid, y_pred_valid, average='macro')\n",
    "recall_micro = recall_score(y_true_valid, y_pred_valid, average='micro')\n",
    "recall_macro = recall_score(y_true_valid, y_pred_valid, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535b422-aeb1-4329-9a84-806b2983166c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8567a3-5dfb-4c11-b15a-22a8d2753429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbb3e6-4c42-4eae-a4b8-8ea897b4cba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03313f-a8e3-4fae-b1c2-da9e2cfe65eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6242ee-ba69-4268-ba5a-6b673423a142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890805fd-68fe-4be2-9646-07388edb1422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce7397-894a-4f82-b9e2-39d98b6e30e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b461c9-794f-4d06-ac35-9851966fe3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff252c-e547-4e9f-8ac3-84f77548b551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739de8a8-39ae-4327-8c84-09b4629af27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6e69f9-ebee-4e47-b287-159f060a1bdd",
   "metadata": {},
   "source": [
    "# NEED TO CHANGE LABELS OF TEST DATA FILE AND PREPROCESS IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4a4f8-3eca-4225-bf11-1a320bfd5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee484548-0f4d-4182-a70f-aef0f0cd5b96",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d02f264-f222-4ffb-be3b-f0097f988b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import ast\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b106f5-091e-4100-af31-f7677c66f1de",
   "metadata": {},
   "source": [
    "# Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1049d0-bd96-4662-988d-0db76fe1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"senses\", \"dependencies\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"]\n",
    "\n",
    "df_train_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_valid_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "# Remove leading and trailing spaces in the \"Sentence\" column\n",
    "df_train_preprocessed['Sentence'] = df_train_preprocessed['Sentence'].str.strip()\n",
    "df_valid_preprocessed['Sentence'] = df_valid_preprocessed['Sentence'].str.strip()\n",
    "\n",
    "# Delete columns \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"\n",
    "df_train_ready = df_train_preprocessed.drop(columns = [\"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "df_valid_ready = df_valid_preprocessed.drop(columns = [\"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "\n",
    "# Shuffle the data\n",
    "#df_train_ready = df_train_preprocessed.sample(frac=1).reset_index(drop=True)\n",
    "#df_valid_ready = df_valid_preprocessed.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade7978-08a0-4c84-be4c-be6ed20f1d04",
   "metadata": {},
   "source": [
    "# Merging Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaba528-c606-4797-aeee-52437715a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {2: 1, 3: 2, 4: 2}\n",
    "\n",
    "# 0 - neutral, 1 - positive, 2 - negative\n",
    "\n",
    "df_train_ready_merged = df_train_ready\n",
    "df_valid_ready_merged = df_valid_ready\n",
    "\n",
    "# Apply the mapping to the 'Label' column\n",
    "df_train_ready_merged['Label'] = df_train_ready_merged['Label'].replace(label_mapping)\n",
    "df_valid_ready_merged['Label'] = df_valid_ready_merged['Label'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a1cf9-5a57-493a-8c3a-7802f92ffba6",
   "metadata": {},
   "source": [
    "# Turning strings back to lists and tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e79989-3230-4c6b-ba9c-17d47543c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(dependencies_str):\n",
    "    # Check if it's a string and if it appears to be in the list of tuples format\n",
    "    if isinstance(dependencies_str, str) and dependencies_str.startswith(\"[\") and dependencies_str.endswith(\"]\"):\n",
    "        try:\n",
    "            # Convert string representation of list back to actual list of tuples\n",
    "            return ast.literal_eval(dependencies_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error parsing: {dependencies_str}\")\n",
    "            raise e\n",
    "    elif isinstance(dependencies_str, list):\n",
    "        # If it's already a list, return as is\n",
    "        return dependencies_str\n",
    "    else:\n",
    "        # If it's another unexpected type, return as is or handle appropriately\n",
    "        return dependencies_str\n",
    "\n",
    "def get_sense(tokens):\n",
    "    #print(tokens)\n",
    "    senses = []\n",
    "    for item in tokens:\n",
    "        #print(item)\n",
    "        if isinstance(item, tuple) and len(item) == 2:\n",
    "            token, pos = item\n",
    "            sense = lesk([token], token)\n",
    "            senses.append((token, sense))\n",
    "        else:\n",
    "            # Handle cases where the token doesn't meet the expected structure\n",
    "            print(f\"Unexpected format: {item}\")\n",
    "            senses.append((item, None))\n",
    "    return senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e53857-0db5-4237-b8c7-e742c1fd955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['dependencies'] = df_train_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_valid_ready_merged['dependencies'] = df_valid_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_train_ready_merged['tokens_pos'] = df_train_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_valid_ready_merged['tokens_pos'] = df_valid_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_train_ready_merged['entities'] = df_train_ready_merged['entities'].apply(convert_to_list)\n",
    "df_valid_ready_merged['entities'] = df_valid_ready_merged['entities'].apply(convert_to_list)\n",
    "df_train_ready_merged['senses'] = df_train_ready_merged['tokens_pos'].apply(get_sense)\n",
    "df_valid_ready_merged['senses'] = df_valid_ready_merged['tokens_pos'].apply(get_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79357fd-7599-4283-9460-848a8ab4880b",
   "metadata": {},
   "source": [
    "# Making a small set for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610909d0-e497-46f1-b4ed-8a69c46b5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ready_merged_small = df_train_ready_merged.head(10)\n",
    "df_valid_ready_merged_small = df_valid_ready_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5898c-ac22-4229-9a87-3a89b6c3912e",
   "metadata": {},
   "source": [
    "# Following the Modified Algorithm of Blame/Praise Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291d5c-c2b0-4bba-ab48-4fc2faf21902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Older versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b404f2-ab1c-4e08-ae6f-bb832282354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Step 2: Process root verbs\\n    for i, root in enumerate(roots):\\n        root_verb, root_index = root\\n        #print(root_verb, root_index)\\n        # Check if the root verb is valid (foreseeability and not coercion)\\n        for token, pos in tokens_pos:\\n            #print(token, pos)\\n            if token == root_verb and pos == \\'VERB\\':\\n                #print(\"Found VERB\")\\n                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\\n                    #print(\"F and not C\") \\n                    root_verbs.append((root_verb, i+1))\\n        # Check for any conj attached to this root verb and add it if valid\\n        for j, conj in enumerate(dependencies):\\n            conj_word, conj_head, conj_rel = conj\\n            if conj_head == root_index and conj_rel == \\'conj\\':\\n                for token_conj, pos_conj in tokens_pos:\\n                    if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                            root_verbs.append((conj_word, j+1))\\n    \\n    #print(root_verbs)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    # Step 2: Process root verbs\n",
    "    for i, root in enumerate(roots):\n",
    "        root_verb, root_index = root\n",
    "        #print(root_verb, root_index)\n",
    "        # Check if the root verb is valid (foreseeability and not coercion)\n",
    "        for token, pos in tokens_pos:\n",
    "            #print(token, pos)\n",
    "            if token == root_verb and pos == 'VERB':\n",
    "                #print(\"Found VERB\")\n",
    "                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\n",
    "                    #print(\"F and not C\") \n",
    "                    root_verbs.append((root_verb, i+1))\n",
    "        # Check for any conj attached to this root verb and add it if valid\n",
    "        for j, conj in enumerate(dependencies):\n",
    "            conj_word, conj_head, conj_rel = conj\n",
    "            if conj_head == root_index and conj_rel == 'conj':\n",
    "                for token_conj, pos_conj in tokens_pos:\n",
    "                    if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                            root_verbs.append((conj_word, j+1))\n",
    "    \n",
    "    #print(root_verbs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0d40cf-2090-4b6a-bcaf-7278232587ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    \\n    # Lists to store categorized verbs\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for i, dep in enumerate(dependencies):\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, i+1))\\n                # Check for any conj attached to this root verb and add it if valid\\n                for j, conj in enumerate(dependencies):\\n                    conj_word, conj_head, conj_rel = conj\\n                    if conj_head == i+1 and conj_rel == \\'conj\\':\\n                        for token_conj, pos_conj in tokens_pos:\\n                            if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                    root_verbs.append((conj_word, j+1))\\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, i+1))\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for j, conj in enumerate(dependencies):\n",
    "                    conj_word, conj_head, conj_rel = conj\n",
    "                    if conj_head == i+1 and conj_rel == 'conj':\n",
    "                        for token_conj, pos_conj in tokens_pos:\n",
    "                            if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                    root_verbs.append((conj_word, j+1))\n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b412f5aa-2f1f-4e37-ad7d-0e12e665ebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# TRYING WITH COUNTER\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                #print(\"Update Counter I\")\\n                counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any conj attached to this root verb and add it if valid\\n                for conj in dependencies:\\n                    if len(conj) == 3:\\n                        conj_word, conj_head, conj_rel = conj\\n                        counter_j = counter_j +1\\n                        #print(conj_word, counter_j)\\n                        if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if conj_head == counter_i and conj_rel == \\'conj\\':\\n                            for token_conj, pos_conj in tokens_pos:\\n                                if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                        root_verbs.append((conj_word, counter_j))\\n                    \\n                    \\n        \\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(roots)\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TRYING WITH COUNTER\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                #print(\"Update Counter I\")\n",
    "                counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for conj in dependencies:\n",
    "                    if len(conj) == 3:\n",
    "                        conj_word, conj_head, conj_rel = conj\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(conj_word, counter_j)\n",
    "                        if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if conj_head == counter_i and conj_rel == 'conj':\n",
    "                            for token_conj, pos_conj in tokens_pos:\n",
    "                                if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                        root_verbs.append((conj_word, counter_j))\n",
    "                    \n",
    "                    \n",
    "        \n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(roots)\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb86b91-082e-4234-bb2d-3b244ede8e74",
   "metadata": {},
   "source": [
    "## Current Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1192b943-3099-4a22-9935-636f4566de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to check if a verb belongs to Foreseeability or Coercion groups\n",
    "\n",
    "def is_foreseeability_verb(verb):\n",
    "    #print(\"is_foreseeable_verb\")\n",
    "    # Synset categories indicating foreseeability\n",
    "    foreseeability_classes = {'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'}\n",
    "    # Get the synsets for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        # Check if the verb belongs to any of the foreseeability classes\n",
    "        lexname = synset.lexname().split('.')[1]\n",
    "        #if verb == \"unleashed\": print(synset.lexname())\n",
    "        if lexname in foreseeability_classes:\n",
    "            #print(synset)\n",
    "            #print(lexname)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_coercion_verb(verb):\n",
    "    #print(\"is_coercion_verb\")\n",
    "    coercion_classes = {'urge-58.1', 'force-59', 'forbid-67'}\n",
    "    # Get the VerbNet classes for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        lemma = synset.lemmas()[0]\n",
    "        #print(synset)\n",
    "        #print(lemma)\n",
    "        vn_classes = lemma.key().split('%')[0]\n",
    "        vn_class_ids = vn.classids(vn_classes)\n",
    "        #if verb == \"unleashed\": print(verb, vn_classes, vn_class_ids)\n",
    "        # Check if any VerbNet class matches the coercion classes\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6632ed-0032-44b0-9830-875c49495651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING WITH all tags in one for cycle\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "       \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    counter_x = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            \n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    #print(\"Update Counter I\")\n",
    "                    counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any relater words attached to this root verb and add it if valid\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(related_word, counter_j)\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if related_head == counter_i and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl', 'conj']:\n",
    "                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\n",
    "                            for token_related, pos_related in tokens_pos:\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\n",
    "                                        \n",
    "                                        if related_rel == 'conj':\n",
    "                                            root_verbs.append((related_word, counter_j))\n",
    "                                            \n",
    "                                        elif related_rel == 'xcomp':\n",
    "                                            xcomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    xcomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'ccomp':\n",
    "                                            ccomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    ccomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'parataxis':\n",
    "                                            parataxis_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    parataxis_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'advcl':\n",
    "                                            advcl_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    advcl_verbs.append((conj_word, counter_x))\n",
    "    \n",
    "\n",
    "    #print(\"NEW ROW\")\n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314989cf-1c6a-4635-b61b-e4ec36054046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_11660\\3554992671.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_11660\\3554992671.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0679a55d-3cd0-4028-b35b-9a106e114263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>related</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan has endorsed Trump and told reporters thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>related</td>\n",
       "      <td>[(Ryan, PROPN), (has, AUX), (endorsed, VERB), ...</td>\n",
       "      <td>[(Ryan, PERSON), (Trump, PERSON), (this past w...</td>\n",
       "      <td>[(Ryan, None), (has, Synset('take.v.35')), (en...</td>\n",
       "      <td>[(Ryan, 3, nsubj), (has, 3, aux), (endorsed, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John McGraw, 78, was charged with assault and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>related</td>\n",
       "      <td>[(John, PROPN), (McGraw, PROPN), (,, PUNCT), (...</td>\n",
       "      <td>[(John McGraw, PERSON), (78, DATE), (Thursday,...</td>\n",
       "      <td>[(John, Synset('john.n.02')), (McGraw, Synset(...</td>\n",
       "      <td>[(John, 7, nsubj:pass), (McGraw, 1, flat), (,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Filipino fighter unleashed a dazzling comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>related</td>\n",
       "      <td>[(The, DET), (Filipino, ADJ), (fighter, NOUN),...</td>\n",
       "      <td>[(Filipino, NORP), (Margarito, PERSON), (Maywe...</td>\n",
       "      <td>[(The, None), (Filipino, Synset('philippine.n....</td>\n",
       "      <td>[(The, 3, det), (Filipino, 3, amod), (fighter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But the Marlins have failed to make the postse...</td>\n",
       "      <td>0</td>\n",
       "      <td>related</td>\n",
       "      <td>[(But, CCONJ), (the, DET), (Marlins, PROPN), (...</td>\n",
       "      <td>[(Marlins, ORG), (Loria, PERSON)]</td>\n",
       "      <td>[(But, Synset('merely.r.01')), (the, None), (M...</td>\n",
       "      <td>[(But, 5, cc), (the, 3, det), (Marlins, 5, nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>So shortly after 2 a.m., campaign chairman Joh...</td>\n",
       "      <td>0</td>\n",
       "      <td>related</td>\n",
       "      <td>[(So, ADV), (shortly, ADV), (after, ADP), (2, ...</td>\n",
       "      <td>[(2 a.m., TIME), (John Podesta, PERSON), (Clin...</td>\n",
       "      <td>[(So, Synset('thus.r.02')), (shortly, Synset('...</td>\n",
       "      <td>[(So, 11, advmod), (shortly, 4, advmod), (afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Church has not answered the allegations ot...</td>\n",
       "      <td>0</td>\n",
       "      <td>related</td>\n",
       "      <td>[(The, DET), (Church, PROPN), (has, AUX), (not...</td>\n",
       "      <td>[(Church, ORG), (Navajo, GPE)]</td>\n",
       "      <td>[(The, None), (Church, Synset('church_service....</td>\n",
       "      <td>[(The, 2, det), (Church, 5, nsubj), (has, 5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bruno Beschizza, the conservative mayor of has...</td>\n",
       "      <td>0</td>\n",
       "      <td>related</td>\n",
       "      <td>[(Bruno, PROPN), (Beschizza, PROPN), (,, PUNCT...</td>\n",
       "      <td>[(Bruno Beschizza, PERSON), (Théo, PERSON)]</td>\n",
       "      <td>[(Bruno, Synset('leo_ix.n.01')), (Beschizza, N...</td>\n",
       "      <td>[(Bruno, 9, nsubj), (Beschizza, 1, flat), (,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>On criminal justice issues, Harris faults Carp...</td>\n",
       "      <td>2</td>\n",
       "      <td>related</td>\n",
       "      <td>[(On, ADP), (criminal, ADJ), (justice, NOUN), ...</td>\n",
       "      <td>[(Harris, PERSON), (Carper, PERSON), (the 1990...</td>\n",
       "      <td>[(On, Synset('on.r.03')), (criminal, Synset('c...</td>\n",
       "      <td>[(On, 4, case), (criminal, 3, amod), (justice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Even the hologame that Finn starts up on the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>related</td>\n",
       "      <td>[(Even, ADV), (the, DET), (hologame, NOUN), (t...</td>\n",
       "      <td>[(Finn, PERSON), (Chewbacca, PERSON), (A New H...</td>\n",
       "      <td>[(Even, Synset('tied.s.05')), (the, None), (ho...</td>\n",
       "      <td>[(Even, 3, advmod), (the, 3, det), (hologame, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label Final_Result  \\\n",
       "0                  George is not supporting Clinton.      2      related   \n",
       "1  Ryan has endorsed Trump and told reporters thi...      1      related   \n",
       "2  John McGraw, 78, was charged with assault and ...      2      related   \n",
       "3  The Filipino fighter unleashed a dazzling comb...      1      related   \n",
       "4  But the Marlins have failed to make the postse...      0      related   \n",
       "5  So shortly after 2 a.m., campaign chairman Joh...      0      related   \n",
       "6  The Church has not answered the allegations ot...      0      related   \n",
       "7  Bruno Beschizza, the conservative mayor of has...      0      related   \n",
       "8  On criminal justice issues, Harris faults Carp...      2      related   \n",
       "9  Even the hologame that Finn starts up on the s...      0      related   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "1  [(Ryan, PROPN), (has, AUX), (endorsed, VERB), ...   \n",
       "2  [(John, PROPN), (McGraw, PROPN), (,, PUNCT), (...   \n",
       "3  [(The, DET), (Filipino, ADJ), (fighter, NOUN),...   \n",
       "4  [(But, CCONJ), (the, DET), (Marlins, PROPN), (...   \n",
       "5  [(So, ADV), (shortly, ADV), (after, ADP), (2, ...   \n",
       "6  [(The, DET), (Church, PROPN), (has, AUX), (not...   \n",
       "7  [(Bruno, PROPN), (Beschizza, PROPN), (,, PUNCT...   \n",
       "8  [(On, ADP), (criminal, ADJ), (justice, NOUN), ...   \n",
       "9  [(Even, ADV), (the, DET), (hologame, NOUN), (t...   \n",
       "\n",
       "                                            entities  \\\n",
       "0              [(George, PERSON), (Clinton, PERSON)]   \n",
       "1  [(Ryan, PERSON), (Trump, PERSON), (this past w...   \n",
       "2  [(John McGraw, PERSON), (78, DATE), (Thursday,...   \n",
       "3  [(Filipino, NORP), (Margarito, PERSON), (Maywe...   \n",
       "4                  [(Marlins, ORG), (Loria, PERSON)]   \n",
       "5  [(2 a.m., TIME), (John Podesta, PERSON), (Clin...   \n",
       "6                     [(Church, ORG), (Navajo, GPE)]   \n",
       "7        [(Bruno Beschizza, PERSON), (Théo, PERSON)]   \n",
       "8  [(Harris, PERSON), (Carper, PERSON), (the 1990...   \n",
       "9  [(Finn, PERSON), (Chewbacca, PERSON), (A New H...   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "1  [(Ryan, None), (has, Synset('take.v.35')), (en...   \n",
       "2  [(John, Synset('john.n.02')), (McGraw, Synset(...   \n",
       "3  [(The, None), (Filipino, Synset('philippine.n....   \n",
       "4  [(But, Synset('merely.r.01')), (the, None), (M...   \n",
       "5  [(So, Synset('thus.r.02')), (shortly, Synset('...   \n",
       "6  [(The, None), (Church, Synset('church_service....   \n",
       "7  [(Bruno, Synset('leo_ix.n.01')), (Beschizza, N...   \n",
       "8  [(On, Synset('on.r.03')), (criminal, Synset('c...   \n",
       "9  [(Even, Synset('tied.s.05')), (the, None), (ho...   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  \n",
       "1  [(Ryan, 3, nsubj), (has, 3, aux), (endorsed, 0...  \n",
       "2  [(John, 7, nsubj:pass), (McGraw, 1, flat), (,,...  \n",
       "3  [(The, 3, det), (Filipino, 3, amod), (fighter,...  \n",
       "4  [(But, 5, cc), (the, 3, det), (Marlins, 5, nsu...  \n",
       "5  [(So, 11, advmod), (shortly, 4, advmod), (afte...  \n",
       "6  [(The, 2, det), (Church, 5, nsubj), (has, 5, a...  \n",
       "7  [(Bruno, 9, nsubj), (Beschizza, 1, flat), (,, ...  \n",
       "8  [(On, 4, case), (criminal, 3, amod), (justice,...  \n",
       "9  [(Even, 3, advmod), (the, 3, det), (hologame, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aecdcd-7444-4943-81f1-e6f398c7260c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f1949-251b-4cab-8736-16038e741e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442f47c-e667-46a7-8102-007f64956aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4afdb-5131-4845-b389-594cb5cf2eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33375c99-60a7-4f95-a5d1-87d16c49c0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the dataset\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged = df_train_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "915deb00-52fe-4572-b9e7-5a7f68e35cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "related    4288\n",
       "0           744\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d1c91f-3ad7-4025-a5fe-671a593c81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "related    465\n",
       "0           85\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d863eeb-ac5c-4246-afa7-8e1beb5a33a4",
   "metadata": {},
   "source": [
    "# ТЕПЕРЬ НАДО РЕАЛИЗОВАТЬ СВЯЗИ РАЗНЫХ ТИПОВ И ИСКАТЬ ОБЪЕКТЫ И АГЕНТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ac0b0-dfe2-44ea-b93b-207835041f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608e78b-0f23-4580-a96e-25df267b5523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c82975-c6c6-4c93-8007-861331393f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768bc06-bbb7-4a28-9678-538e2e33cb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3757815-dbdf-4fe8-884c-ca6ec9e29393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15061f9f-9247-437a-b578-619e2e8931a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9116c2-012b-4c3b-8fad-93ba000313a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9c75dc-5ae1-4504-a84b-30bb8aa66078",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "def get_event_polarity(verb, obj):\n",
    "    # Create a simple context for WSD\n",
    "    context = f\"{verb} {obj}\"\n",
    "    \n",
    "    # Word Sense Disambiguation for the verb and object\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    # Calculate polarity using SentiWordNet\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    # AFINN score\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    # Subjectivity Lexicon score\n",
    "    tokens = context.split()\n",
    "    subj_pos = sum([1 for token in tokens if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in tokens if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    # Determine final polarity\n",
    "    if pos_score > neg_score:\n",
    "        return '1'  # Positive/Praise\n",
    "    elif neg_score > pos_score:\n",
    "        return '2'  # Negative/Blame\n",
    "    else:\n",
    "        return '0'  # Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093c7296-2fa6-497d-9294-7f68d16ae0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events_and_agents(dependencies):\n",
    "    \n",
    "    events = []\n",
    "    verbs = []\n",
    "    \n",
    "    # Step 1: Identify all verbs that are 'root'\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                verbs.append((word, i + 1))  # Save the verb and its index (i+1)\n",
    "    \n",
    "    # Step 2: For each identified verb, find associated subjects and objects\n",
    "    for verb, verb_index in verbs:\n",
    "        subject = None\n",
    "        obj = None\n",
    "\n",
    "        for word, head, deprel in dependencies:\n",
    "            if head == verb_index:  # Compare with the index of the verb, not its head\n",
    "                if deprel in ['nsubj', 'nsubj:pass']:  # Subject of the verb\n",
    "                    subject = word\n",
    "                if deprel in ['obj', 'dobj']:  # Object of the verb\n",
    "                    obj = word\n",
    "        \n",
    "        if subject and obj:\n",
    "            # Calculate polarity specifically for this verb-object pair\n",
    "            polarity = get_event_polarity(verb, obj)\n",
    "            events.append({\n",
    "                'verb': verb,\n",
    "                'object': obj,\n",
    "                'agent': subject,\n",
    "                'polarity': polarity\n",
    "            })\n",
    "    \n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b090888-6087-4f22-becb-a97fb9e9fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['events'] = df_train_ready_merged['dependencies'].apply(extract_events_and_agents)\n",
    "df_valid_ready_merged['events'] = df_valid_ready_merged['dependencies'].apply(extract_events_and_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703541e-99e0-422e-923d-7ad187c8ba13",
   "metadata": {},
   "source": [
    "## Label rows without events \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a515fb98-4c09-4fd6-a01c-40bfb30316bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_final_result_column(df):\n",
    "    # Insert 'Final_Result' column after 'Label' with default value None\n",
    "    label_index = df.columns.get_loc('Label')\n",
    "    df.insert(label_index + 1, 'Final_Result', None)\n",
    "    \n",
    "    # Update 'Final_Result' to 0 where 'events' column is empty\n",
    "    df.loc[df['events'].apply(lambda x: not x), 'Final_Result'] = \"others\"\n",
    "\n",
    "# Apply the function to both dataframes\n",
    "add_final_result_column(df_train_ready_merged)\n",
    "add_final_result_column(df_valid_ready_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf515f-3d9d-40f2-b439-701cc625c035",
   "metadata": {},
   "source": [
    "## Step 2. Agent Causality\n",
    "\n",
    "\"Here, one must establish that a moral agent caused an event. We first make use of a popular explicit intra-sentential pattern for causation expression which is “NP verb NP” where NP is a noun phrase (Girju, 2003) and then we identify the agent within the noun phrase. If the intrasentential pattern is not found we consider verbs in the text that belong to the CAUSE class and the CAUSETO semantic relation which are defined in the WordNet. In order for “Agent Causality” taking the value “True”, the agent must be a person entity (including pronouns).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb437b77-e51e-479e-b426-9f423421414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_causative_verb(verb):\n",
    "    # Check if the verb is in the CAUSE class or has CAUSETO relation in WordNet\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a79467e-69f6-435f-b344-e5e8a8154de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_agent_causality(row):\n",
    "    dependencies = row['dependencies']\n",
    "    entities = row['entities']\n",
    "    agent_causality = False\n",
    "    verbs = []\n",
    "\n",
    "    # Step 1: Identify all verbs that are 'root'\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                verbs.append((word, i + 1))  # Save the verb and its index (i+1)\n",
    "    \n",
    "    # Step 2: For each identified verb, find associated subjects and objects\n",
    "    for verb, verb_index in verbs:\n",
    "        subject = None\n",
    "        obj = None\n",
    "\n",
    "        for word, head, deprel in dependencies:\n",
    "            if head == verb_index:  # Compare with the index of the verb, not its head\n",
    "                if deprel in ['nsubj', 'nsubjpass']:  # Subject of the verb\n",
    "                    subject = word\n",
    "                if deprel in ['obj', 'dobj']:  # Object of the verb\n",
    "                    obj = word\n",
    "        \n",
    "        # Step 3: Validate the agent (subject)\n",
    "        agent_is_valid = False\n",
    "        if subject:\n",
    "            for entity, label in entities:\n",
    "                if entity == subject and label in ['PERSON', 'ORG', 'GPE']:\n",
    "                    agent_is_valid = True\n",
    "                    break\n",
    "            \n",
    "            if not agent_is_valid and 'PRP' in [pos for token, pos in row['tokens_pos'] if token == subject]:\n",
    "                agent_is_valid = True  # It's a pronoun\n",
    "\n",
    "        # Step 4: If both subject and object are found and agent is valid, we have causality\n",
    "        if subject and obj and agent_is_valid:\n",
    "            agent_causality = True\n",
    "            break\n",
    "    \n",
    "    # Step 5: If no \"NP verb NP\" pattern is found, check for causative verbs\n",
    "    if not agent_causality:\n",
    "        for verb, verb_index in verbs:\n",
    "            if check_causative_verb(verb):\n",
    "                agent_causality = True\n",
    "                break\n",
    "    \n",
    "    return agent_causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f0d350-18d2-4bb4-87b7-b9c8b3d732ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_ready_merged['agent_causality'] = df_train_ready_merged.apply(identify_agent_causality, axis=1)\n",
    "df_valid_ready_merged['agent_causality'] = df_valid_ready_merged.apply(identify_agent_causality, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e7abf-087a-4181-a48f-4ec4cdcf2f22",
   "metadata": {},
   "source": [
    "## Label rows without agent causality \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4ae980d-33f4-4f60-9f8e-85ee955684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where agent_causality is False in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['agent_causality'] == False, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where agent_causality is False in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['agent_causality'] == False, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6d564-e230-4694-8844-cf779df7f054",
   "metadata": {},
   "source": [
    "## Step 3. Foreseeability\n",
    "\n",
    "\"Foreseeability. We rely on a set of verbs which indicate foreseeability. These include verbs of communication as suggested in (Mao et al, 2011) and other verb classes which include verbs of creation, verbs of consumption, verbs of competition, verbs of possession and verbs of motion. These classes of verbs are defined in the WordNet7 and can be identified by looking at the WordNet sensekey of the verbs. Example: When I did not speak the truth. In the example above, the communication verb “speak” indicates that the subject “I” had foreknowledge of the event of “speaking the truth”.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defae370-1030-4e1d-82aa-38d92af27075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_foreseeability(row):\n",
    "    dependencies = row['dependencies']\n",
    "    foreseeable = False\n",
    "    \n",
    "    # Identify all verbs in the sentence\n",
    "    verbs = [word for word, head, deprel in dependencies if deprel == 'root']\n",
    "\n",
    "    # Check if any verb indicates foreseeability\n",
    "    for verb in verbs:\n",
    "        if is_foreseeable_verb(verb):\n",
    "            foreseeable = True\n",
    "            break\n",
    "    \n",
    "    return foreseeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fad26a0-54ff-4b1d-96d0-797a14714e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a verb belongs to any foreseeability-related classes\n",
    "def is_foreseeable_verb(verb):\n",
    "    #print(\"NEW\")\n",
    "    #print(verb)\n",
    "    \n",
    "    # Synset categories indicating foreseeability\n",
    "    foreseeability_classes = {\n",
    "        'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'\n",
    "    }\n",
    "\n",
    "    # Get the synsets for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        # Check if the verb belongs to any of the foreseeability classes\n",
    "        lexname = synset.lexname().split('.')[1]\n",
    "        if lexname in foreseeability_classes:\n",
    "            #print(synset.lexname())\n",
    "            #print(synset)\n",
    "            #print(lexname)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5487971-b003-4849-9b54-99a4cfe821fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_ready_merged['foreseeability'] = df_train_ready_merged.apply(determine_foreseeability, axis=1)\n",
    "df_valid_ready_merged['foreseeability'] = df_valid_ready_merged.apply(determine_foreseeability, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb67f423-8435-4508-836b-6e9d6ad80ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foreseeability\n",
       "True     4248\n",
       "False     784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"foreseeability\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f586af-563c-477b-81bc-0afd08b187e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foreseeability\n",
       "True     456\n",
       "False     94\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"foreseeability\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46be908-1ea0-4d86-bf0c-19a3ce5628aa",
   "metadata": {},
   "source": [
    "## Label rows without foreseeability \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b43bbd8-4739-4d9a-9da3-db92cd5cf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where foreseeability is False in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['foreseeability'] == False, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where foreseeability is False in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['foreseeability'] == False, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acc753-293f-40f7-bcd5-33706d86f8e6",
   "metadata": {},
   "source": [
    "## Step 4. Coercion\n",
    "\n",
    "\"To identify coercion, we look at the extension verb classes presented in (Kipper et al, 2006) focusing on verbs in the URGE (13 members), FORCE (46 members) and FORBID (17 members) classes. Example: I was forced to quite the job in the city. In the example above , using word sense disambiguation, the verb “forced” is of sense “to cause to do through pressure or necessity, by physical, moral or intellectual means”. The agent “I” in this case did not willingly quite the job and the sentence does not mention who forced the agent. Thus, the sentence is classified as “Others” (i.e., no blame or praise).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e957c6c-8bc1-4eec-8328-681688346142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_coercion(row):\n",
    "    dependencies = row['dependencies']\n",
    "    coercion = False\n",
    "    \n",
    "    # Identify all verbs in the sentence\n",
    "    verbs = [word for word, head, deprel in dependencies if deprel == 'root']\n",
    "    \n",
    "    # Check if any verb indicates coercion\n",
    "    for verb in verbs:\n",
    "        if is_coercion_verb(verb):\n",
    "            coercion = True\n",
    "            break\n",
    "    \n",
    "    return coercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a66db4-4305-4530-bf76-da83e68682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a verb belongs to any coercion-related VerbNet classes\n",
    "def is_coercion_verb(verb):\n",
    "    coercion_classes = {\n",
    "        'urge-58.1', 'force-59', 'forbid-67'\n",
    "    }\n",
    "    \n",
    "    # Get the VerbNet classes for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        #print(synset)\n",
    "        lemma = synset.lemmas()[0]\n",
    "        #print(lemma)\n",
    "        vn_classes = lemma.key().split('%')[0]\n",
    "        vn_class_ids = vn.classids(vn_classes)\n",
    "        #print(vn_classes)\n",
    "        #print(vn_class_ids)\n",
    "        \n",
    "        # Check if any VerbNet class matches the coercion classes\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636e8008-81ea-4398-8179-f62dc0c6ee91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to both datasets\n",
    "df_train_ready_merged['coercion'] = df_train_ready_merged.apply(determine_coercion, axis=1)\n",
    "df_valid_ready_merged['coercion'] = df_valid_ready_merged.apply(determine_coercion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce37fd5-c0c7-4803-b730-ba2283e7e6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coercion\n",
       "False    4679\n",
       "True      353\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"coercion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e151c5a-eda9-4e12-8539-ca0d089dec54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coercion\n",
       "False    509\n",
       "True      41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"coercion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271831c0-82c8-45e8-be5d-be8f88d3276d",
   "metadata": {},
   "source": [
    "## Label rows with coercion \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de490b1b-3186-4ec5-b589-2144cbd5a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where coercion is True in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['coercion'] == True, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where coercion is True in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['coercion'] == True, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59c24d-5c1c-4e34-bf9e-26df02f7444e",
   "metadata": {},
   "source": [
    "## Negation sucks - redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2dea03-e3b0-4e0d-8281-390515e2fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(dependencies):\n",
    "    negations = [word for word, head, deprel in dependencies if deprel == 'advmod' and (word == 'not' or word == 'n’t')]\n",
    "    return negations\n",
    "\n",
    "df_train_ready_merged['negations'] = df_train_ready_merged['dependencies'].apply(handle_negation)\n",
    "df_valid_ready_merged['negations'] = df_valid_ready_merged['dependencies'].apply(handle_negation)\n",
    "\n",
    "def adjust_sentiment_for_negation(row):\n",
    "    if row['negations']:\n",
    "        if row['final_sentiment'] == \"positive\":\n",
    "            return \"negative\"\n",
    "        elif row['final_sentiment'] == \"negative\":\n",
    "            return \"positive\"\n",
    "    return row['final_sentiment']\n",
    "\n",
    "df_train_ready_merged['final_sentiment_adj'] = df_train_ready_merged.apply(adjust_sentiment_for_negation, axis=1)\n",
    "df_valid_ready_merged['final_sentiment_adj'] = df_valid_ready_merged.apply(adjust_sentiment_for_negation, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307e4da-b2ee-43dc-a32c-9481927a0f13",
   "metadata": {},
   "source": [
    "## Step 5. Final Classification (Blame/Praise/Others)\n",
    "\n",
    "0 - neutral, 1 - positive, 2 - negative - Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "495ef68c-5e2a-42b8-935d-fc387aa052aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>swn_scores</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>subj_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>negations</th>\n",
       "      <th>final_sentiment_adj</th>\n",
       "      <th>events</th>\n",
       "      <th>agent_causality</th>\n",
       "      <th>foreseeability</th>\n",
       "      <th>coercion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "      <td>(0.0, 0.75)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[{'verb': 'supporting', 'object': 'Clinton', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence  Label Final_Result  \\\n",
       "0  George is not supporting Clinton.      2         None   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "\n",
       "                                entities  \\\n",
       "0  [(George, PERSON), (Clinton, PERSON)]   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "\n",
       "                                        dependencies   swn_scores  \\\n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  (0.0, 0.75)   \n",
       "\n",
       "   afinn_score subj_scores final_sentiment negations final_sentiment_adj  \\\n",
       "0          1.0      (1, 0)        positive     [not]            negative   \n",
       "\n",
       "                                              events  agent_causality  \\\n",
       "0  [{'verb': 'supporting', 'object': 'Clinton', '...             True   \n",
       "\n",
       "   foreseeability  coercion  \n",
       "0            True     False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2b082ab-f8d5-4ca3-8423-206847754851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(row):\n",
    "    event_present = row['events'] is not None and len(row['events']) > 0\n",
    "    agent_causality = row['agent_causality']\n",
    "    foreseeability = row['foreseeability']\n",
    "    coercion = row['coercion']\n",
    "    final_sentiment = row['final_sentiment_adj']  # Assuming this is precomputed as positive, negative, or neutral\n",
    "    final_result = row['Final_Result']\n",
    "    \n",
    "    if final_result == None:\n",
    "        if agent_causality and foreseeability and not coercion:\n",
    "            #print(row['events'][0].get('agent'))\n",
    "            if final_sentiment == 'negative':\n",
    "                if row['events'][0].get('agent') == \"I\":\n",
    "                    #print(row['events'][0].get('agent'))\n",
    "                    return \"self-blame\"\n",
    "                else:\n",
    "                    return \"blame-others\"\n",
    "            elif final_sentiment == 'positive':\n",
    "                if row['events'][0].get('agent') == \"I\":\n",
    "                    #print(row['events'][0].get('agent'))\n",
    "                    return \"self-praise\"\n",
    "                else:\n",
    "                    return \"praise-others\"\n",
    "            if final_sentiment == 'neutral':\n",
    "                return \"others\"\n",
    "        else: return \"others\"\n",
    "    else:\n",
    "        return \"others\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2acde65-7380-4e75-96fb-7ce02bc39383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the classification to each row in the DataFrame\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(classify_sentence, axis=1)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(classify_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4672b2c0-d9bd-4486-9f1b-47409dba4359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>swn_scores</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>subj_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>negations</th>\n",
       "      <th>final_sentiment_adj</th>\n",
       "      <th>events</th>\n",
       "      <th>agent_causality</th>\n",
       "      <th>foreseeability</th>\n",
       "      <th>coercion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>blame-others</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "      <td>(0.0, 0.75)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[{'verb': 'supporting', 'object': 'Clinton', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence  Label  Final_Result  \\\n",
       "0  George is not supporting Clinton.      2  blame-others   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "\n",
       "                                entities  \\\n",
       "0  [(George, PERSON), (Clinton, PERSON)]   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "\n",
       "                                        dependencies   swn_scores  \\\n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  (0.0, 0.75)   \n",
       "\n",
       "   afinn_score subj_scores final_sentiment negations final_sentiment_adj  \\\n",
       "0          1.0      (1, 0)        positive     [not]            negative   \n",
       "\n",
       "                                              events  agent_causality  \\\n",
       "0  [{'verb': 'supporting', 'object': 'Clinton', '...             True   \n",
       "\n",
       "   foreseeability  coercion  \n",
       "0            True     False  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a2f83aa-cf27-47a4-bc38-62decfcc66a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>blame-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan has endorsed Trump and told reporters thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>praise-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John McGraw, 78, was charged with assault and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Filipino fighter unleashed a dazzling comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But the Marlins have failed to make the postse...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label   Final_Result\n",
       "0                  George is not supporting Clinton.      2   blame-others\n",
       "1  Ryan has endorsed Trump and told reporters thi...      1  praise-others\n",
       "2  John McGraw, 78, was charged with assault and ...      2         others\n",
       "3  The Filipino fighter unleashed a dazzling comb...      1         others\n",
       "4  But the Marlins have failed to make the postse...      0         others"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[['Sentence', 'Label', 'Final_Result']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "609bc38e-41d9-4aa0-a33f-08753e1c3805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "others           4512\n",
       "blame-others      282\n",
       "praise-others     238\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"Final_Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07696d4b-4fbc-4936-a593-9262710860fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actress Patty Duke died on Tuesday at age 69, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who showrunner, Moffat, will give a masterclas...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Patriots keeping Brady on the bench after ...</td>\n",
       "      <td>0</td>\n",
       "      <td>praise-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia accounted for about 9 percent of Totals...</td>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An Post survey suggests a majority of voters t...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label   Final_Result\n",
       "0  Actress Patty Duke died on Tuesday at age 69, ...      0         others\n",
       "1  Who showrunner, Moffat, will give a masterclas...      0         others\n",
       "2  The Patriots keeping Brady on the bench after ...      0  praise-others\n",
       "3  Russia accounted for about 9 percent of Totals...      2         others\n",
       "4  An Post survey suggests a majority of voters t...      0         others"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c7f8f0-19fb-4a79-8155-e21eb92ee82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "others           480\n",
       "blame-others      45\n",
       "praise-others     25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"Final_Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80232b5-0c37-466e-9c42-8022250cea6e",
   "metadata": {},
   "source": [
    "## Step 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427d92c-2bc0-43d1-9e4a-e7ac7590985c",
   "metadata": {},
   "source": [
    "### Map values in Final_Result column to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d74c33e-c339-4f34-9040-a776c93b3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_16104\\2176863739.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_16104\\2176863739.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {\"others\": 0, \"blame-others\": 2, \"praise-others\": 1}\n",
    "\n",
    "# 0 - neutral, 1 - praise, 2 - blame\n",
    "\n",
    "# Apply the mapping to the 'Final_Result' column\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ee14185-795a-4684-92e8-06358fb39bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval = df_train_ready_merged[['Sentence', 'Label', 'Final_Result']]\n",
    "df_valid_eval = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4565ebbc-4c32-484d-b540-e77af387f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 20 random rows from each DataFrame\n",
    "df_train_sample = df_train_ready_merged.sample(n=20, random_state=1)\n",
    "df_valid_sample = df_valid_ready_merged.sample(n=20, random_state=1)\n",
    "\n",
    "# Save them to an Excel file with different sheets\n",
    "with pd.ExcelWriter('sampled_data.xlsx') as writer:\n",
    "    df_train_sample.to_excel(writer, sheet_name='Train_Sample', index=False)\n",
    "    df_valid_sample.to_excel(writer, sheet_name='Valid_Sample', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e40a5-4319-4f03-921d-cd7c3953eb84",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81a296a3-fa30-49fa-a7f1-701772bc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_train = df_train_eval['Label']\n",
    "y_pred_train = df_train_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29b674ec-f8c7-4c94-a771-73660a1670f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.565382       0.363339          0.475443\n",
      "1  Precision       0.565382       0.537041               NaN\n",
      "2     Recall       0.565382       0.391648               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.94      0.71      2733\n",
      "           1       0.36      0.11      0.17       798\n",
      "           2       0.68      0.13      0.22      1501\n",
      "\n",
      "    accuracy                           0.57      5032\n",
      "   macro avg       0.54      0.39      0.36      5032\n",
      "weighted avg       0.57      0.57      0.48      5032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "f1_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "f1_weighted = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_train, y_pred_train, average='micro')\n",
    "precision_macro = precision_score(y_true_train, y_pred_train, average='macro')\n",
    "recall_micro = recall_score(y_true_train, y_pred_train, average='micro')\n",
    "recall_macro = recall_score(y_true_train, y_pred_train, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46642122-b89b-49fc-9463-56e7b5c447c8",
   "metadata": {},
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8fa5b22-60a0-4dcb-b334-223b84179568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_valid = df_valid_eval['Label']\n",
    "y_pred_valid = df_valid_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dfaea35-0810-4955-870e-75195f16a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.589091       0.400254          0.514723\n",
      "1  Precision       0.589091       0.552778               NaN\n",
      "2     Recall       0.589091       0.412998               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.93      0.72       305\n",
      "           1       0.40      0.13      0.19        78\n",
      "           2       0.67      0.18      0.28       167\n",
      "\n",
      "    accuracy                           0.59       550\n",
      "   macro avg       0.55      0.41      0.40       550\n",
      "weighted avg       0.59      0.59      0.51       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_valid, y_pred_valid, average='micro')\n",
    "f1_macro = f1_score(y_true_valid, y_pred_valid, average='macro')\n",
    "f1_weighted = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_valid, y_pred_valid, average='micro')\n",
    "precision_macro = precision_score(y_true_valid, y_pred_valid, average='macro')\n",
    "recall_micro = recall_score(y_true_valid, y_pred_valid, average='micro')\n",
    "recall_macro = recall_score(y_true_valid, y_pred_valid, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2f4fd-558a-477f-8e2f-821ce4185d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ccda0-0a42-49b7-a418-ed2725456ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6e69f9-ebee-4e47-b287-159f060a1bdd",
   "metadata": {},
   "source": [
    "# NEED TO CHANGE LABELS OF TEST DATA FILE AND PREPROCESS IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4a4f8-3eca-4225-bf11-1a320bfd5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

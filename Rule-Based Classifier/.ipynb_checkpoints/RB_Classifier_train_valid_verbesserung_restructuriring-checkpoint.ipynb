{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee484548-0f4d-4182-a70f-aef0f0cd5b96",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d02f264-f222-4ffb-be3b-f0097f988b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import ast\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b106f5-091e-4100-af31-f7677c66f1de",
   "metadata": {},
   "source": [
    "# Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1049d0-bd96-4662-988d-0db76fe1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"senses\", \"dependencies\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"]\n",
    "\n",
    "df_train_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_valid_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "# Remove leading and trailing spaces in the \"Sentence\" column\n",
    "df_train_preprocessed['Sentence'] = df_train_preprocessed['Sentence'].str.strip()\n",
    "df_valid_preprocessed['Sentence'] = df_valid_preprocessed['Sentence'].str.strip()\n",
    "\n",
    "# Delete columns \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"\n",
    "df_train_ready = df_train_preprocessed.drop(columns = [\"senses\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "df_valid_ready = df_valid_preprocessed.drop(columns = [\"senses\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "\n",
    "# Shuffle the data\n",
    "#df_train_ready = df_train_preprocessed.sample(frac=1).reset_index(drop=True)\n",
    "#df_valid_ready = df_valid_preprocessed.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade7978-08a0-4c84-be4c-be6ed20f1d04",
   "metadata": {},
   "source": [
    "# Merging Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaba528-c606-4797-aeee-52437715a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {2: 1, 3: 2, 4: 2}\n",
    "\n",
    "# 0 - neutral, 1 - positive, 2 - negative\n",
    "\n",
    "df_train_ready_merged = df_train_ready\n",
    "df_valid_ready_merged = df_valid_ready\n",
    "\n",
    "# Apply the mapping to the 'Label' column\n",
    "df_train_ready_merged['Label'] = df_train_ready_merged['Label'].replace(label_mapping)\n",
    "df_valid_ready_merged['Label'] = df_valid_ready_merged['Label'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a1cf9-5a57-493a-8c3a-7802f92ffba6",
   "metadata": {},
   "source": [
    "# Turning strings back to lists and tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e79989-3230-4c6b-ba9c-17d47543c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(dependencies_str):\n",
    "    # Check if it's a string and if it appears to be in the list of tuples format\n",
    "    if isinstance(dependencies_str, str) and dependencies_str.startswith(\"[\") and dependencies_str.endswith(\"]\"):\n",
    "        try:\n",
    "            # Convert string representation of list back to actual list of tuples\n",
    "            return ast.literal_eval(dependencies_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error parsing: {dependencies_str}\")\n",
    "            raise e\n",
    "    elif isinstance(dependencies_str, list):\n",
    "        # If it's already a list, return as is\n",
    "        return dependencies_str\n",
    "    else:\n",
    "        # If it's another unexpected type, return as is or handle appropriately\n",
    "        return dependencies_str\n",
    "\n",
    "def get_sense(tokens):\n",
    "    #print(tokens)\n",
    "    senses = []\n",
    "    for item in tokens:\n",
    "        #print(item)\n",
    "        if isinstance(item, tuple) and len(item) == 2:\n",
    "            token, pos = item\n",
    "            sense = lesk([token], token)\n",
    "            senses.append((token, sense))\n",
    "        else:\n",
    "            # Handle cases where the token doesn't meet the expected structure\n",
    "            print(f\"Unexpected format: {item}\")\n",
    "            senses.append((item, None))\n",
    "    return senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e53857-0db5-4237-b8c7-e742c1fd955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['dependencies'] = df_train_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_valid_ready_merged['dependencies'] = df_valid_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_train_ready_merged['tokens_pos'] = df_train_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_valid_ready_merged['tokens_pos'] = df_valid_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_train_ready_merged['entities'] = df_train_ready_merged['entities'].apply(convert_to_list)\n",
    "df_valid_ready_merged['entities'] = df_valid_ready_merged['entities'].apply(convert_to_list)\n",
    "#df_train_ready_merged['senses'] = df_train_ready_merged['tokens_pos'].apply(get_sense)\n",
    "#df_valid_ready_merged['senses'] = df_valid_ready_merged['tokens_pos'].apply(get_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79357fd-7599-4283-9460-848a8ab4880b",
   "metadata": {},
   "source": [
    "# Making a small set for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610909d0-e497-46f1-b4ed-8a69c46b5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ready_merged_small = df_train_ready_merged.head(10)\n",
    "df_valid_ready_merged_small = df_valid_ready_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5898c-ac22-4229-9a87-3a89b6c3912e",
   "metadata": {},
   "source": [
    "# Following the Modified Algorithm of Blame/Praise Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291d5c-c2b0-4bba-ab48-4fc2faf21902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Older versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b404f2-ab1c-4e08-ae6f-bb832282354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Step 2: Process root verbs\\n    for i, root in enumerate(roots):\\n        root_verb, root_index = root\\n        #print(root_verb, root_index)\\n        # Check if the root verb is valid (foreseeability and not coercion)\\n        for token, pos in tokens_pos:\\n            #print(token, pos)\\n            if token == root_verb and pos == \\'VERB\\':\\n                #print(\"Found VERB\")\\n                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\\n                    #print(\"F and not C\") \\n                    root_verbs.append((root_verb, i+1))\\n        # Check for any conj attached to this root verb and add it if valid\\n        for j, conj in enumerate(dependencies):\\n            conj_word, conj_head, conj_rel = conj\\n            if conj_head == root_index and conj_rel == \\'conj\\':\\n                for token_conj, pos_conj in tokens_pos:\\n                    if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                            root_verbs.append((conj_word, j+1))\\n    \\n    #print(root_verbs)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    # Step 2: Process root verbs\n",
    "    for i, root in enumerate(roots):\n",
    "        root_verb, root_index = root\n",
    "        #print(root_verb, root_index)\n",
    "        # Check if the root verb is valid (foreseeability and not coercion)\n",
    "        for token, pos in tokens_pos:\n",
    "            #print(token, pos)\n",
    "            if token == root_verb and pos == 'VERB':\n",
    "                #print(\"Found VERB\")\n",
    "                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\n",
    "                    #print(\"F and not C\") \n",
    "                    root_verbs.append((root_verb, i+1))\n",
    "        # Check for any conj attached to this root verb and add it if valid\n",
    "        for j, conj in enumerate(dependencies):\n",
    "            conj_word, conj_head, conj_rel = conj\n",
    "            if conj_head == root_index and conj_rel == 'conj':\n",
    "                for token_conj, pos_conj in tokens_pos:\n",
    "                    if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                            root_verbs.append((conj_word, j+1))\n",
    "    \n",
    "    #print(root_verbs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0d40cf-2090-4b6a-bcaf-7278232587ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    \\n    # Lists to store categorized verbs\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for i, dep in enumerate(dependencies):\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, i+1))\\n                # Check for any conj attached to this root verb and add it if valid\\n                for j, conj in enumerate(dependencies):\\n                    conj_word, conj_head, conj_rel = conj\\n                    if conj_head == i+1 and conj_rel == \\'conj\\':\\n                        for token_conj, pos_conj in tokens_pos:\\n                            if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                    root_verbs.append((conj_word, j+1))\\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, i+1))\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for j, conj in enumerate(dependencies):\n",
    "                    conj_word, conj_head, conj_rel = conj\n",
    "                    if conj_head == i+1 and conj_rel == 'conj':\n",
    "                        for token_conj, pos_conj in tokens_pos:\n",
    "                            if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                    root_verbs.append((conj_word, j+1))\n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b412f5aa-2f1f-4e37-ad7d-0e12e665ebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# TRYING WITH COUNTER\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                #print(\"Update Counter I\")\\n                counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any conj attached to this root verb and add it if valid\\n                for conj in dependencies:\\n                    if len(conj) == 3:\\n                        conj_word, conj_head, conj_rel = conj\\n                        counter_j = counter_j +1\\n                        #print(conj_word, counter_j)\\n                        if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if conj_head == counter_i and conj_rel == \\'conj\\':\\n                            for token_conj, pos_conj in tokens_pos:\\n                                if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                        root_verbs.append((conj_word, counter_j))\\n                    \\n                    \\n        \\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(roots)\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TRYING WITH COUNTER\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                #print(\"Update Counter I\")\n",
    "                counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for conj in dependencies:\n",
    "                    if len(conj) == 3:\n",
    "                        conj_word, conj_head, conj_rel = conj\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(conj_word, counter_j)\n",
    "                        if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if conj_head == counter_i and conj_rel == 'conj':\n",
    "                            for token_conj, pos_conj in tokens_pos:\n",
    "                                if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                        root_verbs.append((conj_word, counter_j))\n",
    "                    \n",
    "                    \n",
    "        \n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(roots)\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6632ed-0032-44b0-9830-875c49495651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# TRYING WITH all tags in one for cycle\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n       \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    counter_x = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            \\n            if roots:\\n                if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                    #print(\"Update Counter I\")\\n                    counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any relater words attached to this root verb and add it if valid\\n                for related in dependencies:\\n                    if len(related) == 3:\\n                        related_word, related_head, related_rel = related\\n                        counter_j = counter_j +1\\n                        #print(related_word, counter_j)\\n                        if related_rel == \\'punct\\' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if related_head == counter_i and related_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\', \\'conj\\']:\\n                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\\n                            for token_related, pos_related in tokens_pos:\\n                                if token_related == related_word and \\'VERB\\' in pos_related:\\n                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\\n                                        \\n                                        if related_rel == \\'conj\\':\\n                                            root_verbs.append((related_word, counter_j))\\n                                            \\n                                        elif related_rel == \\'xcomp\\':\\n                                            xcomp_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    xcomp_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'ccomp\\':\\n                                            ccomp_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    ccomp_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'parataxis\\':\\n                                            parataxis_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    parataxis_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'advcl\\':\\n                                            advcl_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    advcl_verbs.append((conj_word, counter_x))\\n    \\n\\n    #print(\"NEW ROW\")\\n    \\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        #print(roots, \" - roots\")\\n        #print(root_verbs, \" - root_verbs\")\\n        #print(xcomp_verbs, \" - xcomp_verbs\")\\n        #print(ccomp_verbs, \" - ccomp_verbs\")\\n        #print(parataxis_verbs, \" - parataxis_verbs\")\\n        #print(advcl_verbs, \" - advcl_verbs\")\\n        #print()\\n        return \\'related\\'\\n    else:\\n        return 0\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# TRYING WITH all tags in one for cycle\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "       \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    counter_x = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            \n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    #print(\"Update Counter I\")\n",
    "                    counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any relater words attached to this root verb and add it if valid\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(related_word, counter_j)\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if related_head == counter_i and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl', 'conj']:\n",
    "                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\n",
    "                            for token_related, pos_related in tokens_pos:\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\n",
    "                                        \n",
    "                                        if related_rel == 'conj':\n",
    "                                            root_verbs.append((related_word, counter_j))\n",
    "                                            \n",
    "                                        elif related_rel == 'xcomp':\n",
    "                                            xcomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    xcomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'ccomp':\n",
    "                                            ccomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    ccomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'parataxis':\n",
    "                                            parataxis_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    parataxis_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'advcl':\n",
    "                                            advcl_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    advcl_verbs.append((conj_word, counter_x))\n",
    "    \n",
    "\n",
    "    #print(\"NEW ROW\")\n",
    "    \n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb86b91-082e-4234-bb2d-3b244ede8e74",
   "metadata": {},
   "source": [
    "## Current Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850365df-dec0-4b8f-a92e-6631d0347707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to check if a verb belongs to Foreseeability or Coercion groups\n",
    "\n",
    "def is_foreseeability_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of foreseeability-related verb classes.\n",
    "    foreseeability_classes = {'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lexname = synset.lexname().split('.')[1]  # Extracts the lexical category (i.e., type of action)\n",
    "        if lexname in foreseeability_classes:  # Checks if the lexical category is in the foreseeability class\n",
    "            return True  # Returns True if the verb matches any foreseeability category\n",
    "    return False  # If no match is found, returns False\n",
    "\n",
    "\n",
    "def is_coercion_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of coercion-related VerbNet classes.\n",
    "    coercion_classes = {'urge-58.1', 'force-59', 'forbid-67'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lemma = synset.lemmas()[0]  # Gets the first lemma for each synset\n",
    "        vn_classes = lemma.key().split('%')[0]  # Extracts the lemma key\n",
    "        vn_class_ids = vn.classids(vn_classes)  # Fetches the VerbNet classes for the lemma\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):  # Checks for a match in coercion classes\n",
    "            return True  # If a match is found in coercion classes, return True\n",
    "    return False  # If no match is found, return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ea980d-d800-4637-a05c-79f56cda0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_one_function(row):\n",
    "    # This is the function to find valid verbs (root, xcomp, ccomp, parataxis, advcl and their conjunctions)\n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence\n",
    "    \n",
    "    counter_i = 0  # Counter for tracking the index of words in the dependency structure\n",
    "    counter_j = 0  # Counter for tracking the index during nested loops\n",
    "    counter_x = 0  # Counter used to track conj words\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "\n",
    "    # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    roots = []  # For root verbs\n",
    "    root_verbs = []  # For valid root verbs (that pass foreseeability and coercion checks)\n",
    "\n",
    "    # (word, own index, head index)\n",
    "    xcomp_verbs = []  # For xcomp verbs\n",
    "    ccomp_verbs = []  # For ccomp verbs\n",
    "    parataxis_verbs = []  # For parataxis verbs\n",
    "    advcl_verbs = []  # For advcl verbs\n",
    "    \n",
    "    # Iterate through dependencies to identify roots and their related verbs\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep  # Unpacking the dependency tuple (word, head, relation)\n",
    "            counter_i += 1  # Increment the index counter for this word\n",
    "            \n",
    "            # If an end of the sentence has been found, reset the counter for punctuation\n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    counter_i = 0  # Reset counter when punctuation is found after root\n",
    "                \n",
    "            # Check if the current word is the root of the sentence\n",
    "            if deprel == 'root':\n",
    "                roots.append((word, counter_i, counter_i))  # Add the root verb and its index\n",
    "                for token, pos in tokens_pos:  # Iterate through POS tokens to find the root as a verb\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            root_verbs.append((word, counter_i, counter_i))  # Add root verb if it passes foreseeability and coercion checks\n",
    "                # looking for related conj\n",
    "                counter_j = 0\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                        counter_j += 1  # Increment the index for the related word\n",
    "                        # Reset the counter for punctuation after root - end of the sentence\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            counter_j = 0\n",
    "                        # Look for conj attached to the verb\n",
    "                        if related_head == counter_i and related_rel in ['conj']:\n",
    "                            roots.append((related_word, counter_j, counter_i))  # Add the root verb and its index\n",
    "                            for token_related, pos_related in tokens_pos:  # Find if the related word is a verb\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        root_verbs.append((related_word, counter_j, counter_i))  # Conj relation to root\n",
    "\n",
    "\n",
    "    \n",
    "    # Find related verbs (xcomp, ccomp, etc.) for root verbs and their conj\n",
    "\n",
    "    for verb in roots:\n",
    "        word, index, head_index = verb\n",
    "        #counter_i += 1\n",
    "\n",
    "        counter_j = 0\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                counter_j += 1  # Increment the index for the related word\n",
    "                \n",
    "                # Reset the counter for punctuation after root - end of the sentence\n",
    "                if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                    counter_j = 0\n",
    "                \n",
    "                # Look for xcomp, ccomp, parataxis or advcl relations attached to the root and its conj\n",
    "                if related_head == index and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found some related word: \", related_rel)\n",
    "                    for token_related, pos_related in tokens_pos:  # Find if the related word is a verb\n",
    "                        if token_related == related_word and 'VERB' in pos_related:\n",
    "                            if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                #print(\"related word passed all checks: \", related_rel)\n",
    "                                # Depending on the relation type, add the related verb to the appropriate list\n",
    "                                #if related_rel == 'conj':\n",
    "                                   # root_verbs.append((related_word, counter_j))  # Conj relation to root\n",
    "                                if related_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append((related_word, counter_j, index))  # xcomp relation to root\n",
    "                                    # Handle conj for xcomp verbs\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                counter_x = 0  # Reset counter for punctuation\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                # Check if the conj word is a valid verb\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            xcomp_verbs.append((conj_word, counter_x, index))  # Conj for xcomp\n",
    "                                \n",
    "                                # Handle ccomp, parataxis, advcl similarly for related verbs and their conjunctions\n",
    "                                elif related_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append((related_word, counter_j, index))  # ccomp relation\n",
    "                                    # Handle conj for ccomp\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            ccomp_verbs.append((conj_word, counter_x, index))  # Conj for ccomp\n",
    "                                \n",
    "                                elif related_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append((related_word, counter_j, index))  # parataxis relation\n",
    "                                    # Handle conj for parataxis\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            parataxis_verbs.append((conj_word, counter_x, index))  # Conj for parataxis\n",
    "                                \n",
    "                                elif related_rel == 'advcl':\n",
    "                                    advcl_verbs.append((related_word, counter_j, index))  # advcl relation\n",
    "                                    # Handle conj for advcl\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            advcl_verbs.append((conj_word, counter_x, index))  # Conj for advcl\n",
    "    #print(\"NEW ROW\")\n",
    "    \n",
    "    # Return the lists of related verbs\n",
    "    #if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "    return roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30dbe389-4669-4a18-9135-f9128335f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_agent_validity(related_word, row, tokens_pos):\n",
    "    \n",
    "    entities = row['entities']\n",
    "    valid_ent_labels = [\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]\n",
    "    valid_additional_words = [\"person\", \"man\", \"woman\", \"police\", \"administration\", \"immigrants\", \"president\", \"minister\", \"senator\", \n",
    "                              \"representative\", \"governor\", \"mayor\", \"council\", \"secretary\", \"ambassador\", \"chancellor\", \"parliamentary\", \"mr.\", \"ms.\", \"mrs.\"]\n",
    "\n",
    "    self = False\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    for entity, label in entities: \n",
    "        if entity in related_word and label in valid_ent_labels: \n",
    "            agent_is_valid = True # is it in NER list?  \n",
    "    if not agent_is_valid and 'PRON' in [pos for token, pos in tokens_pos if token == related_word]: \n",
    "        agent_is_valid = True # is it a promoun?\n",
    "        if related_word.lower() == \"i\" or related_word.lower() == \"we\":\n",
    "            self = True\n",
    "    if not agent_is_valid and related_word.lower() in valid_additional_words: agent_is_valid = True # is it from the list of additional words?\n",
    "\n",
    "    return agent_is_valid, self\n",
    "\n",
    "\n",
    "\n",
    "def check_causative_verb(verb):\n",
    "    # Check if the verb is in the CAUSE class or has CAUSETO relation in WordNet\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def define_polarity(verb, obj):\n",
    "    # Function to define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "    # 0 - others, 1 - positive, 2 - negative\n",
    "\n",
    "    result = 0\n",
    "    \n",
    "    # Create a simple context for WSD\n",
    "    context = f\"{verb} {obj}\"\n",
    "    \n",
    "    # Word Sense Disambiguation for the verb and object\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    # Calculate polarity using SentiWordNet\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    # AFINN score\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    # Subjectivity Lexicon score\n",
    "    tokens = context.split()\n",
    "    subj_pos = sum([1 for token in tokens if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in tokens if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    # Determine final polarity\n",
    "    if pos_score > neg_score:\n",
    "        return 1  # Positive/Praise\n",
    "    elif neg_score > pos_score:\n",
    "        return 2  # Negative/Blame\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "        \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def adjust_sentiment_for_negation(row, polarity, verb):\n",
    "    word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    dependencies = row['dependencies']\n",
    "\n",
    "    for related in dependencies:\n",
    "        if len(related) == 3:\n",
    "            related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "            if related_head == index and related_rel in ['advmod'] and (related_word == 'not' or related_word == 'n’t'):\n",
    "                if polarity == 0:\n",
    "                    return 0\n",
    "                if polarity == 1:\n",
    "                    return 2\n",
    "                if polarity == 2:\n",
    "                    return 1\n",
    "    \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01c3852-ef74-496c-aeb5-95f01c929ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs):\n",
    "    \n",
    "     # This is the function to decide on Agent Causality, find the object, decide on Polarity and classify the row \n",
    "    \n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence  \n",
    "\n",
    "    self = False\n",
    "    result = None\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    # Сonnection 1: nsubj / nsubj:pass - root_verb - obj / iobj / obl - by priority\n",
    "    for verb in root_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "        \n",
    "\n",
    "    \n",
    "    # Сonnection 2: nsubj / nsubj:pass - root_verb - xcomp - obj / iobj / obl - by priority\n",
    "    for verb in xcomp_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "\n",
    "    # Сonnection 3: nsubj / nsubj:pass - parataxis_verbs - (xcomp) - obj / iobj / obl - by priority    \n",
    "    for verb in parataxis_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with parataxis\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "\n",
    "\n",
    "\n",
    "    # Сonnection 4: nsubj / nsubj:pass - advcl_verbs - (xcomp) - obj / iobj / obl - by priority   \n",
    "    for verb in advcl_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with advcl\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "\n",
    "    \n",
    "    \n",
    "    # Checking for other possible connections\n",
    "    # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    # (word, own index, head index)\n",
    "    \n",
    "    # АГЕНТ МБ ПРИСОЕДИНЕН К ROOT, А ОБЪЕКТ К ВСПОМОГАТЕЛЬНОМУ ТЕГУ\n",
    "    # ИСКАТЬ СУБЪЕКТ И К СЕБЕ И К ROOT ПРИСОЕДИНЕННОМУ\n",
    "    # ОБЪЕКТ ИЩЕМ ПРИСОЕДИНЕННЫЙ К СЕБЕ\n",
    "\n",
    "    # Сonnection 5: nsubj - ccomp_verbs - (xcomp) - obj / iobj / obl - by priority   \n",
    "    for verb in ccomp_verbs: \n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                #else:\n",
    "                    #if related_head == head_index and related_rel in ['nsubj']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        #agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with advcl\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # Сonnection 6: nsubj:pass - ccomp_verbs - (xcomp) - obl:agent / obl - by priority   \n",
    "    for verb in ccomp_verbs: \n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['obl:agent', 'obl']: # is that an agent?\n",
    "                    #print(\"Marvel with ccomp passive happened\")\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                #else:\n",
    "                   #if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                       #agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            #print(\"Even Agent is valid\")\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                        \n",
    "                    if related_head == index and related_rel in ['nsubj:pass']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e4e0cf-d699-42a2-968b-063783d3a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_verbs(row):\n",
    "    \n",
    "    # This is the main function to process each row of data and classify the row \n",
    "\n",
    "    # 1 - Find all the related verbs in categories in dependency column 'root', 'xcomp', 'ccomp', 'parataxis', 'advcl', 'conj' (is a verb check - foreseeability check - coercion check)\n",
    "    roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs = step_one_function(row)\n",
    "    \n",
    "    # 2 - If at least one of the lists is not empty - can proceed\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        \n",
    "        # 3 - Take a final decision about the label (0 - others, 1 - positive, 2 - negative)\n",
    "        return step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs)\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314989cf-1c6a-4635-b61b-e4ec36054046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_20128\\4059228178.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_20128\\4059228178.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the small dataset\n",
    "df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c233a97e-f4ff-434f-8f97-5764a84500c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan has endorsed Trump and told reporters thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Ryan, PROPN), (has, AUX), (endorsed, VERB), ...</td>\n",
       "      <td>[(Ryan, PERSON), (Trump, PERSON), (this past w...</td>\n",
       "      <td>[(Ryan, 3, nsubj), (has, 3, aux), (endorsed, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John McGraw, 78, was charged with assault and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[(John, PROPN), (McGraw, PROPN), (,, PUNCT), (...</td>\n",
       "      <td>[(John McGraw, PERSON), (78, DATE), (Thursday,...</td>\n",
       "      <td>[(John, 7, nsubj:pass), (McGraw, 1, flat), (,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Filipino fighter unleashed a dazzling comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(The, DET), (Filipino, ADJ), (fighter, NOUN),...</td>\n",
       "      <td>[(Filipino, NORP), (Margarito, PERSON), (Maywe...</td>\n",
       "      <td>[(The, 3, det), (Filipino, 3, amod), (fighter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But the Marlins have failed to make the postse...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(But, CCONJ), (the, DET), (Marlins, PROPN), (...</td>\n",
       "      <td>[(Marlins, ORG), (Loria, PERSON)]</td>\n",
       "      <td>[(But, 5, cc), (the, 3, det), (Marlins, 5, nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>So shortly after 2 a.m., campaign chairman Joh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(So, ADV), (shortly, ADV), (after, ADP), (2, ...</td>\n",
       "      <td>[(2 a.m., TIME), (John Podesta, PERSON), (Clin...</td>\n",
       "      <td>[(So, 11, advmod), (shortly, 4, advmod), (afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Church has not answered the allegations ot...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[(The, DET), (Church, PROPN), (has, AUX), (not...</td>\n",
       "      <td>[(Church, ORG), (Navajo, GPE)]</td>\n",
       "      <td>[(The, 2, det), (Church, 5, nsubj), (has, 5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bruno Beschizza, the conservative mayor of has...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Bruno, PROPN), (Beschizza, PROPN), (,, PUNCT...</td>\n",
       "      <td>[(Bruno Beschizza, PERSON), (Théo, PERSON)]</td>\n",
       "      <td>[(Bruno, 9, nsubj), (Beschizza, 1, flat), (,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>On criminal justice issues, Harris faults Carp...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(On, ADP), (criminal, ADJ), (justice, NOUN), ...</td>\n",
       "      <td>[(Harris, PERSON), (Carper, PERSON), (the 1990...</td>\n",
       "      <td>[(On, 4, case), (criminal, 3, amod), (justice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Even the hologame that Finn starts up on the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Even, ADV), (the, DET), (hologame, NOUN), (t...</td>\n",
       "      <td>[(Finn, PERSON), (Chewbacca, PERSON), (A New H...</td>\n",
       "      <td>[(Even, 3, advmod), (the, 3, det), (hologame, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  Final_Result  \\\n",
       "0                  George is not supporting Clinton.      2             2   \n",
       "1  Ryan has endorsed Trump and told reporters thi...      1             1   \n",
       "2  John McGraw, 78, was charged with assault and ...      2             0   \n",
       "3  The Filipino fighter unleashed a dazzling comb...      1             0   \n",
       "4  But the Marlins have failed to make the postse...      0             0   \n",
       "5  So shortly after 2 a.m., campaign chairman Joh...      0             0   \n",
       "6  The Church has not answered the allegations ot...      0             1   \n",
       "7  Bruno Beschizza, the conservative mayor of has...      0             0   \n",
       "8  On criminal justice issues, Harris faults Carp...      2             2   \n",
       "9  Even the hologame that Finn starts up on the s...      0             0   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "1  [(Ryan, PROPN), (has, AUX), (endorsed, VERB), ...   \n",
       "2  [(John, PROPN), (McGraw, PROPN), (,, PUNCT), (...   \n",
       "3  [(The, DET), (Filipino, ADJ), (fighter, NOUN),...   \n",
       "4  [(But, CCONJ), (the, DET), (Marlins, PROPN), (...   \n",
       "5  [(So, ADV), (shortly, ADV), (after, ADP), (2, ...   \n",
       "6  [(The, DET), (Church, PROPN), (has, AUX), (not...   \n",
       "7  [(Bruno, PROPN), (Beschizza, PROPN), (,, PUNCT...   \n",
       "8  [(On, ADP), (criminal, ADJ), (justice, NOUN), ...   \n",
       "9  [(Even, ADV), (the, DET), (hologame, NOUN), (t...   \n",
       "\n",
       "                                            entities  \\\n",
       "0              [(George, PERSON), (Clinton, PERSON)]   \n",
       "1  [(Ryan, PERSON), (Trump, PERSON), (this past w...   \n",
       "2  [(John McGraw, PERSON), (78, DATE), (Thursday,...   \n",
       "3  [(Filipino, NORP), (Margarito, PERSON), (Maywe...   \n",
       "4                  [(Marlins, ORG), (Loria, PERSON)]   \n",
       "5  [(2 a.m., TIME), (John Podesta, PERSON), (Clin...   \n",
       "6                     [(Church, ORG), (Navajo, GPE)]   \n",
       "7        [(Bruno Beschizza, PERSON), (Théo, PERSON)]   \n",
       "8  [(Harris, PERSON), (Carper, PERSON), (the 1990...   \n",
       "9  [(Finn, PERSON), (Chewbacca, PERSON), (A New H...   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  \n",
       "1  [(Ryan, 3, nsubj), (has, 3, aux), (endorsed, 0...  \n",
       "2  [(John, 7, nsubj:pass), (McGraw, 1, flat), (,,...  \n",
       "3  [(The, 3, det), (Filipino, 3, amod), (fighter,...  \n",
       "4  [(But, 5, cc), (the, 3, det), (Marlins, 5, nsu...  \n",
       "5  [(So, 11, advmod), (shortly, 4, advmod), (afte...  \n",
       "6  [(The, 2, det), (Church, 5, nsubj), (has, 5, a...  \n",
       "7  [(Bruno, 9, nsubj), (Beschizza, 1, flat), (,, ...  \n",
       "8  [(On, 4, case), (criminal, 3, amod), (justice,...  \n",
       "9  [(Even, 3, advmod), (the, 3, det), (hologame, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33375c99-60a7-4f95-a5d1-87d16c49c0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the dataset\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged = df_train_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "915deb00-52fe-4572-b9e7-5a7f68e35cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0           3701\n",
       "2            721\n",
       "1            562\n",
       "self - 1      31\n",
       "self - 2      17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11d1c91f-3ad7-4025-a5fe-671a593c81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0           416\n",
       "2            82\n",
       "1            49\n",
       "self - 2      2\n",
       "self - 1      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc10ca-614a-4f59-b8bd-2ecd6d0c916f",
   "metadata": {},
   "source": [
    "## Save to Excel random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca906596-78b7-4b4b-8426-eccb6403ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 20 random rows from each DataFrame\n",
    "df_train_sample = df_train_ready_merged.sample(n=200, random_state=42)\n",
    "df_valid_sample = df_valid_ready_merged.sample(n=200, random_state=42)\n",
    "\n",
    "# Save them to an Excel file with different sheets\n",
    "with pd.ExcelWriter('sampled_data_2.xlsx') as writer:\n",
    "    df_train_sample.to_excel(writer, sheet_name='Train_Sample', index=False)\n",
    "    df_valid_sample.to_excel(writer, sheet_name='Valid_Sample', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80232b5-0c37-466e-9c42-8022250cea6e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427d92c-2bc0-43d1-9e4a-e7ac7590985c",
   "metadata": {},
   "source": [
    "## Map values in Final_Result column to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d74c33e-c339-4f34-9040-a776c93b3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_20128\\908398405.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_20128\\908398405.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {\"self - 1\": 1, \"self - 2\": 2}\n",
    "\n",
    "# 0 - neutral, 1 - praise, 2 - blame\n",
    "\n",
    "# Apply the mapping to the 'Final_Result' column\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18124c1a-5036-4314-b0fa-5da17bead4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0    3701\n",
       "2     738\n",
       "1     593\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef282da6-6eb9-4226-a64e-22840708b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0    416\n",
       "2     84\n",
       "1     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ee14185-795a-4684-92e8-06358fb39bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval = df_train_ready_merged[['Sentence', 'Label', 'Final_Result']]\n",
    "df_valid_eval = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e40a5-4319-4f03-921d-cd7c3953eb84",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a296a3-fa30-49fa-a7f1-701772bc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_train = df_train_eval['Label']\n",
    "y_pred_train = df_train_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b674ec-f8c7-4c94-a771-73660a1670f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.566375       0.459475          0.535049\n",
      "1  Precision       0.566375       0.512161               NaN\n",
      "2     Recall       0.566375       0.454808               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69      2733\n",
      "           1       0.37      0.28      0.32       798\n",
      "           2       0.57      0.28      0.37      1501\n",
      "\n",
      "    accuracy                           0.57      5032\n",
      "   macro avg       0.51      0.45      0.46      5032\n",
      "weighted avg       0.55      0.57      0.54      5032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "f1_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "f1_weighted = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_train, y_pred_train, average='micro')\n",
    "precision_macro = precision_score(y_true_train, y_pred_train, average='macro')\n",
    "recall_micro = recall_score(y_true_train, y_pred_train, average='micro')\n",
    "recall_macro = recall_score(y_true_train, y_pred_train, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46642122-b89b-49fc-9463-56e7b5c447c8",
   "metadata": {},
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8fa5b22-60a0-4dcb-b334-223b84179568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_valid = df_valid_eval['Label']\n",
    "y_pred_valid = df_valid_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dfaea35-0810-4955-870e-75195f16a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.574545       0.466072          0.541533\n",
      "1  Precision       0.574545       0.527924               NaN\n",
      "2     Recall       0.574545       0.456872               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69       305\n",
      "           1       0.44      0.28      0.34        78\n",
      "           2       0.55      0.28      0.37       167\n",
      "\n",
      "    accuracy                           0.57       550\n",
      "   macro avg       0.53      0.46      0.47       550\n",
      "weighted avg       0.56      0.57      0.54       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_valid, y_pred_valid, average='micro')\n",
    "f1_macro = f1_score(y_true_valid, y_pred_valid, average='macro')\n",
    "f1_weighted = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_valid, y_pred_valid, average='micro')\n",
    "precision_macro = precision_score(y_true_valid, y_pred_valid, average='macro')\n",
    "recall_micro = recall_score(y_true_valid, y_pred_valid, average='micro')\n",
    "recall_macro = recall_score(y_true_valid, y_pred_valid, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2f4fd-558a-477f-8e2f-821ce4185d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ccda0-0a42-49b7-a418-ed2725456ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6e69f9-ebee-4e47-b287-159f060a1bdd",
   "metadata": {},
   "source": [
    "# NEED TO CHANGE LABELS OF TEST DATA FILE AND PREPROCESS IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4a4f8-3eca-4225-bf11-1a320bfd5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee484548-0f4d-4182-a70f-aef0f0cd5b96",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d02f264-f222-4ffb-be3b-f0097f988b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import ast\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b106f5-091e-4100-af31-f7677c66f1de",
   "metadata": {},
   "source": [
    "# Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1049d0-bd96-4662-988d-0db76fe1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"senses\", \"dependencies\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"]\n",
    "\n",
    "df_train_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_valid_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "# Remove leading and trailing spaces in the \"Sentence\" column\n",
    "df_train_preprocessed['Sentence'] = df_train_preprocessed['Sentence'].str.strip()\n",
    "df_valid_preprocessed['Sentence'] = df_valid_preprocessed['Sentence'].str.strip()\n",
    "\n",
    "# Delete columns \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"\n",
    "df_train_ready = df_train_preprocessed.drop(columns = [\"senses\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "df_valid_ready = df_valid_preprocessed.drop(columns = [\"senses\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"])\n",
    "\n",
    "# Shuffle the data\n",
    "#df_train_ready = df_train_preprocessed.sample(frac=1).reset_index(drop=True)\n",
    "#df_valid_ready = df_valid_preprocessed.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade7978-08a0-4c84-be4c-be6ed20f1d04",
   "metadata": {},
   "source": [
    "# Merging Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaba528-c606-4797-aeee-52437715a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {2: 1, 3: 2, 4: 2}\n",
    "\n",
    "# 0 - neutral, 1 - positive, 2 - negative\n",
    "\n",
    "df_train_ready_merged = df_train_ready\n",
    "df_valid_ready_merged = df_valid_ready\n",
    "\n",
    "# Apply the mapping to the 'Label' column\n",
    "df_train_ready_merged['Label'] = df_train_ready_merged['Label'].replace(label_mapping)\n",
    "df_valid_ready_merged['Label'] = df_valid_ready_merged['Label'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a1cf9-5a57-493a-8c3a-7802f92ffba6",
   "metadata": {},
   "source": [
    "# Turning strings back to lists and tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e79989-3230-4c6b-ba9c-17d47543c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(dependencies_str):\n",
    "    # Check if it's a string and if it appears to be in the list of tuples format\n",
    "    if isinstance(dependencies_str, str) and dependencies_str.startswith(\"[\") and dependencies_str.endswith(\"]\"):\n",
    "        try:\n",
    "            # Convert string representation of list back to actual list of tuples\n",
    "            return ast.literal_eval(dependencies_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error parsing: {dependencies_str}\")\n",
    "            raise e\n",
    "    elif isinstance(dependencies_str, list):\n",
    "        # If it's already a list, return as is\n",
    "        return dependencies_str\n",
    "    else:\n",
    "        # If it's another unexpected type, return as is or handle appropriately\n",
    "        return dependencies_str\n",
    "\n",
    "def get_sense(tokens):\n",
    "    #print(tokens)\n",
    "    senses = []\n",
    "    for item in tokens:\n",
    "        #print(item)\n",
    "        if isinstance(item, tuple) and len(item) == 2:\n",
    "            token, pos = item\n",
    "            sense = lesk([token], token)\n",
    "            senses.append((token, sense))\n",
    "        else:\n",
    "            # Handle cases where the token doesn't meet the expected structure\n",
    "            print(f\"Unexpected format: {item}\")\n",
    "            senses.append((item, None))\n",
    "    return senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e53857-0db5-4237-b8c7-e742c1fd955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['dependencies'] = df_train_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_valid_ready_merged['dependencies'] = df_valid_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_train_ready_merged['tokens_pos'] = df_train_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_valid_ready_merged['tokens_pos'] = df_valid_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_train_ready_merged['entities'] = df_train_ready_merged['entities'].apply(convert_to_list)\n",
    "df_valid_ready_merged['entities'] = df_valid_ready_merged['entities'].apply(convert_to_list)\n",
    "#df_train_ready_merged['senses'] = df_train_ready_merged['tokens_pos'].apply(get_sense)\n",
    "#df_valid_ready_merged['senses'] = df_valid_ready_merged['tokens_pos'].apply(get_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79357fd-7599-4283-9460-848a8ab4880b",
   "metadata": {},
   "source": [
    "# Making a small set for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610909d0-e497-46f1-b4ed-8a69c46b5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ready_merged_small = df_train_ready_merged.head(10)\n",
    "df_valid_ready_merged_small = df_valid_ready_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5898c-ac22-4229-9a87-3a89b6c3912e",
   "metadata": {},
   "source": [
    "# Following the Modified Algorithm of Blame/Praise Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291d5c-c2b0-4bba-ab48-4fc2faf21902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Older versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b404f2-ab1c-4e08-ae6f-bb832282354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Step 2: Process root verbs\\n    for i, root in enumerate(roots):\\n        root_verb, root_index = root\\n        #print(root_verb, root_index)\\n        # Check if the root verb is valid (foreseeability and not coercion)\\n        for token, pos in tokens_pos:\\n            #print(token, pos)\\n            if token == root_verb and pos == \\'VERB\\':\\n                #print(\"Found VERB\")\\n                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\\n                    #print(\"F and not C\") \\n                    root_verbs.append((root_verb, i+1))\\n        # Check for any conj attached to this root verb and add it if valid\\n        for j, conj in enumerate(dependencies):\\n            conj_word, conj_head, conj_rel = conj\\n            if conj_head == root_index and conj_rel == \\'conj\\':\\n                for token_conj, pos_conj in tokens_pos:\\n                    if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                            root_verbs.append((conj_word, j+1))\\n    \\n    #print(root_verbs)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    # Step 2: Process root verbs\n",
    "    for i, root in enumerate(roots):\n",
    "        root_verb, root_index = root\n",
    "        #print(root_verb, root_index)\n",
    "        # Check if the root verb is valid (foreseeability and not coercion)\n",
    "        for token, pos in tokens_pos:\n",
    "            #print(token, pos)\n",
    "            if token == root_verb and pos == 'VERB':\n",
    "                #print(\"Found VERB\")\n",
    "                if is_foreseeability_verb(root_verb) and not is_coercion_verb(root_verb):\n",
    "                    #print(\"F and not C\") \n",
    "                    root_verbs.append((root_verb, i+1))\n",
    "        # Check for any conj attached to this root verb and add it if valid\n",
    "        for j, conj in enumerate(dependencies):\n",
    "            conj_word, conj_head, conj_rel = conj\n",
    "            if conj_head == root_index and conj_rel == 'conj':\n",
    "                for token_conj, pos_conj in tokens_pos:\n",
    "                    if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                        if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                            root_verbs.append((conj_word, j+1))\n",
    "    \n",
    "    #print(root_verbs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0d40cf-2090-4b6a-bcaf-7278232587ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    \\n    # Lists to store categorized verbs\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for i, dep in enumerate(dependencies):\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, i+1))\\n                # Check for any conj attached to this root verb and add it if valid\\n                for j, conj in enumerate(dependencies):\\n                    conj_word, conj_head, conj_rel = conj\\n                    if conj_head == i+1 and conj_rel == \\'conj\\':\\n                        for token_conj, pos_conj in tokens_pos:\\n                            if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                    root_verbs.append((conj_word, j+1))\\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, i+1))\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for j, conj in enumerate(dependencies):\n",
    "                    conj_word, conj_head, conj_rel = conj\n",
    "                    if conj_head == i+1 and conj_rel == 'conj':\n",
    "                        for token_conj, pos_conj in tokens_pos:\n",
    "                            if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                    root_verbs.append((conj_word, j+1))\n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b412f5aa-2f1f-4e37-ad7d-0e12e665ebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# TRYING WITH COUNTER\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n    print(\"NEW ROW\")\\n    \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    # Step 1: Identify all root verbs and their indices\\n    #roots = []\\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                #print(\"Update Counter I\")\\n                counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any conj attached to this root verb and add it if valid\\n                for conj in dependencies:\\n                    if len(conj) == 3:\\n                        conj_word, conj_head, conj_rel = conj\\n                        counter_j = counter_j +1\\n                        #print(conj_word, counter_j)\\n                        if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if conj_head == counter_i and conj_rel == \\'conj\\':\\n                            for token_conj, pos_conj in tokens_pos:\\n                                if token_conj == conj_word and \\'VERB\\' in pos_conj:\\n                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                        root_verbs.append((conj_word, counter_j))\\n                    \\n                    \\n        \\n    #print(root_verbs)\\n\\n    \\n    # TILL HERE WORKS \\n    \\n    # ВОПРОС С БУМАЖКИ\\n    \\n    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\\n    for root_verb, root_index in root_verbs:\\n        for i, dep in enumerate(dependencies):\\n            if len(dep) == 3:\\n                dep_word, dep_head, dep_rel = dep\\n                \\n                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\\n                #print(dep_head, root_index, dep_rel)\\n                if dep_head == root_index and dep_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\']:\\n                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\\n                    #print(dep_word, dep_head, dep_rel)\\n                    for token, pos in tokens_pos:\\n                        if token == dep_word and \\'VERB\\' in pos:\\n                            #print(token, pos)\\n                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\\n                                # Add to the appropriate list based on dep_rel\\n                                if dep_rel == \\'xcomp\\':\\n                                    xcomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'ccomp\\':\\n                                    ccomp_verbs.append(dep_word)\\n                                elif dep_rel == \\'parataxis\\':\\n                                    parataxis_verbs.append(dep_word)\\n                                elif dep_rel == \\'advcl\\':\\n                                    advcl_verbs.append(dep_word)\\n                                \\n                                # Check for any conj attached to this verb and add it\\n                                for conj_word, conj_head, conj_rel in dependencies:\\n                                    if conj_head == i + 1 and conj_rel == \\'conj\\':  # Look for conj attached to the current word\\n                                        # Check if the conj word is a verb and passes the checks\\n                                        for token, pos in tokens_pos:\\n                                            if token == conj_word and \\'VERB\\' in pos:\\n                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\\n                                                    if dep_rel == \\'xcomp\\':\\n                                                        xcomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'ccomp\\':\\n                                                        ccomp_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'parataxis\\':\\n                                                        parataxis_verbs.append(conj_word)\\n                                                    elif dep_rel == \\'advcl\\':\\n                                                        advcl_verbs.append(conj_word)\\n                                #break\\n    \\n    # Step 4: If any of the verb lists are not empty, assign to related category\\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        print(roots)\\n        print(root_verbs)\\n        print(xcomp_verbs)\\n        print(ccomp_verbs)\\n        print(parataxis_verbs)\\n        print(advcl_verbs)\\n        return \\'related\\'\\n    else:\\n        return \\'others\\'\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TRYING WITH COUNTER\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "    print(\"NEW ROW\")\n",
    "    \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    # Step 1: Identify all root verbs and their indices\n",
    "    #roots = []\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                #print(\"Update Counter I\")\n",
    "                counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any conj attached to this root verb and add it if valid\n",
    "                for conj in dependencies:\n",
    "                    if len(conj) == 3:\n",
    "                        conj_word, conj_head, conj_rel = conj\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(conj_word, counter_j)\n",
    "                        if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if conj_head == counter_i and conj_rel == 'conj':\n",
    "                            for token_conj, pos_conj in tokens_pos:\n",
    "                                if token_conj == conj_word and 'VERB' in pos_conj:\n",
    "                                    if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                        root_verbs.append((conj_word, counter_j))\n",
    "                    \n",
    "                    \n",
    "        \n",
    "    #print(root_verbs)\n",
    "\n",
    "    \n",
    "    # TILL HERE WORKS \n",
    "    \n",
    "    # ВОПРОС С БУМАЖКИ\n",
    "    \n",
    "    # Step 3: Process each root verb and find related tags (xcomp, ccomp, conj, parataxis, advcl)\n",
    "    for root_verb, root_index in root_verbs:\n",
    "        for i, dep in enumerate(dependencies):\n",
    "            if len(dep) == 3:\n",
    "                dep_word, dep_head, dep_rel = dep\n",
    "                \n",
    "                # Handle each type of relation (xcomp, ccomp, parataxis, advcl)\n",
    "                #print(dep_head, root_index, dep_rel)\n",
    "                if dep_head == root_index and dep_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found one of xcomp, ccomp, parataxis, advcl\")\n",
    "                    #print(dep_word, dep_head, dep_rel)\n",
    "                    for token, pos in tokens_pos:\n",
    "                        if token == dep_word and 'VERB' in pos:\n",
    "                            #print(token, pos)\n",
    "                            if is_foreseeability_verb(dep_word) and not is_coercion_verb(dep_word):\n",
    "                                # Add to the appropriate list based on dep_rel\n",
    "                                if dep_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append(dep_word)\n",
    "                                elif dep_rel == 'advcl':\n",
    "                                    advcl_verbs.append(dep_word)\n",
    "                                \n",
    "                                # Check for any conj attached to this verb and add it\n",
    "                                for conj_word, conj_head, conj_rel in dependencies:\n",
    "                                    if conj_head == i + 1 and conj_rel == 'conj':  # Look for conj attached to the current word\n",
    "                                        # Check if the conj word is a verb and passes the checks\n",
    "                                        for token, pos in tokens_pos:\n",
    "                                            if token == conj_word and 'VERB' in pos:\n",
    "                                                if is_foreseeability_verb(conj_word) and not is_coercion_verb(conj_word):\n",
    "                                                    if dep_rel == 'xcomp':\n",
    "                                                        xcomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'ccomp':\n",
    "                                                        ccomp_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'parataxis':\n",
    "                                                        parataxis_verbs.append(conj_word)\n",
    "                                                    elif dep_rel == 'advcl':\n",
    "                                                        advcl_verbs.append(conj_word)\n",
    "                                #break\n",
    "    \n",
    "    # Step 4: If any of the verb lists are not empty, assign to related category\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        print(roots)\n",
    "        print(root_verbs)\n",
    "        print(xcomp_verbs)\n",
    "        print(ccomp_verbs)\n",
    "        print(parataxis_verbs)\n",
    "        print(advcl_verbs)\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 'others'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6632ed-0032-44b0-9830-875c49495651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# TRYING WITH all tags in one for cycle\\n\\n# The main function to process each sentence\\ndef find_valid_verbs(row):\\n       \\n    dependencies = row[\\'dependencies\\']\\n    tokens_pos = row[\\'tokens_pos\\']\\n\\n    counter_i = 0\\n    counter_j = 0\\n    counter_x = 0\\n    \\n    # Lists to store categorized verbs\\n    roots = []\\n    root_verbs = []\\n    xcomp_verbs = []\\n    ccomp_verbs = []\\n    parataxis_verbs = []\\n    advcl_verbs = []\\n    \\n    for dep in dependencies:\\n        if len(dep) == 3:\\n            word, head, deprel = dep\\n            counter_i = counter_i + 1\\n            #print(word, counter_i)\\n            \\n            if roots:\\n                if deprel == \\'punct\\' and (word == \".\" or word == \":\") and head == roots[0][1]:\\n                    #print(\"Update Counter I\")\\n                    counter_i = 0\\n                \\n            if deprel == \\'root\\':\\n                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\\n                roots.append((word, counter_i))\\n                for token, pos in tokens_pos:\\n                    if token == word and pos == \\'VERB\\':\\n                        #print(\"Found VERB\")\\n                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\\n                            #print(\"F and not C\") \\n                            root_verbs.append((word, counter_i))\\n\\n                counter_j = 0\\n                # Check for any relater words attached to this root verb and add it if valid\\n                for related in dependencies:\\n                    if len(related) == 3:\\n                        related_word, related_head, related_rel = related\\n                        counter_j = counter_j +1\\n                        #print(related_word, counter_j)\\n                        if related_rel == \\'punct\\' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\\n                            #print(\"Update Counter J\")\\n                            counter_j = 0\\n                        if related_head == counter_i and related_rel in [\\'xcomp\\', \\'ccomp\\', \\'parataxis\\', \\'advcl\\', \\'conj\\']:\\n                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\\n                            for token_related, pos_related in tokens_pos:\\n                                if token_related == related_word and \\'VERB\\' in pos_related:\\n                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\\n                                        \\n                                        if related_rel == \\'conj\\':\\n                                            root_verbs.append((related_word, counter_j))\\n                                            \\n                                        elif related_rel == \\'xcomp\\':\\n                                            xcomp_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    xcomp_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'ccomp\\':\\n                                            ccomp_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    ccomp_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'parataxis\\':\\n                                            parataxis_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    parataxis_verbs.append((conj_word, counter_x))\\n                                        \\n                                        elif related_rel == \\'advcl\\':\\n                                            advcl_verbs.append((related_word, counter_j))\\n                                            # найти conj для этого\\n                                            counter_x = 0\\n                                            for conj in dependencies:\\n                                                if len(conj) == 3:\\n                                                    conj_word, conj_head, conj_rel = conj\\n                                                    counter_x = counter_x +1\\n                                                    if conj_rel == \\'punct\\' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\\n                                                        #print(\"Update Counter J\")\\n                                                        counter_x = 0\\n                                                    if conj_head == counter_j and conj_rel == \\'conj\\':\\n                                                        for token_related, pos_related in tokens_pos:\\n                                                            if token_related == related_word and \\'VERB\\' in pos_related:\\n                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\\n                                                                    advcl_verbs.append((conj_word, counter_x))\\n    \\n\\n    #print(\"NEW ROW\")\\n    \\n    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\\n        #print(roots, \" - roots\")\\n        #print(root_verbs, \" - root_verbs\")\\n        #print(xcomp_verbs, \" - xcomp_verbs\")\\n        #print(ccomp_verbs, \" - ccomp_verbs\")\\n        #print(parataxis_verbs, \" - parataxis_verbs\")\\n        #print(advcl_verbs, \" - advcl_verbs\")\\n        #print()\\n        return \\'related\\'\\n    else:\\n        return 0\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# TRYING WITH all tags in one for cycle\n",
    "\n",
    "# The main function to process each sentence\n",
    "def find_valid_verbs(row):\n",
    "       \n",
    "    dependencies = row['dependencies']\n",
    "    tokens_pos = row['tokens_pos']\n",
    "\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    counter_x = 0\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []\n",
    "    root_verbs = []\n",
    "    xcomp_verbs = []\n",
    "    ccomp_verbs = []\n",
    "    parataxis_verbs = []\n",
    "    advcl_verbs = []\n",
    "    \n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            counter_i = counter_i + 1\n",
    "            #print(word, counter_i)\n",
    "            \n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    #print(\"Update Counter I\")\n",
    "                    counter_i = 0\n",
    "                \n",
    "            if deprel == 'root':\n",
    "                #roots.append((word, i + 1))  # Save the root verb and its index (i+1)\n",
    "                roots.append((word, counter_i))\n",
    "                for token, pos in tokens_pos:\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        #print(\"Found VERB\")\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            #print(\"F and not C\") \n",
    "                            root_verbs.append((word, counter_i))\n",
    "\n",
    "                counter_j = 0\n",
    "                # Check for any relater words attached to this root verb and add it if valid\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related\n",
    "                        counter_j = counter_j +1\n",
    "                        #print(related_word, counter_j)\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            #print(\"Update Counter J\")\n",
    "                            counter_j = 0\n",
    "                        if related_head == counter_i and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl', 'conj']:\n",
    "                            #print(\"FOUND RELATED: \", related_rel, \" - \", related_word)\n",
    "                            for token_related, pos_related in tokens_pos:\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        #print(\"RELATED WORD \", related_word, \" PASSED ALL CHECKS\")\n",
    "                                        \n",
    "                                        if related_rel == 'conj':\n",
    "                                            root_verbs.append((related_word, counter_j))\n",
    "                                            \n",
    "                                        elif related_rel == 'xcomp':\n",
    "                                            xcomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    xcomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'ccomp':\n",
    "                                            ccomp_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    ccomp_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'parataxis':\n",
    "                                            parataxis_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    parataxis_verbs.append((conj_word, counter_x))\n",
    "                                        \n",
    "                                        elif related_rel == 'advcl':\n",
    "                                            advcl_verbs.append((related_word, counter_j))\n",
    "                                            # найти conj для этого\n",
    "                                            counter_x = 0\n",
    "                                            for conj in dependencies:\n",
    "                                                if len(conj) == 3:\n",
    "                                                    conj_word, conj_head, conj_rel = conj\n",
    "                                                    counter_x = counter_x +1\n",
    "                                                    if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                        #print(\"Update Counter J\")\n",
    "                                                        counter_x = 0\n",
    "                                                    if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                        for token_related, pos_related in tokens_pos:\n",
    "                                                            if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                                if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                                    advcl_verbs.append((conj_word, counter_x))\n",
    "    \n",
    "\n",
    "    #print(\"NEW ROW\")\n",
    "    \n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "        return 'related'\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb86b91-082e-4234-bb2d-3b244ede8e74",
   "metadata": {},
   "source": [
    "## Current Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850365df-dec0-4b8f-a92e-6631d0347707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to check if a verb belongs to Foreseeability or Coercion groups\n",
    "\n",
    "def is_foreseeability_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of foreseeability-related verb classes.\n",
    "    foreseeability_classes = {'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lexname = synset.lexname().split('.')[1]  # Extracts the lexical category (i.e., type of action)\n",
    "        if lexname in foreseeability_classes:  # Checks if the lexical category is in the foreseeability class\n",
    "            return True  # Returns True if the verb matches any foreseeability category\n",
    "    return False  # If no match is found, returns False\n",
    "\n",
    "\n",
    "def is_coercion_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of coercion-related VerbNet classes.\n",
    "    coercion_classes = {'urge-58.1', 'force-59', 'forbid-67'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lemma = synset.lemmas()[0]  # Gets the first lemma for each synset\n",
    "        vn_classes = lemma.key().split('%')[0]  # Extracts the lemma key\n",
    "        vn_class_ids = vn.classids(vn_classes)  # Fetches the VerbNet classes for the lemma\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):  # Checks for a match in coercion classes\n",
    "            return True  # If a match is found in coercion classes, return True\n",
    "    return False  # If no match is found, return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ea980d-d800-4637-a05c-79f56cda0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_one_function(row):\n",
    "    # This is the function to find valid verbs (root, xcomp, ccomp, parataxis, advcl and their conjunctions)\n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence\n",
    "    \n",
    "    counter_i = 0  # Counter for tracking the index of words in the dependency structure\n",
    "    counter_j = 0  # Counter for tracking the index during nested loops\n",
    "    counter_x = 0  # Counter used to track conj words\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "\n",
    "    # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    roots = []  # For root verbs\n",
    "    root_verbs = []  # For valid root verbs (that pass foreseeability and coercion checks)\n",
    "\n",
    "    # (word, own index, head index)\n",
    "    xcomp_verbs = []  # For xcomp verbs\n",
    "    ccomp_verbs = []  # For ccomp verbs\n",
    "    parataxis_verbs = []  # For parataxis verbs\n",
    "    advcl_verbs = []  # For advcl verbs\n",
    "    \n",
    "    # Iterate through dependencies to identify roots and their related verbs\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep  # Unpacking the dependency tuple (word, head, relation)\n",
    "            counter_i += 1  # Increment the index counter for this word\n",
    "            \n",
    "            # If an end of the sentence has been found, reset the counter for punctuation\n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    counter_i = 0  # Reset counter when punctuation is found after root\n",
    "                \n",
    "            # Check if the current word is the root of the sentence\n",
    "            if deprel == 'root':\n",
    "                roots.append((word, counter_i, counter_i))  # Add the root verb and its index\n",
    "                for token, pos in tokens_pos:  # Iterate through POS tokens to find the root as a verb\n",
    "                    if token == word and pos == 'VERB':\n",
    "                        if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                            root_verbs.append((word, counter_i, counter_i))  # Add root verb if it passes foreseeability and coercion checks\n",
    "                # looking for related conj\n",
    "                counter_j = 0\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                        counter_j += 1  # Increment the index for the related word\n",
    "                        # Reset the counter for punctuation after root - end of the sentence\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            counter_j = 0\n",
    "                        # Look for conj attached to the verb\n",
    "                        if related_head == counter_i and related_rel in ['conj']:\n",
    "                            roots.append((related_word, counter_j, counter_i))  # Add the root verb and its index\n",
    "                            for token_related, pos_related in tokens_pos:  # Find if the related word is a verb\n",
    "                                if token_related == related_word and 'VERB' in pos_related:\n",
    "                                    if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                        root_verbs.append((related_word, counter_j, counter_i))  # Conj relation to root\n",
    "\n",
    "\n",
    "    \n",
    "    # Find related verbs (xcomp, ccomp, etc.) for root verbs and their conj\n",
    "\n",
    "    for verb in roots:\n",
    "        word, index, head_index = verb\n",
    "        #counter_i += 1\n",
    "\n",
    "        counter_j = 0\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                counter_j += 1  # Increment the index for the related word\n",
    "                \n",
    "                # Reset the counter for punctuation after root - end of the sentence\n",
    "                if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                    counter_j = 0\n",
    "                \n",
    "                # Look for xcomp, ccomp, parataxis or advcl relations attached to the root and its conj\n",
    "                if related_head == index and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl']:\n",
    "                    #print(\"found some related word: \", related_rel)\n",
    "                    for token_related, pos_related in tokens_pos:  # Find if the related word is a verb\n",
    "                        if token_related == related_word and 'VERB' in pos_related:\n",
    "                            if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                #print(\"related word passed all checks: \", related_rel)\n",
    "                                # Depending on the relation type, add the related verb to the appropriate list\n",
    "                                #if related_rel == 'conj':\n",
    "                                   # root_verbs.append((related_word, counter_j))  # Conj relation to root\n",
    "                                if related_rel == 'xcomp':\n",
    "                                    xcomp_verbs.append((related_word, counter_j, index))  # xcomp relation to root\n",
    "                                    # Handle conj for xcomp verbs\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\") and conj_head == roots[0][1]:\n",
    "                                                counter_x = 0  # Reset counter for punctuation\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                # Check if the conj word is a valid verb\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            xcomp_verbs.append((conj_word, counter_x, index))  # Conj for xcomp\n",
    "                                \n",
    "                                # Handle ccomp, parataxis, advcl similarly for related verbs and their conjunctions\n",
    "                                elif related_rel == 'ccomp':\n",
    "                                    ccomp_verbs.append((related_word, counter_j, index))  # ccomp relation\n",
    "                                    # Handle conj for ccomp\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            ccomp_verbs.append((conj_word, counter_x, index))  # Conj for ccomp\n",
    "                                \n",
    "                                elif related_rel == 'parataxis':\n",
    "                                    parataxis_verbs.append((related_word, counter_j, index))  # parataxis relation\n",
    "                                    # Handle conj for parataxis\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            parataxis_verbs.append((conj_word, counter_x, index))  # Conj for parataxis\n",
    "                                \n",
    "                                elif related_rel == 'advcl':\n",
    "                                    advcl_verbs.append((related_word, counter_j, index))  # advcl relation\n",
    "                                    # Handle conj for advcl\n",
    "                                    counter_x = 0\n",
    "                                    for conj in dependencies:\n",
    "                                        if len(conj) == 3:\n",
    "                                            conj_word, conj_head, conj_rel = conj\n",
    "                                            counter_x += 1\n",
    "                                            if conj_head == counter_j and conj_rel == 'conj':\n",
    "                                                for token_related, pos_related in tokens_pos:\n",
    "                                                    if token_related == related_word and 'VERB' in pos_related:\n",
    "                                                        if is_foreseeability_verb(related_word) and not is_coercion_verb(related_word):\n",
    "                                                            advcl_verbs.append((conj_word, counter_x, index))  # Conj for advcl\n",
    "    #print(\"NEW ROW\")\n",
    "    \n",
    "    # Return the lists of related verbs\n",
    "    #if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        #print(roots, \" - roots\")\n",
    "        #print(root_verbs, \" - root_verbs\")\n",
    "        #print(xcomp_verbs, \" - xcomp_verbs\")\n",
    "        #print(ccomp_verbs, \" - ccomp_verbs\")\n",
    "        #print(parataxis_verbs, \" - parataxis_verbs\")\n",
    "        #print(advcl_verbs, \" - advcl_verbs\")\n",
    "        #print()\n",
    "    return roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30dbe389-4669-4a18-9135-f9128335f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_agent_validity(related_word, row, tokens_pos):\n",
    "    \n",
    "    entities = row['entities']\n",
    "    valid_ent_labels = [\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]\n",
    "    valid_additional_words = [\"person\", \"man\", \"woman\", \"police\", \"administration\", \"immigrants\", \"president\", \"minister\", \"senator\", \n",
    "                              \"representative\", \"governor\", \"mayor\", \"council\", \"secretary\", \"ambassador\", \"chancellor\", \"parliamentary\", \"mr.\", \"ms.\", \"mrs.\"]\n",
    "\n",
    "    self = False\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    for entity, label in entities: \n",
    "        if entity in related_word and label in valid_ent_labels: \n",
    "            agent_is_valid = True # is it in NER list?  \n",
    "    if not agent_is_valid and 'PRON' in [pos for token, pos in tokens_pos if token == related_word]: \n",
    "        agent_is_valid = True # is it a promoun?\n",
    "        if related_word.lower() == \"i\" or related_word.lower() == \"we\":\n",
    "            self = True\n",
    "    if not agent_is_valid and related_word.lower() in valid_additional_words: agent_is_valid = True # is it from the list of additional words?\n",
    "\n",
    "    return agent_is_valid, self\n",
    "\n",
    "\n",
    "\n",
    "def check_causative_verb(verb):\n",
    "    # Check if the verb is in the CAUSE class or has CAUSETO relation in WordNet\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def define_polarity(verb, obj):\n",
    "    # Function to define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "    # 0 - others, 1 - positive, 2 - negative\n",
    "\n",
    "    result = 0\n",
    "    \n",
    "    # Create a simple context for WSD\n",
    "    context = f\"{verb} {obj}\"\n",
    "    \n",
    "    # Word Sense Disambiguation for the verb and object\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    # Calculate polarity using SentiWordNet\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    # AFINN score\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    # Subjectivity Lexicon score\n",
    "    tokens = context.split()\n",
    "    subj_pos = sum([1 for token in tokens if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in tokens if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    # Determine final polarity\n",
    "    if pos_score > neg_score:\n",
    "        return 1  # Positive/Praise\n",
    "    elif neg_score > pos_score:\n",
    "        return 2  # Negative/Blame\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "        \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def adjust_sentiment_for_negation(row, polarity, verb):\n",
    "    word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    dependencies = row['dependencies']\n",
    "\n",
    "    for related in dependencies:\n",
    "        if len(related) == 3:\n",
    "            related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "            if related_head == index and related_rel in ['advmod'] and (related_word == 'not' or related_word == 'n’t'):\n",
    "                if polarity == 0:\n",
    "                    return 0\n",
    "                if polarity == 1:\n",
    "                    return 2\n",
    "                if polarity == 2:\n",
    "                    return 1\n",
    "    \n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01c3852-ef74-496c-aeb5-95f01c929ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs):\n",
    "    \n",
    "     # This is the function to decide on Agent Causality, find the object, decide on Polarity and classify the row \n",
    "    \n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence  \n",
    "\n",
    "    self = False\n",
    "    result = None\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    # Сonnection 1: nsubj / nsubj:pass - root_verb - obj / iobj / obl - by priority\n",
    "    for verb in root_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "        \n",
    "\n",
    "    \n",
    "    # Сonnection 2: nsubj / nsubj:pass - root_verb - xcomp - obj / iobj / obl - by priority\n",
    "    for verb in xcomp_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "\n",
    "    # Сonnection 3: nsubj / nsubj:pass - parataxis_verbs - (xcomp) - obj / iobj / obl - by priority    \n",
    "    for verb in parataxis_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "\n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with parataxis\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "\n",
    "\n",
    "\n",
    "    # Сonnection 4: nsubj / nsubj:pass - advcl_verbs - (xcomp) - obj / iobj / obl - by priority   \n",
    "    for verb in advcl_verbs:\n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with advcl\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "\n",
    "    \n",
    "    \n",
    "    # Checking for other possible connections\n",
    "    # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    # (word, own index, head index)\n",
    "    \n",
    "    # АГЕНТ МБ ПРИСОЕДИНЕН К ROOT, А ОБЪЕКТ К ВСПОМОГАТЕЛЬНОМУ ТЕГУ\n",
    "    # ИСКАТЬ СУБЪЕКТ И К СЕБЕ И К ROOT ПРИСОЕДИНЕННОМУ\n",
    "    # ОБЪЕКТ ИЩЕМ ПРИСОЕДИНЕННЫЙ К СЕБЕ\n",
    "\n",
    "    # Сonnection 5: nsubj - ccomp_verbs - (xcomp) - obj / iobj / obl - by priority   \n",
    "    for verb in ccomp_verbs: \n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['nsubj']: # is that an agent?\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                #else:\n",
    "                    #if related_head == head_index and related_rel in ['nsubj']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                        #agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                    if related_head == index and related_rel in ['obj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['iobj']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['obl']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "                    if related_head == index and related_rel in ['xcomp']:\n",
    "                        #print(\"Marvel had happened with advcl\")\n",
    "                        for related_to_xcomp in dependencies:\n",
    "                            if len(related_to_xcomp) == 3:\n",
    "                                related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp  # Unpacking the dependency\n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['iobj']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "            \n",
    "                                if related_to_xcomp_head == counter_j and related_rel in ['obl']:\n",
    "                                    # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                                    polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                                    polarity = adjust_sentiment_for_negation(row, polarity, related)\n",
    "                                    if polarity != 0:\n",
    "                                        if self:\n",
    "                                            result = \"self - \" + str(polarity)\n",
    "                                            return result\n",
    "                                        return polarity\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # Сonnection 6: nsubj:pass - ccomp_verbs - (xcomp) - obl:agent / obl - by priority   \n",
    "    for verb in ccomp_verbs: \n",
    "        word, index, head_index = verb # if index == head_index - main root, not conj\n",
    "    \n",
    "        # 1 - Find an agent connected to the given verb \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                # subject connected to the word itself or to its root\n",
    "                if related_head == index and related_rel in ['obl:agent', 'obl']: # is that an agent?\n",
    "                    #print(\"Marvel with ccomp passive happened\")\n",
    "                    # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                #else:\n",
    "                   #if related_head == head_index and related_rel in ['nsubj', 'nsubj:pass']: # is that an agent?\n",
    "                        # 2 - Check if it is a relevant agent - NER categories + list of additional terms (secretary and so on)\n",
    "                       #agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "        # If agent is not valid, check for causative verbs\n",
    "        if not agent_is_valid:\n",
    "            if check_causative_verb(word):\n",
    "                agent_is_valid = True\n",
    "\n",
    "        # 3 - Find an object connected to the given verb\n",
    "        # obj / iobj / obl - by priority\n",
    "        if agent_is_valid:\n",
    "            #print(\"Even Agent is valid\")\n",
    "            counter_j = 0\n",
    "            for related in dependencies:\n",
    "                if len(related) == 3:\n",
    "                    related_word, related_head, related_rel = related  # Unpacking the dependency\n",
    "                    counter_j += 1  # Increment the index for the related word\n",
    "                    \n",
    "                    # Reset the counter for punctuation after root - end of the sentence\n",
    "                    if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                        counter_j = 0\n",
    "                        \n",
    "                    if related_head == index and related_rel in ['nsubj:pass']:\n",
    "                        # 4 - Define the Polarity of the combination verb + object taking into attention a negation connected to that verb\n",
    "                        polarity = define_polarity(word, related_word)\n",
    "                        polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                        if polarity != 0:\n",
    "                            if self:\n",
    "                                result = \"self - \" + str(polarity)\n",
    "                                return result\n",
    "                            return polarity\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e4e0cf-d699-42a2-968b-063783d3a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_verbs(row):\n",
    "    \n",
    "    # This is the main function to process each row of data and classify the row \n",
    "\n",
    "    # 1 - Find all the related verbs in categories in dependency column 'root', 'xcomp', 'ccomp', 'parataxis', 'advcl', 'conj' (is a verb check - foreseeability check - coercion check)\n",
    "    roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs = step_one_function(row)\n",
    "    \n",
    "    # 2 - If at least one of the lists is not empty - can proceed\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        \n",
    "        # 3 - Take a final decision about the label (0 - others, 1 - positive, 2 - negative)\n",
    "        return step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs)\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314989cf-1c6a-4635-b61b-e4ec36054046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_26500\\4059228178.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_26500\\4059228178.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the small dataset\n",
    "df_train_ready_merged_small['Final_Result'] = df_train_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged_small['Final_Result'] = df_valid_ready_merged_small.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged_small = df_train_ready_merged_small[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c233a97e-f4ff-434f-8f97-5764a84500c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan has endorsed Trump and told reporters thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Ryan, PROPN), (has, AUX), (endorsed, VERB), ...</td>\n",
       "      <td>[(Ryan, PERSON), (Trump, PERSON), (this past w...</td>\n",
       "      <td>[(Ryan, 3, nsubj), (has, 3, aux), (endorsed, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John McGraw, 78, was charged with assault and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[(John, PROPN), (McGraw, PROPN), (,, PUNCT), (...</td>\n",
       "      <td>[(John McGraw, PERSON), (78, DATE), (Thursday,...</td>\n",
       "      <td>[(John, 7, nsubj:pass), (McGraw, 1, flat), (,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Filipino fighter unleashed a dazzling comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(The, DET), (Filipino, ADJ), (fighter, NOUN),...</td>\n",
       "      <td>[(Filipino, NORP), (Margarito, PERSON), (Maywe...</td>\n",
       "      <td>[(The, 3, det), (Filipino, 3, amod), (fighter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But the Marlins have failed to make the postse...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(But, CCONJ), (the, DET), (Marlins, PROPN), (...</td>\n",
       "      <td>[(Marlins, ORG), (Loria, PERSON)]</td>\n",
       "      <td>[(But, 5, cc), (the, 3, det), (Marlins, 5, nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>So shortly after 2 a.m., campaign chairman Joh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(So, ADV), (shortly, ADV), (after, ADP), (2, ...</td>\n",
       "      <td>[(2 a.m., TIME), (John Podesta, PERSON), (Clin...</td>\n",
       "      <td>[(So, 11, advmod), (shortly, 4, advmod), (afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Church has not answered the allegations ot...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[(The, DET), (Church, PROPN), (has, AUX), (not...</td>\n",
       "      <td>[(Church, ORG), (Navajo, GPE)]</td>\n",
       "      <td>[(The, 2, det), (Church, 5, nsubj), (has, 5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bruno Beschizza, the conservative mayor of has...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Bruno, PROPN), (Beschizza, PROPN), (,, PUNCT...</td>\n",
       "      <td>[(Bruno Beschizza, PERSON), (Théo, PERSON)]</td>\n",
       "      <td>[(Bruno, 9, nsubj), (Beschizza, 1, flat), (,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>On criminal justice issues, Harris faults Carp...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[(On, ADP), (criminal, ADJ), (justice, NOUN), ...</td>\n",
       "      <td>[(Harris, PERSON), (Carper, PERSON), (the 1990...</td>\n",
       "      <td>[(On, 4, case), (criminal, 3, amod), (justice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Even the hologame that Finn starts up on the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Even, ADV), (the, DET), (hologame, NOUN), (t...</td>\n",
       "      <td>[(Finn, PERSON), (Chewbacca, PERSON), (A New H...</td>\n",
       "      <td>[(Even, 3, advmod), (the, 3, det), (hologame, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  Final_Result  \\\n",
       "0                  George is not supporting Clinton.      2             2   \n",
       "1  Ryan has endorsed Trump and told reporters thi...      1             1   \n",
       "2  John McGraw, 78, was charged with assault and ...      2             0   \n",
       "3  The Filipino fighter unleashed a dazzling comb...      1             0   \n",
       "4  But the Marlins have failed to make the postse...      0             0   \n",
       "5  So shortly after 2 a.m., campaign chairman Joh...      0             0   \n",
       "6  The Church has not answered the allegations ot...      0             1   \n",
       "7  Bruno Beschizza, the conservative mayor of has...      0             0   \n",
       "8  On criminal justice issues, Harris faults Carp...      2             2   \n",
       "9  Even the hologame that Finn starts up on the s...      0             0   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "1  [(Ryan, PROPN), (has, AUX), (endorsed, VERB), ...   \n",
       "2  [(John, PROPN), (McGraw, PROPN), (,, PUNCT), (...   \n",
       "3  [(The, DET), (Filipino, ADJ), (fighter, NOUN),...   \n",
       "4  [(But, CCONJ), (the, DET), (Marlins, PROPN), (...   \n",
       "5  [(So, ADV), (shortly, ADV), (after, ADP), (2, ...   \n",
       "6  [(The, DET), (Church, PROPN), (has, AUX), (not...   \n",
       "7  [(Bruno, PROPN), (Beschizza, PROPN), (,, PUNCT...   \n",
       "8  [(On, ADP), (criminal, ADJ), (justice, NOUN), ...   \n",
       "9  [(Even, ADV), (the, DET), (hologame, NOUN), (t...   \n",
       "\n",
       "                                            entities  \\\n",
       "0              [(George, PERSON), (Clinton, PERSON)]   \n",
       "1  [(Ryan, PERSON), (Trump, PERSON), (this past w...   \n",
       "2  [(John McGraw, PERSON), (78, DATE), (Thursday,...   \n",
       "3  [(Filipino, NORP), (Margarito, PERSON), (Maywe...   \n",
       "4                  [(Marlins, ORG), (Loria, PERSON)]   \n",
       "5  [(2 a.m., TIME), (John Podesta, PERSON), (Clin...   \n",
       "6                     [(Church, ORG), (Navajo, GPE)]   \n",
       "7        [(Bruno Beschizza, PERSON), (Théo, PERSON)]   \n",
       "8  [(Harris, PERSON), (Carper, PERSON), (the 1990...   \n",
       "9  [(Finn, PERSON), (Chewbacca, PERSON), (A New H...   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  \n",
       "1  [(Ryan, 3, nsubj), (has, 3, aux), (endorsed, 0...  \n",
       "2  [(John, 7, nsubj:pass), (McGraw, 1, flat), (,,...  \n",
       "3  [(The, 3, det), (Filipino, 3, amod), (fighter,...  \n",
       "4  [(But, 5, cc), (the, 3, det), (Marlins, 5, nsu...  \n",
       "5  [(So, 11, advmod), (shortly, 4, advmod), (afte...  \n",
       "6  [(The, 2, det), (Church, 5, nsubj), (has, 5, a...  \n",
       "7  [(Bruno, 9, nsubj), (Beschizza, 1, flat), (,, ...  \n",
       "8  [(On, 4, case), (criminal, 3, amod), (justice,...  \n",
       "9  [(Even, 3, advmod), (the, 3, det), (hologame, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33375c99-60a7-4f95-a5d1-87d16c49c0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the dataset\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_train_ready_merged = df_train_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(find_valid_verbs, axis=1)\n",
    "df_valid_ready_merged = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result'] + [col for col in df_train_ready_merged_small.columns if col not in ['Sentence', 'Label', 'Final_Result']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "915deb00-52fe-4572-b9e7-5a7f68e35cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0           3701\n",
       "2            721\n",
       "1            562\n",
       "self - 1      31\n",
       "self - 2      17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11d1c91f-3ad7-4025-a5fe-671a593c81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "0           416\n",
       "2            82\n",
       "1            49\n",
       "self - 2      2\n",
       "self - 1      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53004aeb-fd41-47e9-a745-f90899870a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I don't want them to think I was swearing at t...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(I, PRON), (do, AUX), (n't, PART), (want, VER...</td>\n",
       "      <td>[(Albarn, PERSON), (several minutes, TIME), (B...</td>\n",
       "      <td>[(I, 4, nsubj), (do, 4, aux), (n't, 4, advmod)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>To truly understand Breitbartism on immigratio...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(To, PART), (truly, ADV), (understand, VERB),...</td>\n",
       "      <td>[(the last three weeks, DATE), (Trump’s, ORG)]</td>\n",
       "      <td>[(To, 3, mark), (truly, 3, advmod), (understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Like all my museum colleagues, I support fundi...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Like, ADP), (all, DET), (my, PRON), (museum,...</td>\n",
       "      <td>[(the National Endowment for the Arts, ORG), (...</td>\n",
       "      <td>[(Like, 5, case), (all, 5, det:predet), (my, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>While not every member of our organization sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(While, SCONJ), (not, PART), (every, DET), (m...</td>\n",
       "      <td>[(Trump, PERSON), (GOP, ORG), (Republicans, NO...</td>\n",
       "      <td>[(While, 8, mark), (not, 4, advmod), (every, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>I have said from the outset that I would vote ...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(I, PRON), (have, AUX), (said, VERB), (from, ...</td>\n",
       "      <td>[(Donald Trump, PERSON), (Republican, NORP)]</td>\n",
       "      <td>[(I, 3, nsubj), (have, 3, aux), (said, 0, root...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>PG&amp;E noted that the cause of the fire has yet ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(PG&amp;E, PROPN), (noted, VERB), (that, SCONJ), ...</td>\n",
       "      <td>[(PG&amp;E, ORG), (PUC, ORG), (Pickers, PERSON), (...</td>\n",
       "      <td>[(PG&amp;E, 2, nsubj), (noted, 0, root), (that, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>Some of the bonuses went to people at the AIG ...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Some, DET), (of, ADP), (the, DET), (bonuses,...</td>\n",
       "      <td>[(AIG, ORG), (Grassley, PERSON), (Tuesday, DATE)]</td>\n",
       "      <td>[(Some, 5, nsubj), (of, 4, case), (the, 4, det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>Co-producer Saralena Weinfield told People mag...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Co-producer, NOUN), (Saralena, PROPN), (Wein...</td>\n",
       "      <td>[(Saralena Weinfield, PERSON), (People, ORG), ...</td>\n",
       "      <td>[(Co-producer, 2, compound), (Saralena, 4, nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>No way will we ever support Cruz,’” said a for...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(No, DET), (way, NOUN), (will, AUX), (we, PRO...</td>\n",
       "      <td>[(Cruz, PERSON), (Republican, NORP)]</td>\n",
       "      <td>[(No, 2, det), (way, 11, ccomp), (will, 6, aux...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>I like supporting smaller Etsy designers and a...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(I, PRON), (like, VERB), (supporting, VERB), ...</td>\n",
       "      <td>[(Etsy, ORG), (Kaufman, PERSON)]</td>\n",
       "      <td>[(I, 2, nsubj), (like, 0, root), (supporting, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>I committed at the outset, I will support the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(I, PRON), (committed, VERB), (at, ADP), (the...</td>\n",
       "      <td>[(Republican, NORP), (Cruz, PERSON)]</td>\n",
       "      <td>[(I, 2, nsubj), (committed, 0, root), (at, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>I committed at the outset, I will support the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(I, PRON), (committed, VERB), (at, ADP), (the...</td>\n",
       "      <td>[(Republican, NORP), (Cruz, PERSON)]</td>\n",
       "      <td>[(I, 2, nsubj), (committed, 0, root), (at, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>I ended up in Twitter jail because I congratul...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(I, PRON), (ended, VERB), (up, ADP), (in, ADP...</td>\n",
       "      <td>[(Twitter, ORG), (American, NORP), (Courtney O...</td>\n",
       "      <td>[(I, 2, nsubj), (ended, 0, root), (up, 2, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>Are we finally seeing some good climate news f...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Are, AUX), (we, PRON), (finally, ADV), (seei...</td>\n",
       "      <td>[(US, GPE), (China, GPE)]</td>\n",
       "      <td>[(Are, 4, aux), (we, 4, nsubj), (finally, 4, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>From author T. A. Frank, an liberal, in the pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(From, ADP), (author, NOUN), (T., PROPN), (A....</td>\n",
       "      <td>[(T. A. Frank, PERSON), (Vanity Fair, ORG), (W...</td>\n",
       "      <td>[(From, 2, case), (author, 19, obl), (T., 2, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>and I've followed his work, I can say with con...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(and, CCONJ), (I, PRON), ('ve, AUX), (followe...</td>\n",
       "      <td>[(I've, PERSON), (Journal's, ORG), (Howey, PER...</td>\n",
       "      <td>[(and, 4, cc), (I, 4, nsubj), ('ve, 4, aux), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>Mitchellvii I’m strongly supporting Mr. Trump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Mitchellvii, PROPN), (I, PRON), (’m, AUX), (...</td>\n",
       "      <td>[(Mitchellvii I’m, PERSON), (Trump, PERSON)]</td>\n",
       "      <td>[(Mitchellvii, 5, vocative), (I, 5, nsubj), (’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>Nokia has started to build a new business by o...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Nokia, PROPN), (has, AUX), (started, VERB), ...</td>\n",
       "      <td>[(Nokia, ORG), (Jorma Ollila, PERSON), (Thursd...</td>\n",
       "      <td>[(Nokia, 3, nsubj), (has, 3, aux), (started, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>As Judge Gorsuch’s nomination comes to the flo...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(As, SCONJ), (Judge, PROPN), (Gorsuch, PROPN)...</td>\n",
       "      <td>[(Gorsuch’s, PERSON), (seven, CARDINAL), (eigh...</td>\n",
       "      <td>[(As, 6, mark), (Judge, 5, nmod:poss), (Gorsuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>The new Library of Birmingham sounds as if it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(The, DET), (new, ADJ), (Library, PROPN), (of...</td>\n",
       "      <td>[(Library of Birmingham, ORG), (5,000, CARDINA...</td>\n",
       "      <td>[(The, 3, det), (new, 3, amod), (Library, 6, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>In GLAA's brief, we commend various D.C. offic...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(In, ADP), (GLAA, PROPN), ('s, PART), (brief,...</td>\n",
       "      <td>[(GLAA's, ORG), (D.C., GPE)]</td>\n",
       "      <td>[(In, 4, case), (GLAA, 4, nmod:poss), ('s, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>Eventually, I reached out to the Spotify suppo...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Eventually, ADV), (,, PUNCT), (I, PRON), (re...</td>\n",
       "      <td>[(Spotify, ORG), (Facebook, ORG), (two, CARDIN...</td>\n",
       "      <td>[(Eventually, 4, advmod), (,, 4, punct), (I, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>As a law-abiding Israeli citizen, I accept my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(As, ADP), (a, DET), (law, NOUN), (-, PUNCT),...</td>\n",
       "      <td>[(Israeli, NORP), (Yossi, PERSON)]</td>\n",
       "      <td>[(As, 7, case), (a, 7, det), (law, 5, compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>As Phil Johnston and I were developing, we sta...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(As, SCONJ), (Phil, PROPN), (Johnston, PROPN)...</td>\n",
       "      <td>[(Phil Johnston, PERSON), (Ralph's, PERSON)]</td>\n",
       "      <td>[(As, 7, mark), (Phil, 7, nsubj), (Johnston, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>In a statement published on their official web...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(In, ADP), (a, DET), (statement, NOUN), (publ...</td>\n",
       "      <td>[(May 12, DATE), (López Rivera, PERSON), (Osca...</td>\n",
       "      <td>[(In, 3, case), (a, 3, det), (statement, 14, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>Bow Wow has been lucky enough to realise the d...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Bow, PROPN), (Wow, PROPN), (has, AUX), (been...</td>\n",
       "      <td>[(Keynes, PERSON)]</td>\n",
       "      <td>[(Bow, 2, compound), (Wow, 5, nsubj), (has, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4335</th>\n",
       "      <td>They, too, were crying.I want to thank the law...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(They, PRON), (,, PUNCT), (too, ADV), (,, PUN...</td>\n",
       "      <td>[(Vasquez, PERSON), (Cecelia Stevenson, PERSON...</td>\n",
       "      <td>[(They, 6, nsubj), (,, 6, punct), (too, 6, adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>No way will we ever support Cruz,’” said a for...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(No, DET), (way, NOUN), (will, AUX), (we, PRO...</td>\n",
       "      <td>[(Cruz, PERSON), (Republican, NORP)]</td>\n",
       "      <td>[(No, 2, det), (way, 11, ccomp), (will, 6, aux...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>To truly understand Breitbartism on immigratio...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(To, PART), (truly, ADV), (understand, VERB),...</td>\n",
       "      <td>[(the last three weeks, DATE), (Trump’s, ORG)]</td>\n",
       "      <td>[(To, 3, mark), (truly, 3, advmod), (understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>Mitchellvii I’m strongly supporting Mr. Trump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(Mitchellvii, PROPN), (I, PRON), (’m, AUX), (...</td>\n",
       "      <td>[(Mitchellvii I’m, PERSON), (Trump, PERSON)]</td>\n",
       "      <td>[(Mitchellvii, 5, vocative), (I, 5, nsubj), (’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4862</th>\n",
       "      <td>We must thank Sir Alex Ferguson for this joyou...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 1</td>\n",
       "      <td>[(We, PRON), (must, AUX), (thank, VERB), (Sir,...</td>\n",
       "      <td>[(Alex Ferguson, PERSON), (Gazzetta dello Spor...</td>\n",
       "      <td>[(We, 3, nsubj), (must, 3, aux), (thank, 0, ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  Label Final_Result  \\\n",
       "41    I don't want them to think I was swearing at t...      0     self - 1   \n",
       "62    To truly understand Breitbartism on immigratio...      0     self - 1   \n",
       "116   Like all my museum colleagues, I support fundi...      0     self - 1   \n",
       "689   While not every member of our organization sup...      0     self - 1   \n",
       "1204  I have said from the outset that I would vote ...      0     self - 1   \n",
       "1558  PG&E noted that the cause of the fire has yet ...      1     self - 1   \n",
       "1592  Some of the bonuses went to people at the AIG ...      0     self - 1   \n",
       "1637  Co-producer Saralena Weinfield told People mag...      0     self - 1   \n",
       "1846  No way will we ever support Cruz,’” said a for...      2     self - 1   \n",
       "1865  I like supporting smaller Etsy designers and a...      1     self - 1   \n",
       "1996  I committed at the outset, I will support the ...      1     self - 1   \n",
       "2154  I committed at the outset, I will support the ...      1     self - 1   \n",
       "2282  I ended up in Twitter jail because I congratul...      0     self - 1   \n",
       "2452  Are we finally seeing some good climate news f...      0     self - 1   \n",
       "2619  From author T. A. Frank, an liberal, in the pa...      0     self - 1   \n",
       "2713  and I've followed his work, I can say with con...      1     self - 1   \n",
       "3199  Mitchellvii I’m strongly supporting Mr. Trump ...      1     self - 1   \n",
       "3290  Nokia has started to build a new business by o...      1     self - 1   \n",
       "3338  As Judge Gorsuch’s nomination comes to the flo...      0     self - 1   \n",
       "3572  The new Library of Birmingham sounds as if it ...      0     self - 1   \n",
       "3631  In GLAA's brief, we commend various D.C. offic...      0     self - 1   \n",
       "3760  Eventually, I reached out to the Spotify suppo...      0     self - 1   \n",
       "3827  As a law-abiding Israeli citizen, I accept my ...      1     self - 1   \n",
       "4046  As Phil Johnston and I were developing, we sta...      0     self - 1   \n",
       "4067  In a statement published on their official web...      0     self - 1   \n",
       "4134  Bow Wow has been lucky enough to realise the d...      0     self - 1   \n",
       "4335  They, too, were crying.I want to thank the law...      2     self - 1   \n",
       "4507  No way will we ever support Cruz,’” said a for...      2     self - 1   \n",
       "4508  To truly understand Breitbartism on immigratio...      0     self - 1   \n",
       "4690  Mitchellvii I’m strongly supporting Mr. Trump ...      1     self - 1   \n",
       "4862  We must thank Sir Alex Ferguson for this joyou...      1     self - 1   \n",
       "\n",
       "                                             tokens_pos  \\\n",
       "41    [(I, PRON), (do, AUX), (n't, PART), (want, VER...   \n",
       "62    [(To, PART), (truly, ADV), (understand, VERB),...   \n",
       "116   [(Like, ADP), (all, DET), (my, PRON), (museum,...   \n",
       "689   [(While, SCONJ), (not, PART), (every, DET), (m...   \n",
       "1204  [(I, PRON), (have, AUX), (said, VERB), (from, ...   \n",
       "1558  [(PG&E, PROPN), (noted, VERB), (that, SCONJ), ...   \n",
       "1592  [(Some, DET), (of, ADP), (the, DET), (bonuses,...   \n",
       "1637  [(Co-producer, NOUN), (Saralena, PROPN), (Wein...   \n",
       "1846  [(No, DET), (way, NOUN), (will, AUX), (we, PRO...   \n",
       "1865  [(I, PRON), (like, VERB), (supporting, VERB), ...   \n",
       "1996  [(I, PRON), (committed, VERB), (at, ADP), (the...   \n",
       "2154  [(I, PRON), (committed, VERB), (at, ADP), (the...   \n",
       "2282  [(I, PRON), (ended, VERB), (up, ADP), (in, ADP...   \n",
       "2452  [(Are, AUX), (we, PRON), (finally, ADV), (seei...   \n",
       "2619  [(From, ADP), (author, NOUN), (T., PROPN), (A....   \n",
       "2713  [(and, CCONJ), (I, PRON), ('ve, AUX), (followe...   \n",
       "3199  [(Mitchellvii, PROPN), (I, PRON), (’m, AUX), (...   \n",
       "3290  [(Nokia, PROPN), (has, AUX), (started, VERB), ...   \n",
       "3338  [(As, SCONJ), (Judge, PROPN), (Gorsuch, PROPN)...   \n",
       "3572  [(The, DET), (new, ADJ), (Library, PROPN), (of...   \n",
       "3631  [(In, ADP), (GLAA, PROPN), ('s, PART), (brief,...   \n",
       "3760  [(Eventually, ADV), (,, PUNCT), (I, PRON), (re...   \n",
       "3827  [(As, ADP), (a, DET), (law, NOUN), (-, PUNCT),...   \n",
       "4046  [(As, SCONJ), (Phil, PROPN), (Johnston, PROPN)...   \n",
       "4067  [(In, ADP), (a, DET), (statement, NOUN), (publ...   \n",
       "4134  [(Bow, PROPN), (Wow, PROPN), (has, AUX), (been...   \n",
       "4335  [(They, PRON), (,, PUNCT), (too, ADV), (,, PUN...   \n",
       "4507  [(No, DET), (way, NOUN), (will, AUX), (we, PRO...   \n",
       "4508  [(To, PART), (truly, ADV), (understand, VERB),...   \n",
       "4690  [(Mitchellvii, PROPN), (I, PRON), (’m, AUX), (...   \n",
       "4862  [(We, PRON), (must, AUX), (thank, VERB), (Sir,...   \n",
       "\n",
       "                                               entities  \\\n",
       "41    [(Albarn, PERSON), (several minutes, TIME), (B...   \n",
       "62       [(the last three weeks, DATE), (Trump’s, ORG)]   \n",
       "116   [(the National Endowment for the Arts, ORG), (...   \n",
       "689   [(Trump, PERSON), (GOP, ORG), (Republicans, NO...   \n",
       "1204       [(Donald Trump, PERSON), (Republican, NORP)]   \n",
       "1558  [(PG&E, ORG), (PUC, ORG), (Pickers, PERSON), (...   \n",
       "1592  [(AIG, ORG), (Grassley, PERSON), (Tuesday, DATE)]   \n",
       "1637  [(Saralena Weinfield, PERSON), (People, ORG), ...   \n",
       "1846               [(Cruz, PERSON), (Republican, NORP)]   \n",
       "1865                   [(Etsy, ORG), (Kaufman, PERSON)]   \n",
       "1996               [(Republican, NORP), (Cruz, PERSON)]   \n",
       "2154               [(Republican, NORP), (Cruz, PERSON)]   \n",
       "2282  [(Twitter, ORG), (American, NORP), (Courtney O...   \n",
       "2452                          [(US, GPE), (China, GPE)]   \n",
       "2619  [(T. A. Frank, PERSON), (Vanity Fair, ORG), (W...   \n",
       "2713  [(I've, PERSON), (Journal's, ORG), (Howey, PER...   \n",
       "3199       [(Mitchellvii I’m, PERSON), (Trump, PERSON)]   \n",
       "3290  [(Nokia, ORG), (Jorma Ollila, PERSON), (Thursd...   \n",
       "3338  [(Gorsuch’s, PERSON), (seven, CARDINAL), (eigh...   \n",
       "3572  [(Library of Birmingham, ORG), (5,000, CARDINA...   \n",
       "3631                       [(GLAA's, ORG), (D.C., GPE)]   \n",
       "3760  [(Spotify, ORG), (Facebook, ORG), (two, CARDIN...   \n",
       "3827                 [(Israeli, NORP), (Yossi, PERSON)]   \n",
       "4046       [(Phil Johnston, PERSON), (Ralph's, PERSON)]   \n",
       "4067  [(May 12, DATE), (López Rivera, PERSON), (Osca...   \n",
       "4134                                 [(Keynes, PERSON)]   \n",
       "4335  [(Vasquez, PERSON), (Cecelia Stevenson, PERSON...   \n",
       "4507               [(Cruz, PERSON), (Republican, NORP)]   \n",
       "4508     [(the last three weeks, DATE), (Trump’s, ORG)]   \n",
       "4690       [(Mitchellvii I’m, PERSON), (Trump, PERSON)]   \n",
       "4862  [(Alex Ferguson, PERSON), (Gazzetta dello Spor...   \n",
       "\n",
       "                                           dependencies  \n",
       "41    [(I, 4, nsubj), (do, 4, aux), (n't, 4, advmod)...  \n",
       "62    [(To, 3, mark), (truly, 3, advmod), (understan...  \n",
       "116   [(Like, 5, case), (all, 5, det:predet), (my, 5...  \n",
       "689   [(While, 8, mark), (not, 4, advmod), (every, 4...  \n",
       "1204  [(I, 3, nsubj), (have, 3, aux), (said, 0, root...  \n",
       "1558  [(PG&E, 2, nsubj), (noted, 0, root), (that, 9,...  \n",
       "1592  [(Some, 5, nsubj), (of, 4, case), (the, 4, det...  \n",
       "1637  [(Co-producer, 2, compound), (Saralena, 4, nsu...  \n",
       "1846  [(No, 2, det), (way, 11, ccomp), (will, 6, aux...  \n",
       "1865  [(I, 2, nsubj), (like, 0, root), (supporting, ...  \n",
       "1996  [(I, 2, nsubj), (committed, 0, root), (at, 5, ...  \n",
       "2154  [(I, 2, nsubj), (committed, 0, root), (at, 5, ...  \n",
       "2282  [(I, 2, nsubj), (ended, 0, root), (up, 2, comp...  \n",
       "2452  [(Are, 4, aux), (we, 4, nsubj), (finally, 4, a...  \n",
       "2619  [(From, 2, case), (author, 19, obl), (T., 2, f...  \n",
       "2713  [(and, 4, cc), (I, 4, nsubj), ('ve, 4, aux), (...  \n",
       "3199  [(Mitchellvii, 5, vocative), (I, 5, nsubj), (’...  \n",
       "3290  [(Nokia, 3, nsubj), (has, 3, aux), (started, 0...  \n",
       "3338  [(As, 6, mark), (Judge, 5, nmod:poss), (Gorsuc...  \n",
       "3572  [(The, 3, det), (new, 3, amod), (Library, 6, n...  \n",
       "3631  [(In, 4, case), (GLAA, 4, nmod:poss), ('s, 2, ...  \n",
       "3760  [(Eventually, 4, advmod), (,, 4, punct), (I, 4...  \n",
       "3827  [(As, 7, case), (a, 7, det), (law, 5, compound...  \n",
       "4046  [(As, 7, mark), (Phil, 7, nsubj), (Johnston, 2...  \n",
       "4067  [(In, 3, case), (a, 3, det), (statement, 14, o...  \n",
       "4134  [(Bow, 2, compound), (Wow, 5, nsubj), (has, 5,...  \n",
       "4335  [(They, 6, nsubj), (,, 6, punct), (too, 6, adv...  \n",
       "4507  [(No, 2, det), (way, 11, ccomp), (will, 6, aux...  \n",
       "4508  [(To, 3, mark), (truly, 3, advmod), (understan...  \n",
       "4690  [(Mitchellvii, 5, vocative), (I, 5, nsubj), (’...  \n",
       "4862  [(We, 3, nsubj), (must, 3, aux), (thank, 0, ro...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[df_train_ready_merged['Final_Result'] == 'self - 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fad6e40-9b53-4b48-98b1-fa0a7b431e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>If the U. S. comes with reckless military mane...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(If, SCONJ), (the, DET), (U., PROPN), (S., PR...</td>\n",
       "      <td>[(the U. S., GPE)]</td>\n",
       "      <td>[(If, 5, mark), (the, 4, det), (U., 4, compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>After the terrorist attacks on 9/11 we respond...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(After, ADP), (the, DET), (terrorist, ADJ), (...</td>\n",
       "      <td>[(9/11, EVENT), (the Patriot Act, LAW), (Cabin...</td>\n",
       "      <td>[(After, 4, case), (the, 4, det), (terrorist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Whether we defeat Donald Trump before the conv...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(Whether, SCONJ), (we, PRON), (defeat, VERB),...</td>\n",
       "      <td>[(Donald Trump, PERSON), (North Dakota, GPE)]</td>\n",
       "      <td>[(Whether, 3, mark), (we, 3, nsubj), (defeat, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Now Christie says, ”I didn’t support Sonia Sot...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(Now, ADV), (Christie, PROPN), (says, VERB), ...</td>\n",
       "      <td>[(Christie, PERSON), (Sonia Sotomayor, PERSON)]</td>\n",
       "      <td>[(Now, 3, advmod), (Christie, 3, nsubj), (says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>We condemn Israeli attempts to prevent Egyptia...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(We, PRON), (condemn, VERB), (Israeli, ADJ), ...</td>\n",
       "      <td>[(Israeli, NORP), (Egyptians, NORP), (Egypt, G...</td>\n",
       "      <td>[(We, 2, nsubj), (condemn, 0, root), (Israeli,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Bochy said Cain could handle the assignment bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(Bochy, PROPN), (said, VERB), (Cain, PROPN), ...</td>\n",
       "      <td>[(Bochy, PERSON), (Cain, PERSON), (Johnson, PE...</td>\n",
       "      <td>[(Bochy, 2, nsubj), (said, 0, root), (Cain, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>she stood waiting to enter the hall, Savage sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(she, PRON), (stood, VERB), (waiting, VERB), ...</td>\n",
       "      <td>[(Savage, PERSON), (Assange's, PERSON), (Savag...</td>\n",
       "      <td>[(she, 2, nsubj), (stood, 0, root), (waiting, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>Now Christie says, ”I didn’t support Sonia Sot...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(Now, ADV), (Christie, PROPN), (says, VERB), ...</td>\n",
       "      <td>[(Christie, PERSON), (Sonia Sotomayor, PERSON)]</td>\n",
       "      <td>[(Now, 3, advmod), (Christie, 3, nsubj), (says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>60 Minutes executive producer Jeff Fager said ...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(60, NUM), (Minutes, NOUN), (executive, ADJ),...</td>\n",
       "      <td>[(60 Minutes, TIME), (Jeff Fager, PERSON), (on...</td>\n",
       "      <td>[(60, 2, nummod), (Minutes, 4, compound), (exe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>As Courtney Enlow pointed out in her viral ran...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(As, SCONJ), (Courtney, PROPN), (Enlow, PROPN...</td>\n",
       "      <td>[(Courtney Enlow, PERSON), (Sanders, PERSON)]</td>\n",
       "      <td>[(As, 4, mark), (Courtney, 4, nsubj), (Enlow, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>appropriate.Monica Baird, deputy medical direc...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(appropriate, ADJ), (., PUNCT), (Monica, PROP...</td>\n",
       "      <td>[(Monica Baird, PERSON), (North Bristol NHS, O...</td>\n",
       "      <td>[(appropriate, 0, root), (., 1, punct), (Monic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>If the U. S. comes with reckless military mane...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(If, SCONJ), (the, DET), (U., PROPN), (S., PR...</td>\n",
       "      <td>[(the U. S., GPE)]</td>\n",
       "      <td>[(If, 5, mark), (the, 4, det), (U., 4, compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>As a result, I think I missed kick-off the nig...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(As, ADP), (a, DET), (result, NOUN), (,, PUNC...</td>\n",
       "      <td>[(England, GPE), (France, GPE)]</td>\n",
       "      <td>[(As, 3, case), (a, 3, det), (result, 6, obl),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>I wouldn't be surprised if Steven Gerrard call...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(I, PRON), (would, AUX), (n't, PART), (be, AU...</td>\n",
       "      <td>[(Steven Gerrard, PERSON), (this World Cup, EV...</td>\n",
       "      <td>[(I, 5, nsubj), (would, 5, aux), (n't, 5, advm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>I see only one team good enoughMedia playback ...</td>\n",
       "      <td>1</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(I, PRON), (see, VERB), (only, ADV), (one, NU...</td>\n",
       "      <td>[(one, CARDINAL), (Moyes, PERSON), (Hammers', ...</td>\n",
       "      <td>[(I, 2, nsubj), (see, 0, root), (only, 5, advm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>Chevrette said that when the Sept. 11 attacks ...</td>\n",
       "      <td>2</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(Chevrette, PROPN), (said, VERB), (that, SCON...</td>\n",
       "      <td>[(Chevrette, PERSON), (Sept. 11, DATE), (Hanjo...</td>\n",
       "      <td>[(Chevrette, 2, nsubj), (said, 0, root), (that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>But those problems are re-emerging, rebels say...</td>\n",
       "      <td>0</td>\n",
       "      <td>self - 2</td>\n",
       "      <td>[(But, CCONJ), (those, DET), (problems, NOUN),...</td>\n",
       "      <td>[(Tiji, GPE), (pro-Gaddafi, NORP), (days, DATE...</td>\n",
       "      <td>[(But, 5, cc), (those, 3, det), (problems, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  Label Final_Result  \\\n",
       "128   If the U. S. comes with reckless military mane...      2     self - 2   \n",
       "181   After the terrorist attacks on 9/11 we respond...      0     self - 2   \n",
       "538   Whether we defeat Donald Trump before the conv...      0     self - 2   \n",
       "558   Now Christie says, ”I didn’t support Sonia Sot...      2     self - 2   \n",
       "572   We condemn Israeli attempts to prevent Egyptia...      2     self - 2   \n",
       "648   Bochy said Cain could handle the assignment bu...      0     self - 2   \n",
       "832   she stood waiting to enter the hall, Savage sa...      0     self - 2   \n",
       "1042  Now Christie says, ”I didn’t support Sonia Sot...      2     self - 2   \n",
       "1054  60 Minutes executive producer Jeff Fager said ...      2     self - 2   \n",
       "1133  As Courtney Enlow pointed out in her viral ran...      0     self - 2   \n",
       "2095  appropriate.Monica Baird, deputy medical direc...      0     self - 2   \n",
       "2196  If the U. S. comes with reckless military mane...      2     self - 2   \n",
       "2325  As a result, I think I missed kick-off the nig...      0     self - 2   \n",
       "2570  I wouldn't be surprised if Steven Gerrard call...      2     self - 2   \n",
       "2762  I see only one team good enoughMedia playback ...      1     self - 2   \n",
       "3200  Chevrette said that when the Sept. 11 attacks ...      2     self - 2   \n",
       "3822  But those problems are re-emerging, rebels say...      0     self - 2   \n",
       "\n",
       "                                             tokens_pos  \\\n",
       "128   [(If, SCONJ), (the, DET), (U., PROPN), (S., PR...   \n",
       "181   [(After, ADP), (the, DET), (terrorist, ADJ), (...   \n",
       "538   [(Whether, SCONJ), (we, PRON), (defeat, VERB),...   \n",
       "558   [(Now, ADV), (Christie, PROPN), (says, VERB), ...   \n",
       "572   [(We, PRON), (condemn, VERB), (Israeli, ADJ), ...   \n",
       "648   [(Bochy, PROPN), (said, VERB), (Cain, PROPN), ...   \n",
       "832   [(she, PRON), (stood, VERB), (waiting, VERB), ...   \n",
       "1042  [(Now, ADV), (Christie, PROPN), (says, VERB), ...   \n",
       "1054  [(60, NUM), (Minutes, NOUN), (executive, ADJ),...   \n",
       "1133  [(As, SCONJ), (Courtney, PROPN), (Enlow, PROPN...   \n",
       "2095  [(appropriate, ADJ), (., PUNCT), (Monica, PROP...   \n",
       "2196  [(If, SCONJ), (the, DET), (U., PROPN), (S., PR...   \n",
       "2325  [(As, ADP), (a, DET), (result, NOUN), (,, PUNC...   \n",
       "2570  [(I, PRON), (would, AUX), (n't, PART), (be, AU...   \n",
       "2762  [(I, PRON), (see, VERB), (only, ADV), (one, NU...   \n",
       "3200  [(Chevrette, PROPN), (said, VERB), (that, SCON...   \n",
       "3822  [(But, CCONJ), (those, DET), (problems, NOUN),...   \n",
       "\n",
       "                                               entities  \\\n",
       "128                                  [(the U. S., GPE)]   \n",
       "181   [(9/11, EVENT), (the Patriot Act, LAW), (Cabin...   \n",
       "538       [(Donald Trump, PERSON), (North Dakota, GPE)]   \n",
       "558     [(Christie, PERSON), (Sonia Sotomayor, PERSON)]   \n",
       "572   [(Israeli, NORP), (Egyptians, NORP), (Egypt, G...   \n",
       "648   [(Bochy, PERSON), (Cain, PERSON), (Johnson, PE...   \n",
       "832   [(Savage, PERSON), (Assange's, PERSON), (Savag...   \n",
       "1042    [(Christie, PERSON), (Sonia Sotomayor, PERSON)]   \n",
       "1054  [(60 Minutes, TIME), (Jeff Fager, PERSON), (on...   \n",
       "1133      [(Courtney Enlow, PERSON), (Sanders, PERSON)]   \n",
       "2095  [(Monica Baird, PERSON), (North Bristol NHS, O...   \n",
       "2196                                 [(the U. S., GPE)]   \n",
       "2325                    [(England, GPE), (France, GPE)]   \n",
       "2570  [(Steven Gerrard, PERSON), (this World Cup, EV...   \n",
       "2762  [(one, CARDINAL), (Moyes, PERSON), (Hammers', ...   \n",
       "3200  [(Chevrette, PERSON), (Sept. 11, DATE), (Hanjo...   \n",
       "3822  [(Tiji, GPE), (pro-Gaddafi, NORP), (days, DATE...   \n",
       "\n",
       "                                           dependencies  \n",
       "128   [(If, 5, mark), (the, 4, det), (U., 4, compoun...  \n",
       "181   [(After, 4, case), (the, 4, det), (terrorist, ...  \n",
       "538   [(Whether, 3, mark), (we, 3, nsubj), (defeat, ...  \n",
       "558   [(Now, 3, advmod), (Christie, 3, nsubj), (says...  \n",
       "572   [(We, 2, nsubj), (condemn, 0, root), (Israeli,...  \n",
       "648   [(Bochy, 2, nsubj), (said, 0, root), (Cain, 5,...  \n",
       "832   [(she, 2, nsubj), (stood, 0, root), (waiting, ...  \n",
       "1042  [(Now, 3, advmod), (Christie, 3, nsubj), (says...  \n",
       "1054  [(60, 2, nummod), (Minutes, 4, compound), (exe...  \n",
       "1133  [(As, 4, mark), (Courtney, 4, nsubj), (Enlow, ...  \n",
       "2095  [(appropriate, 0, root), (., 1, punct), (Monic...  \n",
       "2196  [(If, 5, mark), (the, 4, det), (U., 4, compoun...  \n",
       "2325  [(As, 3, case), (a, 3, det), (result, 6, obl),...  \n",
       "2570  [(I, 5, nsubj), (would, 5, aux), (n't, 5, advm...  \n",
       "2762  [(I, 2, nsubj), (see, 0, root), (only, 5, advm...  \n",
       "3200  [(Chevrette, 2, nsubj), (said, 0, root), (that...  \n",
       "3822  [(But, 5, cc), (those, 3, det), (problems, 5, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[df_train_ready_merged['Final_Result'] == 'self - 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca906596-78b7-4b4b-8426-eccb6403ee50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608e78b-0f23-4580-a96e-25df267b5523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c82975-c6c6-4c93-8007-861331393f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768bc06-bbb7-4a28-9678-538e2e33cb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3757815-dbdf-4fe8-884c-ca6ec9e29393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15061f9f-9247-437a-b578-619e2e8931a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9116c2-012b-4c3b-8fad-93ba000313a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9c75dc-5ae1-4504-a84b-30bb8aa66078",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "def get_event_polarity(verb, obj):\n",
    "    # Create a simple context for WSD\n",
    "    context = f\"{verb} {obj}\"\n",
    "    \n",
    "    # Word Sense Disambiguation for the verb and object\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    # Calculate polarity using SentiWordNet\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    # AFINN score\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    # Subjectivity Lexicon score\n",
    "    tokens = context.split()\n",
    "    subj_pos = sum([1 for token in tokens if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in tokens if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    # Determine final polarity\n",
    "    if pos_score > neg_score:\n",
    "        return '1'  # Positive/Praise\n",
    "    elif neg_score > pos_score:\n",
    "        return '2'  # Negative/Blame\n",
    "    else:\n",
    "        return '0'  # Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093c7296-2fa6-497d-9294-7f68d16ae0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events_and_agents(dependencies):\n",
    "    \n",
    "    events = []\n",
    "    verbs = []\n",
    "    \n",
    "    # Step 1: Identify all verbs that are 'root'\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                verbs.append((word, i + 1))  # Save the verb and its index (i+1)\n",
    "    \n",
    "    # Step 2: For each identified verb, find associated subjects and objects\n",
    "    for verb, verb_index in verbs:\n",
    "        subject = None\n",
    "        obj = None\n",
    "\n",
    "        for word, head, deprel in dependencies:\n",
    "            if head == verb_index:  # Compare with the index of the verb, not its head\n",
    "                if deprel in ['nsubj', 'nsubj:pass']:  # Subject of the verb\n",
    "                    subject = word\n",
    "                if deprel in ['obj', 'dobj']:  # Object of the verb\n",
    "                    obj = word\n",
    "        \n",
    "        if subject and obj:\n",
    "            # Calculate polarity specifically for this verb-object pair\n",
    "            polarity = get_event_polarity(verb, obj)\n",
    "            events.append({\n",
    "                'verb': verb,\n",
    "                'object': obj,\n",
    "                'agent': subject,\n",
    "                'polarity': polarity\n",
    "            })\n",
    "    \n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b090888-6087-4f22-becb-a97fb9e9fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['events'] = df_train_ready_merged['dependencies'].apply(extract_events_and_agents)\n",
    "df_valid_ready_merged['events'] = df_valid_ready_merged['dependencies'].apply(extract_events_and_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703541e-99e0-422e-923d-7ad187c8ba13",
   "metadata": {},
   "source": [
    "## Label rows without events \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a515fb98-4c09-4fd6-a01c-40bfb30316bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_final_result_column(df):\n",
    "    # Insert 'Final_Result' column after 'Label' with default value None\n",
    "    label_index = df.columns.get_loc('Label')\n",
    "    df.insert(label_index + 1, 'Final_Result', None)\n",
    "    \n",
    "    # Update 'Final_Result' to 0 where 'events' column is empty\n",
    "    df.loc[df['events'].apply(lambda x: not x), 'Final_Result'] = \"others\"\n",
    "\n",
    "# Apply the function to both dataframes\n",
    "add_final_result_column(df_train_ready_merged)\n",
    "add_final_result_column(df_valid_ready_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf515f-3d9d-40f2-b439-701cc625c035",
   "metadata": {},
   "source": [
    "## Step 2. Agent Causality\n",
    "\n",
    "\"Here, one must establish that a moral agent caused an event. We first make use of a popular explicit intra-sentential pattern for causation expression which is “NP verb NP” where NP is a noun phrase (Girju, 2003) and then we identify the agent within the noun phrase. If the intrasentential pattern is not found we consider verbs in the text that belong to the CAUSE class and the CAUSETO semantic relation which are defined in the WordNet. In order for “Agent Causality” taking the value “True”, the agent must be a person entity (including pronouns).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb437b77-e51e-479e-b426-9f423421414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_causative_verb(verb):\n",
    "    # Check if the verb is in the CAUSE class or has CAUSETO relation in WordNet\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a79467e-69f6-435f-b344-e5e8a8154de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_agent_causality(row):\n",
    "    dependencies = row['dependencies']\n",
    "    entities = row['entities']\n",
    "    agent_causality = False\n",
    "    verbs = []\n",
    "\n",
    "    # Step 1: Identify all verbs that are 'root'\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                verbs.append((word, i + 1))  # Save the verb and its index (i+1)\n",
    "    \n",
    "    # Step 2: For each identified verb, find associated subjects and objects\n",
    "    for verb, verb_index in verbs:\n",
    "        subject = None\n",
    "        obj = None\n",
    "\n",
    "        for word, head, deprel in dependencies:\n",
    "            if head == verb_index:  # Compare with the index of the verb, not its head\n",
    "                if deprel in ['nsubj', 'nsubjpass']:  # Subject of the verb\n",
    "                    subject = word\n",
    "                if deprel in ['obj', 'dobj']:  # Object of the verb\n",
    "                    obj = word\n",
    "        \n",
    "        # Step 3: Validate the agent (subject)\n",
    "        agent_is_valid = False\n",
    "        if subject:\n",
    "            for entity, label in entities:\n",
    "                if entity == subject and label in ['PERSON', 'ORG', 'GPE']:\n",
    "                    agent_is_valid = True\n",
    "                    break\n",
    "            \n",
    "            if not agent_is_valid and 'PRP' in [pos for token, pos in row['tokens_pos'] if token == subject]:\n",
    "                agent_is_valid = True  # It's a pronoun\n",
    "\n",
    "        # Step 4: If both subject and object are found and agent is valid, we have causality\n",
    "        if subject and obj and agent_is_valid:\n",
    "            agent_causality = True\n",
    "            break\n",
    "    \n",
    "    # Step 5: If no \"NP verb NP\" pattern is found, check for causative verbs\n",
    "    if not agent_causality:\n",
    "        for verb, verb_index in verbs:\n",
    "            if check_causative_verb(verb):\n",
    "                agent_causality = True\n",
    "                break\n",
    "    \n",
    "    return agent_causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f0d350-18d2-4bb4-87b7-b9c8b3d732ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_ready_merged['agent_causality'] = df_train_ready_merged.apply(identify_agent_causality, axis=1)\n",
    "df_valid_ready_merged['agent_causality'] = df_valid_ready_merged.apply(identify_agent_causality, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e7abf-087a-4181-a48f-4ec4cdcf2f22",
   "metadata": {},
   "source": [
    "## Label rows without agent causality \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4ae980d-33f4-4f60-9f8e-85ee955684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where agent_causality is False in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['agent_causality'] == False, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where agent_causality is False in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['agent_causality'] == False, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6d564-e230-4694-8844-cf779df7f054",
   "metadata": {},
   "source": [
    "## Step 3. Foreseeability\n",
    "\n",
    "\"Foreseeability. We rely on a set of verbs which indicate foreseeability. These include verbs of communication as suggested in (Mao et al, 2011) and other verb classes which include verbs of creation, verbs of consumption, verbs of competition, verbs of possession and verbs of motion. These classes of verbs are defined in the WordNet7 and can be identified by looking at the WordNet sensekey of the verbs. Example: When I did not speak the truth. In the example above, the communication verb “speak” indicates that the subject “I” had foreknowledge of the event of “speaking the truth”.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defae370-1030-4e1d-82aa-38d92af27075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_foreseeability(row):\n",
    "    dependencies = row['dependencies']\n",
    "    foreseeable = False\n",
    "    \n",
    "    # Identify all verbs in the sentence\n",
    "    verbs = [word for word, head, deprel in dependencies if deprel == 'root']\n",
    "\n",
    "    # Check if any verb indicates foreseeability\n",
    "    for verb in verbs:\n",
    "        if is_foreseeable_verb(verb):\n",
    "            foreseeable = True\n",
    "            break\n",
    "    \n",
    "    return foreseeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fad26a0-54ff-4b1d-96d0-797a14714e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a verb belongs to any foreseeability-related classes\n",
    "def is_foreseeable_verb(verb):\n",
    "    #print(\"NEW\")\n",
    "    #print(verb)\n",
    "    \n",
    "    # Synset categories indicating foreseeability\n",
    "    foreseeability_classes = {\n",
    "        'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'\n",
    "    }\n",
    "\n",
    "    # Get the synsets for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        # Check if the verb belongs to any of the foreseeability classes\n",
    "        lexname = synset.lexname().split('.')[1]\n",
    "        if lexname in foreseeability_classes:\n",
    "            #print(synset.lexname())\n",
    "            #print(synset)\n",
    "            #print(lexname)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5487971-b003-4849-9b54-99a4cfe821fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_ready_merged['foreseeability'] = df_train_ready_merged.apply(determine_foreseeability, axis=1)\n",
    "df_valid_ready_merged['foreseeability'] = df_valid_ready_merged.apply(determine_foreseeability, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb67f423-8435-4508-836b-6e9d6ad80ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foreseeability\n",
       "True     4248\n",
       "False     784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"foreseeability\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f586af-563c-477b-81bc-0afd08b187e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foreseeability\n",
       "True     456\n",
       "False     94\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"foreseeability\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46be908-1ea0-4d86-bf0c-19a3ce5628aa",
   "metadata": {},
   "source": [
    "## Label rows without foreseeability \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b43bbd8-4739-4d9a-9da3-db92cd5cf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where foreseeability is False in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['foreseeability'] == False, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where foreseeability is False in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['foreseeability'] == False, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acc753-293f-40f7-bcd5-33706d86f8e6",
   "metadata": {},
   "source": [
    "## Step 4. Coercion\n",
    "\n",
    "\"To identify coercion, we look at the extension verb classes presented in (Kipper et al, 2006) focusing on verbs in the URGE (13 members), FORCE (46 members) and FORBID (17 members) classes. Example: I was forced to quite the job in the city. In the example above , using word sense disambiguation, the verb “forced” is of sense “to cause to do through pressure or necessity, by physical, moral or intellectual means”. The agent “I” in this case did not willingly quite the job and the sentence does not mention who forced the agent. Thus, the sentence is classified as “Others” (i.e., no blame or praise).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e957c6c-8bc1-4eec-8328-681688346142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_coercion(row):\n",
    "    dependencies = row['dependencies']\n",
    "    coercion = False\n",
    "    \n",
    "    # Identify all verbs in the sentence\n",
    "    verbs = [word for word, head, deprel in dependencies if deprel == 'root']\n",
    "    \n",
    "    # Check if any verb indicates coercion\n",
    "    for verb in verbs:\n",
    "        if is_coercion_verb(verb):\n",
    "            coercion = True\n",
    "            break\n",
    "    \n",
    "    return coercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a66db4-4305-4530-bf76-da83e68682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a verb belongs to any coercion-related VerbNet classes\n",
    "def is_coercion_verb(verb):\n",
    "    coercion_classes = {\n",
    "        'urge-58.1', 'force-59', 'forbid-67'\n",
    "    }\n",
    "    \n",
    "    # Get the VerbNet classes for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        #print(synset)\n",
    "        lemma = synset.lemmas()[0]\n",
    "        #print(lemma)\n",
    "        vn_classes = lemma.key().split('%')[0]\n",
    "        vn_class_ids = vn.classids(vn_classes)\n",
    "        #print(vn_classes)\n",
    "        #print(vn_class_ids)\n",
    "        \n",
    "        # Check if any VerbNet class matches the coercion classes\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636e8008-81ea-4398-8179-f62dc0c6ee91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to both datasets\n",
    "df_train_ready_merged['coercion'] = df_train_ready_merged.apply(determine_coercion, axis=1)\n",
    "df_valid_ready_merged['coercion'] = df_valid_ready_merged.apply(determine_coercion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce37fd5-c0c7-4803-b730-ba2283e7e6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coercion\n",
       "False    4679\n",
       "True      353\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"coercion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e151c5a-eda9-4e12-8539-ca0d089dec54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coercion\n",
       "False    509\n",
       "True      41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"coercion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271831c0-82c8-45e8-be5d-be8f88d3276d",
   "metadata": {},
   "source": [
    "## Label rows with coercion \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de490b1b-3186-4ec5-b589-2144cbd5a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where coercion is True in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['coercion'] == True, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where coercion is True in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['coercion'] == True, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59c24d-5c1c-4e34-bf9e-26df02f7444e",
   "metadata": {},
   "source": [
    "## Negation sucks - redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2dea03-e3b0-4e0d-8281-390515e2fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(dependencies):\n",
    "    negations = [word for word, head, deprel in dependencies if deprel == 'advmod' and (word == 'not' or word == 'n’t')]\n",
    "    return negations\n",
    "\n",
    "df_train_ready_merged['negations'] = df_train_ready_merged['dependencies'].apply(handle_negation)\n",
    "df_valid_ready_merged['negations'] = df_valid_ready_merged['dependencies'].apply(handle_negation)\n",
    "\n",
    "def adjust_sentiment_for_negation(row):\n",
    "    if row['negations']:\n",
    "        if row['final_sentiment'] == \"positive\":\n",
    "            return \"negative\"\n",
    "        elif row['final_sentiment'] == \"negative\":\n",
    "            return \"positive\"\n",
    "    return row['final_sentiment']\n",
    "\n",
    "df_train_ready_merged['final_sentiment_adj'] = df_train_ready_merged.apply(adjust_sentiment_for_negation, axis=1)\n",
    "df_valid_ready_merged['final_sentiment_adj'] = df_valid_ready_merged.apply(adjust_sentiment_for_negation, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307e4da-b2ee-43dc-a32c-9481927a0f13",
   "metadata": {},
   "source": [
    "## Step 5. Final Classification (Blame/Praise/Others)\n",
    "\n",
    "0 - neutral, 1 - positive, 2 - negative - Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "495ef68c-5e2a-42b8-935d-fc387aa052aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>swn_scores</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>subj_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>negations</th>\n",
       "      <th>final_sentiment_adj</th>\n",
       "      <th>events</th>\n",
       "      <th>agent_causality</th>\n",
       "      <th>foreseeability</th>\n",
       "      <th>coercion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "      <td>(0.0, 0.75)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[{'verb': 'supporting', 'object': 'Clinton', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence  Label Final_Result  \\\n",
       "0  George is not supporting Clinton.      2         None   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "\n",
       "                                entities  \\\n",
       "0  [(George, PERSON), (Clinton, PERSON)]   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "\n",
       "                                        dependencies   swn_scores  \\\n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  (0.0, 0.75)   \n",
       "\n",
       "   afinn_score subj_scores final_sentiment negations final_sentiment_adj  \\\n",
       "0          1.0      (1, 0)        positive     [not]            negative   \n",
       "\n",
       "                                              events  agent_causality  \\\n",
       "0  [{'verb': 'supporting', 'object': 'Clinton', '...             True   \n",
       "\n",
       "   foreseeability  coercion  \n",
       "0            True     False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2b082ab-f8d5-4ca3-8423-206847754851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(row):\n",
    "    event_present = row['events'] is not None and len(row['events']) > 0\n",
    "    agent_causality = row['agent_causality']\n",
    "    foreseeability = row['foreseeability']\n",
    "    coercion = row['coercion']\n",
    "    final_sentiment = row['final_sentiment_adj']  # Assuming this is precomputed as positive, negative, or neutral\n",
    "    final_result = row['Final_Result']\n",
    "    \n",
    "    if final_result == None:\n",
    "        if agent_causality and foreseeability and not coercion:\n",
    "            #print(row['events'][0].get('agent'))\n",
    "            if final_sentiment == 'negative':\n",
    "                if row['events'][0].get('agent') == \"I\":\n",
    "                    #print(row['events'][0].get('agent'))\n",
    "                    return \"self-blame\"\n",
    "                else:\n",
    "                    return \"blame-others\"\n",
    "            elif final_sentiment == 'positive':\n",
    "                if row['events'][0].get('agent') == \"I\":\n",
    "                    #print(row['events'][0].get('agent'))\n",
    "                    return \"self-praise\"\n",
    "                else:\n",
    "                    return \"praise-others\"\n",
    "            if final_sentiment == 'neutral':\n",
    "                return \"others\"\n",
    "        else: return \"others\"\n",
    "    else:\n",
    "        return \"others\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2acde65-7380-4e75-96fb-7ce02bc39383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the classification to each row in the DataFrame\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(classify_sentence, axis=1)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(classify_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4672b2c0-d9bd-4486-9f1b-47409dba4359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>swn_scores</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>subj_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>negations</th>\n",
       "      <th>final_sentiment_adj</th>\n",
       "      <th>events</th>\n",
       "      <th>agent_causality</th>\n",
       "      <th>foreseeability</th>\n",
       "      <th>coercion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>blame-others</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "      <td>(0.0, 0.75)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[{'verb': 'supporting', 'object': 'Clinton', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence  Label  Final_Result  \\\n",
       "0  George is not supporting Clinton.      2  blame-others   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "\n",
       "                                entities  \\\n",
       "0  [(George, PERSON), (Clinton, PERSON)]   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "\n",
       "                                        dependencies   swn_scores  \\\n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  (0.0, 0.75)   \n",
       "\n",
       "   afinn_score subj_scores final_sentiment negations final_sentiment_adj  \\\n",
       "0          1.0      (1, 0)        positive     [not]            negative   \n",
       "\n",
       "                                              events  agent_causality  \\\n",
       "0  [{'verb': 'supporting', 'object': 'Clinton', '...             True   \n",
       "\n",
       "   foreseeability  coercion  \n",
       "0            True     False  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a2f83aa-cf27-47a4-bc38-62decfcc66a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>blame-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan has endorsed Trump and told reporters thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>praise-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John McGraw, 78, was charged with assault and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Filipino fighter unleashed a dazzling comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But the Marlins have failed to make the postse...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label   Final_Result\n",
       "0                  George is not supporting Clinton.      2   blame-others\n",
       "1  Ryan has endorsed Trump and told reporters thi...      1  praise-others\n",
       "2  John McGraw, 78, was charged with assault and ...      2         others\n",
       "3  The Filipino fighter unleashed a dazzling comb...      1         others\n",
       "4  But the Marlins have failed to make the postse...      0         others"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[['Sentence', 'Label', 'Final_Result']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "609bc38e-41d9-4aa0-a33f-08753e1c3805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "others           4512\n",
       "blame-others      282\n",
       "praise-others     238\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"Final_Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07696d4b-4fbc-4936-a593-9262710860fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actress Patty Duke died on Tuesday at age 69, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who showrunner, Moffat, will give a masterclas...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Patriots keeping Brady on the bench after ...</td>\n",
       "      <td>0</td>\n",
       "      <td>praise-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia accounted for about 9 percent of Totals...</td>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An Post survey suggests a majority of voters t...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label   Final_Result\n",
       "0  Actress Patty Duke died on Tuesday at age 69, ...      0         others\n",
       "1  Who showrunner, Moffat, will give a masterclas...      0         others\n",
       "2  The Patriots keeping Brady on the bench after ...      0  praise-others\n",
       "3  Russia accounted for about 9 percent of Totals...      2         others\n",
       "4  An Post survey suggests a majority of voters t...      0         others"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c7f8f0-19fb-4a79-8155-e21eb92ee82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "others           480\n",
       "blame-others      45\n",
       "praise-others     25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"Final_Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80232b5-0c37-466e-9c42-8022250cea6e",
   "metadata": {},
   "source": [
    "## Step 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427d92c-2bc0-43d1-9e4a-e7ac7590985c",
   "metadata": {},
   "source": [
    "### Map values in Final_Result column to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d74c33e-c339-4f34-9040-a776c93b3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_16104\\2176863739.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_16104\\2176863739.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {\"others\": 0, \"blame-others\": 2, \"praise-others\": 1}\n",
    "\n",
    "# 0 - neutral, 1 - praise, 2 - blame\n",
    "\n",
    "# Apply the mapping to the 'Final_Result' column\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ee14185-795a-4684-92e8-06358fb39bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval = df_train_ready_merged[['Sentence', 'Label', 'Final_Result']]\n",
    "df_valid_eval = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4565ebbc-4c32-484d-b540-e77af387f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 20 random rows from each DataFrame\n",
    "df_train_sample = df_train_ready_merged.sample(n=20, random_state=1)\n",
    "df_valid_sample = df_valid_ready_merged.sample(n=20, random_state=1)\n",
    "\n",
    "# Save them to an Excel file with different sheets\n",
    "with pd.ExcelWriter('sampled_data.xlsx') as writer:\n",
    "    df_train_sample.to_excel(writer, sheet_name='Train_Sample', index=False)\n",
    "    df_valid_sample.to_excel(writer, sheet_name='Valid_Sample', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e40a5-4319-4f03-921d-cd7c3953eb84",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81a296a3-fa30-49fa-a7f1-701772bc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_train = df_train_eval['Label']\n",
    "y_pred_train = df_train_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29b674ec-f8c7-4c94-a771-73660a1670f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.565382       0.363339          0.475443\n",
      "1  Precision       0.565382       0.537041               NaN\n",
      "2     Recall       0.565382       0.391648               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.94      0.71      2733\n",
      "           1       0.36      0.11      0.17       798\n",
      "           2       0.68      0.13      0.22      1501\n",
      "\n",
      "    accuracy                           0.57      5032\n",
      "   macro avg       0.54      0.39      0.36      5032\n",
      "weighted avg       0.57      0.57      0.48      5032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "f1_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "f1_weighted = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_train, y_pred_train, average='micro')\n",
    "precision_macro = precision_score(y_true_train, y_pred_train, average='macro')\n",
    "recall_micro = recall_score(y_true_train, y_pred_train, average='micro')\n",
    "recall_macro = recall_score(y_true_train, y_pred_train, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46642122-b89b-49fc-9463-56e7b5c447c8",
   "metadata": {},
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8fa5b22-60a0-4dcb-b334-223b84179568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_valid = df_valid_eval['Label']\n",
    "y_pred_valid = df_valid_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dfaea35-0810-4955-870e-75195f16a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.589091       0.400254          0.514723\n",
      "1  Precision       0.589091       0.552778               NaN\n",
      "2     Recall       0.589091       0.412998               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.93      0.72       305\n",
      "           1       0.40      0.13      0.19        78\n",
      "           2       0.67      0.18      0.28       167\n",
      "\n",
      "    accuracy                           0.59       550\n",
      "   macro avg       0.55      0.41      0.40       550\n",
      "weighted avg       0.59      0.59      0.51       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_valid, y_pred_valid, average='micro')\n",
    "f1_macro = f1_score(y_true_valid, y_pred_valid, average='macro')\n",
    "f1_weighted = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_valid, y_pred_valid, average='micro')\n",
    "precision_macro = precision_score(y_true_valid, y_pred_valid, average='macro')\n",
    "recall_micro = recall_score(y_true_valid, y_pred_valid, average='micro')\n",
    "recall_macro = recall_score(y_true_valid, y_pred_valid, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2f4fd-558a-477f-8e2f-821ce4185d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ccda0-0a42-49b7-a418-ed2725456ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6e69f9-ebee-4e47-b287-159f060a1bdd",
   "metadata": {},
   "source": [
    "# NEED TO CHANGE LABELS OF TEST DATA FILE AND PREPROCESS IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4a4f8-3eca-4225-bf11-1a320bfd5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

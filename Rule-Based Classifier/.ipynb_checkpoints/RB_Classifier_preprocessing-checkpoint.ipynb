{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3c7715-37fc-438c-adcd-6e35a645887b",
   "metadata": {
    "id": "4b3c7715-37fc-438c-adcd-6e35a645887b"
   },
   "source": [
    "## Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634b2faf-13e3-44da-b6d4-46daf8b3d52a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "634b2faf-13e3-44da-b6d4-46daf8b3d52a",
    "outputId": "45b06265-a21e-4576-eed1-cee370c50fb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('incrimination.n.01'), Synset('blame.n.02'), Synset('blame.v.01'), Synset('blame.v.02'), Synset('blame.v.03'), Synset('blasted.s.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Example: Finding synonyms for a word\n",
    "synonyms = wordnet.synsets('blame')\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44616a38-2c4e-4e33-8c9c-a8367fd76af0",
   "metadata": {
    "id": "44616a38-2c4e-4e33-8c9c-a8367fd76af0"
   },
   "source": [
    "## Tokenization and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8786510-44d7-4715-94e6-f2a0c3b9078e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8786510-44d7-4715-94e6-f2a0c3b9078e",
    "outputId": "e65daaa1-391a-4cd5-e288-27d64da5498c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('responsible', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('failure', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Example: POS tagging a sentence\n",
    "sentence = \"He is responsible for the failure.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc200-36f4-4e01-9dee-abe66385ddbc",
   "metadata": {
    "id": "c8bdc200-36f4-4e01-9dee-abe66385ddbc"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679b42e0-84f6-4a57-81db-1d38d2466a6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "679b42e0-84f6-4a57-81db-1d38d2466a6a",
    "outputId": "9df10951-545a-4d8b-d408-b4ae18a08e67"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentiwordnet \u001b[38;5;28;01mas\u001b[39;00m swn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example: Getting sentiment scores for a word\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mswn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msenti_synsets\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblame\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mpos_score()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Negative: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mneg_score()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Objective: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mobj_score()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\reader\\sentiwordnet.py:53\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader.__init__\u001b[1;34m(self, root, fileids, encoding)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one file must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_src_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\reader\\sentiwordnet.py:58\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader._parse_src_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileids[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m     57\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m((\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*#\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)), lines)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[0;32m     59\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt+\u001b[39m\u001b[38;5;124m\"\u001b[39m, line)]\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\reader\\sentiwordnet.py:57\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader._parse_src_file.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_src_file\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     56\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileids[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m---> 57\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m((\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), lines)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[0;32m     59\u001b[0m         fields \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt+\u001b[39m\u001b[38;5;124m\"\u001b[39m, line)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Example: Getting sentiment scores for a word\n",
    "word = list(swn.senti_synsets('blame'))[0]\n",
    "print(f\"Positive: {word.pos_score()}, Negative: {word.neg_score()}, Objective: {word.obj_score()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7790052-ef05-469d-847d-c138b8b8d913",
   "metadata": {
    "id": "f7790052-ef05-469d-847d-c138b8b8d913"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411920ea-b43c-467d-9d1a-05a30506fbb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "411920ea-b43c-467d-9d1a-05a30506fbb9",
    "outputId": "255e3df5-5c1d-4041-cd36-d86e1fe237a3"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Example: Tokenizing a paragraph into sentences and words\n",
    "paragraph = \"This is a sentence. Here is another one.\"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Words:\", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c2050-dce8-4e05-b0fa-0862bd1565f7",
   "metadata": {
    "id": "cc8c2050-dce8-4e05-b0fa-0862bd1565f7"
   },
   "source": [
    "## Open Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831d4b8-de0a-46ea-beb2-4d20b8277f14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1831d4b8-de0a-46ea-beb2-4d20b8277f14",
    "outputId": "f5de8b8e-1552-4e6d-e6b7-16b7bfdc2862"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Example: Accessing positive and negative word lists\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "print(\"Positive words example:\", positive_words[:5])\n",
    "print(\"Negative words example:\", negative_words[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e731c-fea2-47f8-84fc-fcfae836bf1c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f76ffa-f9bd-45bf-ab56-f441f88c5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import ast\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XgpcdWkBTA2i",
   "metadata": {
    "id": "XgpcdWkBTA2i"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OXxtrS-FTIcH",
   "metadata": {
    "id": "OXxtrS-FTIcH"
   },
   "source": [
    "* datasets_cleaned/test_cleaned\n",
    "* datasets_cleaned/train_cleaned\n",
    "* datasets_cleaned/valid_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qx4EcsPvYerO",
   "metadata": {
    "id": "Qx4EcsPvYerO"
   },
   "source": [
    "Need to shuffle the data because the rows are sorted by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "SorE_ErxS9FO",
   "metadata": {
    "id": "SorE_ErxS9FO"
   },
   "outputs": [],
   "source": [
    "column_names = [\"Sentence\", \"Ent1\", \"Ent2\", \"Label\"]\n",
    "\n",
    "df_train = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Real_News_Data_preparation/datasets_only_same_ents_clean/train.txt', sep='\\t', header=None, names=column_names)\n",
    "df_train_dropped = df_train.drop(columns=[\"Ent1\", \"Ent2\"])\n",
    "df_train_shuffled = df_train_dropped.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_valid = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Real_News_Data_preparation/datasets_only_same_ents_clean/valid.txt', sep='\\t', header=None, names=column_names)\n",
    "df_valid_dropped = df_valid.drop(columns=[\"Ent1\", \"Ent2\"])\n",
    "df_valid_shuffled = df_valid_dropped.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_test = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Real_News_Data_preparation/datasets_only_same_ents_clean/test.txt', sep='\\t', header=None, names=column_names)\n",
    "df_test_dropped = df_test.drop(columns=[\"Ent1\", \"Ent2\"])\n",
    "df_test_shuffled = df_test_dropped.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c1c19-2c75-49ca-953f-971d5945dd14",
   "metadata": {
    "id": "861c1c19-2c75-49ca-953f-971d5945dd14"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "How to Sequence These Steps\n",
    "The preprocessing pipeline generally follows this order:\n",
    "- Sentence Splitting and Tokenization\n",
    "- POS Tagging\n",
    "- Named Entity Recognition (NER)\n",
    "- Word Sense Disambiguation (WSD)\n",
    "- Dependency Parsing (including verb-object detection)\n",
    "- Polarity Detection (Sentiment Analysis) - SentiWordNet, AFINN and the Subjectivity Lexicon\n",
    "- Negation Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d8cb3-86c2-4118-bfd2-a9afa7f5d4a3",
   "metadata": {
    "id": "462d8cb3-86c2-4118-bfd2-a9afa7f5d4a3"
   },
   "source": [
    "## Step 1: Tokenization, POS Tagging, and Named Entity Recognition (NER) using Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6347a9a-04af-4de6-8b26-0da6552cd393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 13:53:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d443530206c74f0e9db955a492a9acf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 13:53:50 INFO: Downloaded file to C:\\Users\\Anastasiia Belkina\\stanza_resources\\resources.json\n",
      "2024-09-11 13:53:51 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-09-11 13:53:51 INFO: Using device: cpu\n",
      "2024-09-11 13:53:51 INFO: Loading: tokenize\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-09-11 13:53:52 INFO: Loading: mwt\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\mwt\\trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-09-11 13:53:52 INFO: Loading: pos\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-09-11 13:53:52 INFO: Loading: lemma\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\lemma\\trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-09-11 13:53:52 INFO: Loading: depparse\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-09-11 13:53:52 INFO: Loading: ner\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\ner\\trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-09-11 13:53:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stanza pipeline\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n",
    "\n",
    "# Tokenization, POS tagging, and NER\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]\n",
    "    entities = [(ent.text, ent.type) for ent in doc.entities]\n",
    "    return tokens, entities\n",
    "\n",
    "df_train_shuffled['tokens_pos'], df_train_shuffled['entities'] = zip(*df_train_shuffled['Sentence'].apply(process_text))\n",
    "df_valid_shuffled['tokens_pos'], df_valid_shuffled['entities'] = zip(*df_valid_shuffled['Sentence'].apply(process_text))\n",
    "df_test_shuffled['tokens_pos'], df_test_shuffled['entities'] = zip(*df_test_shuffled['Sentence'].apply(process_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c1923-27c4-4044-9266-96712fa73b06",
   "metadata": {
    "id": "db0c1923-27c4-4044-9266-96712fa73b06"
   },
   "source": [
    "## Step 2: Dependency Parsing using Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0acc62c9-0ac2-4b36-b4ad-823d30eff69a",
   "metadata": {
    "id": "0acc62c9-0ac2-4b36-b4ad-823d30eff69a"
   },
   "outputs": [],
   "source": [
    "def dependency_parse(text):\n",
    "    doc = nlp(text)\n",
    "    dependencies = [(word.text, word.head, word.deprel) for sent in doc.sentences for word in sent.words]\n",
    "    return dependencies\n",
    "\n",
    "df_train_shuffled['dependencies'] = df_train_shuffled['Sentence'].apply(dependency_parse)\n",
    "df_valid_shuffled['dependencies'] = df_valid_shuffled['Sentence'].apply(dependency_parse)\n",
    "df_test_shuffled['dependencies'] = df_test_shuffled['Sentence'].apply(dependency_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2513d1-be44-4aa0-84c0-47248ab4bba5",
   "metadata": {},
   "source": [
    "## Checking the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c41a1a-c92a-4413-a397-db790b5e880a",
   "metadata": {
    "id": "b8c41a1a-c92a-4413-a397-db790b5e880a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a. m. Initial eyewitness accounts of such inci...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(a., X), (m., NOUN), (Initial, ADJ), (eyewitn...</td>\n",
       "      <td>[(British, NORP), (Cox’s, PERSON)]</td>\n",
       "      <td>[(a., 10, dep), (m., 10, nsubj), (Initial, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shortly after the beginning of the attack, the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Shortly, ADV), (after, ADP), (the, DET), (be...</td>\n",
       "      <td>[(Talibans, NORP), (Zabihullah Mujahid, PERSON)]</td>\n",
       "      <td>[(Shortly, 4, advmod), (after, 4, case), (the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Judge Pryor initially supported Judge Moore bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Judge, NOUN), (Pryor, PROPN), (initially, AD...</td>\n",
       "      <td>[(Pryor, PERSON), (Moore, PERSON)]</td>\n",
       "      <td>[(Judge, 4, nsubj), (Pryor, 1, flat), (initial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump also expects to receive a major new fina...</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Trump, PROPN), (also, ADV), (expects, VERB),...</td>\n",
       "      <td>[(Trump, PERSON), (the United States, GPE), (t...</td>\n",
       "      <td>[(Trump, 3, nsubj), (also, 3, advmod), (expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just decentralisation.Mr Purcell praised the C...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(just, ADV), (decentralisation, NOUN), (., PU...</td>\n",
       "      <td>[(Purcell, PERSON), (Coalition, ORG)]</td>\n",
       "      <td>[(just, 2, advmod), (decentralisation, 0, root...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0  a. m. Initial eyewitness accounts of such inci...      0   \n",
       "1  Shortly after the beginning of the attack, the...      1   \n",
       "2  Judge Pryor initially supported Judge Moore bu...      0   \n",
       "3  Trump also expects to receive a major new fina...      3   \n",
       "4  just decentralisation.Mr Purcell praised the C...      1   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(a., X), (m., NOUN), (Initial, ADJ), (eyewitn...   \n",
       "1  [(Shortly, ADV), (after, ADP), (the, DET), (be...   \n",
       "2  [(Judge, NOUN), (Pryor, PROPN), (initially, AD...   \n",
       "3  [(Trump, PROPN), (also, ADV), (expects, VERB),...   \n",
       "4  [(just, ADV), (decentralisation, NOUN), (., PU...   \n",
       "\n",
       "                                            entities  \\\n",
       "0                 [(British, NORP), (Cox’s, PERSON)]   \n",
       "1   [(Talibans, NORP), (Zabihullah Mujahid, PERSON)]   \n",
       "2                 [(Pryor, PERSON), (Moore, PERSON)]   \n",
       "3  [(Trump, PERSON), (the United States, GPE), (t...   \n",
       "4              [(Purcell, PERSON), (Coalition, ORG)]   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(a., 10, dep), (m., 10, nsubj), (Initial, 5, ...  \n",
       "1  [(Shortly, 4, advmod), (after, 4, case), (the,...  \n",
       "2  [(Judge, 4, nsubj), (Pryor, 1, flat), (initial...  \n",
       "3  [(Trump, 3, nsubj), (also, 3, advmod), (expect...  \n",
       "4  [(just, 2, advmod), (decentralisation, 0, root...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca38b83c-f07c-4973-ab0e-dd933c3a0678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U. S. equities started mostly flat on Wednesda...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(U., PROPN), (S., PROPN), (equities, NOUN), (...</td>\n",
       "      <td>[(U. S., ORG), (Wednesday, DATE), (Republican,...</td>\n",
       "      <td>[(U., 3, compound), (S., 3, compound), (equiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: Rubio supporters cheer Jeb!</td>\n",
       "      <td>0</td>\n",
       "      <td>[(:, PUNCT), (Rubio, PROPN), (supporters, NOUN...</td>\n",
       "      <td>[(Rubio, PERSON), (Jeb, PERSON)]</td>\n",
       "      <td>[(:, 4, punct), (Rubio, 3, compound), (support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qatars announcement of plans to boost LNG outp...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(Qatars, PROPN), (announcement, NOUN), (of, A...</td>\n",
       "      <td>[(Qatars, ORG), (LNG, ORG), (Gulf, LOC), (Saud...</td>\n",
       "      <td>[(Qatars, 2, compound), (announcement, 10, nsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man who said that he was the one later detai...</td>\n",
       "      <td>3</td>\n",
       "      <td>[(A, DET), (man, NOUN), (who, PRON), (said, VE...</td>\n",
       "      <td>[(Frank Flack, PERSON), (Edwards, PERSON)]</td>\n",
       "      <td>[(A, 2, det), (man, 19, nsubj), (who, 4, nsubj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>player.The entire game was intensely played, c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(player, NOUN), (., PUNCT), (The, DET), (enti...</td>\n",
       "      <td>[(Priory, ORG), (Buck Matthews, PERSON)]</td>\n",
       "      <td>[(player, 0, root), (., 1, punct), (The, 3, de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0  U. S. equities started mostly flat on Wednesda...      0   \n",
       "1                      : Rubio supporters cheer Jeb!      0   \n",
       "2  Qatars announcement of plans to boost LNG outp...      2   \n",
       "3  A man who said that he was the one later detai...      3   \n",
       "4  player.The entire game was intensely played, c...      0   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(U., PROPN), (S., PROPN), (equities, NOUN), (...   \n",
       "1  [(:, PUNCT), (Rubio, PROPN), (supporters, NOUN...   \n",
       "2  [(Qatars, PROPN), (announcement, NOUN), (of, A...   \n",
       "3  [(A, DET), (man, NOUN), (who, PRON), (said, VE...   \n",
       "4  [(player, NOUN), (., PUNCT), (The, DET), (enti...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [(U. S., ORG), (Wednesday, DATE), (Republican,...   \n",
       "1                   [(Rubio, PERSON), (Jeb, PERSON)]   \n",
       "2  [(Qatars, ORG), (LNG, ORG), (Gulf, LOC), (Saud...   \n",
       "3         [(Frank Flack, PERSON), (Edwards, PERSON)]   \n",
       "4           [(Priory, ORG), (Buck Matthews, PERSON)]   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(U., 3, compound), (S., 3, compound), (equiti...  \n",
       "1  [(:, 4, punct), (Rubio, 3, compound), (support...  \n",
       "2  [(Qatars, 2, compound), (announcement, 10, nsu...  \n",
       "3  [(A, 2, det), (man, 19, nsubj), (who, 4, nsubj...  \n",
       "4  [(player, 0, root), (., 1, punct), (The, 3, de...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68bda171-9b82-4f3c-babf-10fd2460f6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In November 2015 Donald Trump proclaimed to a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(In, ADP), (November, PROPN), (2015, NUM), (D...</td>\n",
       "      <td>[(November 2015, DATE), (Donald Trump, PERSON)...</td>\n",
       "      <td>[(In, 3, case), (November, 6, obl), (2015, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berlusconi's allies accused the Milan magistra...</td>\n",
       "      <td>3</td>\n",
       "      <td>[(Berlusconi, PROPN), ('s, PART), (allies, NOU...</td>\n",
       "      <td>[(Berlusconi's, NORP), (Milan, GPE)]</td>\n",
       "      <td>[(Berlusconi, 3, nmod:poss), ('s, 1, case), (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But when Laila called, she was told that Arlin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(But, CCONJ), (when, ADV), (Laila, PROPN), (c...</td>\n",
       "      <td>[(Laila, PERSON), (Arlington, GPE)]</td>\n",
       "      <td>[(But, 8, cc), (when, 4, advmod), (Laila, 4, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Results for the 801 likely Democratic primary ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(Results, NOUN), (for, ADP), (the, DET), (801...</td>\n",
       "      <td>[(801, CARDINAL), (Democratic, NORP), (plus or...</td>\n",
       "      <td>[(Results, 9, nsubj), (for, 8, case), (the, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The continuing acceleration of the number of S...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(The, DET), (continuing, VERB), (acceleration...</td>\n",
       "      <td>[(Syrian, NORP), (Obama, PERSON)]</td>\n",
       "      <td>[(The, 3, det), (continuing, 3, amod), (accele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0  In November 2015 Donald Trump proclaimed to a ...      0   \n",
       "1  Berlusconi's allies accused the Milan magistra...      3   \n",
       "2  But when Laila called, she was told that Arlin...      0   \n",
       "3  Results for the 801 likely Democratic primary ...      0   \n",
       "4  The continuing acceleration of the number of S...      0   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(In, ADP), (November, PROPN), (2015, NUM), (D...   \n",
       "1  [(Berlusconi, PROPN), ('s, PART), (allies, NOU...   \n",
       "2  [(But, CCONJ), (when, ADV), (Laila, PROPN), (c...   \n",
       "3  [(Results, NOUN), (for, ADP), (the, DET), (801...   \n",
       "4  [(The, DET), (continuing, VERB), (acceleration...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [(November 2015, DATE), (Donald Trump, PERSON)...   \n",
       "1               [(Berlusconi's, NORP), (Milan, GPE)]   \n",
       "2                [(Laila, PERSON), (Arlington, GPE)]   \n",
       "3  [(801, CARDINAL), (Democratic, NORP), (plus or...   \n",
       "4                  [(Syrian, NORP), (Obama, PERSON)]   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [(In, 3, case), (November, 6, obl), (2015, 2, ...  \n",
       "1  [(Berlusconi, 3, nmod:poss), ('s, 1, case), (a...  \n",
       "2  [(But, 8, cc), (when, 4, advmod), (Laila, 4, n...  \n",
       "3  [(Results, 9, nsubj), (for, 8, case), (the, 8,...  \n",
       "4  [(The, 3, det), (continuing, 3, amod), (accele...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa5551-aa47-4147-b9e0-2a016bc429e4",
   "metadata": {},
   "source": [
    "## Save the data in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4771fda6-8ca5-4d36-8c16-3f5ca6ec0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving df_train_shuffled to a text file with tab separators\n",
    "df_train_shuffled.to_csv('datasets_preprocessed/df_train_shuffled.txt', sep='\\t', index=False, header=True)\n",
    "\n",
    "# Saving df_valid_shuffled to a text file with tab separators\n",
    "df_valid_shuffled.to_csv('datasets_preprocessed/df_valid_shuffled.txt', sep='\\t', index=False, header=True)\n",
    "\n",
    "# Saving df_valid_shuffled to a text file with tab separators\n",
    "df_test_shuffled.to_csv('datasets_preprocessed/df_test_shuffled.txt', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce0ca3-6e5b-44c5-8cd4-1320eabd6718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

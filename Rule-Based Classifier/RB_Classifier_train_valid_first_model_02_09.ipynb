{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
   "metadata": {
    "id": "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Examples of NLTK libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c7715-37fc-438c-adcd-6e35a645887b",
   "metadata": {
    "id": "4b3c7715-37fc-438c-adcd-6e35a645887b"
   },
   "source": [
    "## Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634b2faf-13e3-44da-b6d4-46daf8b3d52a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "634b2faf-13e3-44da-b6d4-46daf8b3d52a",
    "outputId": "45b06265-a21e-4576-eed1-cee370c50fb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('incrimination.n.01'), Synset('blame.n.02'), Synset('blame.v.01'), Synset('blame.v.02'), Synset('blame.v.03'), Synset('blasted.s.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Example: Finding synonyms for a word\n",
    "synonyms = wordnet.synsets('blame')\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44616a38-2c4e-4e33-8c9c-a8367fd76af0",
   "metadata": {
    "id": "44616a38-2c4e-4e33-8c9c-a8367fd76af0"
   },
   "source": [
    "## Tokenization and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8786510-44d7-4715-94e6-f2a0c3b9078e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8786510-44d7-4715-94e6-f2a0c3b9078e",
    "outputId": "e65daaa1-391a-4cd5-e288-27d64da5498c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('responsible', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('failure', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Example: POS tagging a sentence\n",
    "sentence = \"He is responsible for the failure.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc200-36f4-4e01-9dee-abe66385ddbc",
   "metadata": {
    "id": "c8bdc200-36f4-4e01-9dee-abe66385ddbc"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679b42e0-84f6-4a57-81db-1d38d2466a6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "679b42e0-84f6-4a57-81db-1d38d2466a6a",
    "outputId": "9df10951-545a-4d8b-d408-b4ae18a08e67"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentiwordnet \u001b[38;5;28;01mas\u001b[39;00m swn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example: Getting sentiment scores for a word\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mswn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msenti_synsets\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblame\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mpos_score()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Negative: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mneg_score()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Objective: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mobj_score()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\reader\\sentiwordnet.py:53\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader.__init__\u001b[1;34m(self, root, fileids, encoding)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one file must be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_src_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\reader\\sentiwordnet.py:58\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader._parse_src_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileids[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m     57\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m((\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*#\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)), lines)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[0;32m     59\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt+\u001b[39m\u001b[38;5;124m\"\u001b[39m, line)]\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\nltk\\corpus\\reader\\sentiwordnet.py:57\u001b[0m, in \u001b[0;36mSentiWordNetCorpusReader._parse_src_file.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_src_file\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     56\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileids[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m---> 57\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m((\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), lines)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[0;32m     59\u001b[0m         fields \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt+\u001b[39m\u001b[38;5;124m\"\u001b[39m, line)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Example: Getting sentiment scores for a word\n",
    "word = list(swn.senti_synsets('blame'))[0]\n",
    "print(f\"Positive: {word.pos_score()}, Negative: {word.neg_score()}, Objective: {word.obj_score()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7790052-ef05-469d-847d-c138b8b8d913",
   "metadata": {
    "id": "f7790052-ef05-469d-847d-c138b8b8d913"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411920ea-b43c-467d-9d1a-05a30506fbb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "411920ea-b43c-467d-9d1a-05a30506fbb9",
    "outputId": "255e3df5-5c1d-4041-cd36-d86e1fe237a3"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Example: Tokenizing a paragraph into sentences and words\n",
    "paragraph = \"This is a sentence. Here is another one.\"\n",
    "sentences = sent_tokenize(paragraph)\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Words:\", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c2050-dce8-4e05-b0fa-0862bd1565f7",
   "metadata": {
    "id": "cc8c2050-dce8-4e05-b0fa-0862bd1565f7"
   },
   "source": [
    "## Open Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831d4b8-de0a-46ea-beb2-4d20b8277f14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1831d4b8-de0a-46ea-beb2-4d20b8277f14",
    "outputId": "f5de8b8e-1552-4e6d-e6b7-16b7bfdc2862"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Example: Accessing positive and negative word lists\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "print(\"Positive words example:\", positive_words[:5])\n",
    "print(\"Negative words example:\", negative_words[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XgpcdWkBTA2i",
   "metadata": {
    "id": "XgpcdWkBTA2i",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OXxtrS-FTIcH",
   "metadata": {
    "id": "OXxtrS-FTIcH"
   },
   "source": [
    "* datasets_cleaned/test_cleaned\n",
    "* datasets_cleaned/train_cleaned\n",
    "* datasets_cleaned/valid_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qx4EcsPvYerO",
   "metadata": {
    "id": "Qx4EcsPvYerO"
   },
   "source": [
    "Need to shuffle the data because the rows are sorted by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SorE_ErxS9FO",
   "metadata": {
    "id": "SorE_ErxS9FO"
   },
   "outputs": [],
   "source": [
    "column_names = [\"Sentence\", \"Ent1\", \"Ent2\", \"Label\"]\n",
    "\n",
    "df_train = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Real_News_Data_preparation/datasets_only_same_ents_clean/train.txt', sep='\\t', header=None, names=column_names)\n",
    "df_train_dropped = df_train.drop(columns=[\"Ent1\", \"Ent2\"])\n",
    "df_train_shuffled = df_train_dropped.sample(frac=1).reset_index(drop=True)\n",
    "df_valid = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Real_News_Data_preparation/datasets_only_same_ents_clean/valid.txt', sep='\\t', header=None, names=column_names)\n",
    "df_valid_dropped = df_valid.drop(columns=[\"Ent1\", \"Ent2\"])\n",
    "df_valid_shuffled = df_valid_dropped.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y3uRpEcnXpxE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Y3uRpEcnXpxE",
    "outputId": "5d22f368-be98-42e0-f1fb-4d62419ba64c"
   },
   "outputs": [],
   "source": [
    "df_train_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CWFvVIUfTDs_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CWFvVIUfTDs_",
    "outputId": "382f7e57-00a8-4b06-ccba-2992a985f4b4"
   },
   "outputs": [],
   "source": [
    "df_valid_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c1c19-2c75-49ca-953f-971d5945dd14",
   "metadata": {
    "id": "861c1c19-2c75-49ca-953f-971d5945dd14",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "How to Sequence These Steps\n",
    "The preprocessing pipeline generally follows this order:\n",
    "- Sentence Splitting and Tokenization\n",
    "- POS Tagging\n",
    "- Named Entity Recognition (NER)\n",
    "- Word Sense Disambiguation (WSD)\n",
    "- Dependency Parsing (including verb-object detection)\n",
    "- Polarity Detection (Sentiment Analysis) - SentiWordNet, AFINN and the Subjectivity Lexicon\n",
    "- Negation Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d8cb3-86c2-4118-bfd2-a9afa7f5d4a3",
   "metadata": {
    "id": "462d8cb3-86c2-4118-bfd2-a9afa7f5d4a3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: Tokenization, POS Tagging, and Named Entity Recognition (NER) using Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6347a9a-04af-4de6-8b26-0da6552cd393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 18:03:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a38f6d02b44ac1a1b405b28ad5e2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 18:03:40 INFO: Downloaded file to C:\\Users\\Anastasiia Belkina\\stanza_resources\\resources.json\n",
      "2024-08-30 18:03:41 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-08-30 18:03:41 INFO: Using device: cpu\n",
      "2024-08-30 18:03:41 INFO: Loading: tokenize\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-30 18:03:43 INFO: Loading: mwt\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\mwt\\trainer.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-30 18:03:43 INFO: Loading: pos\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-30 18:03:43 INFO: Loading: lemma\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\lemma\\trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-30 18:03:43 INFO: Loading: depparse\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-30 18:03:44 INFO: Loading: ner\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\stanza\\models\\ner\\trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-08-30 18:03:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stanza pipeline\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6c8a0-5c51-4670-8d12-cfae1237a141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729,
     "referenced_widgets": [
      "aee8df7ec2544bd89bdfbdb36a75191f",
      "9ddd39e4df4f422d894bd00d63808d90",
      "b74d16cecf6e4c81b3ffd3df6be7845b",
      "6835ae446b484d3ca602ec2f617aaff4",
      "ae273c49ff2a4099804ec2402c4e2d9a",
      "3c9aa0c7fa734470bfc3feed6592d3bc",
      "d29596beefdc42bea19fafd84f93dadb",
      "0489b43fc7e64734882843d7d1dbccce",
      "0fb17667761c4591ab2da8e3f71fa0bd",
      "a0dc4784cf42446fb83ce5e6f1162d21",
      "04d91f88d2bd41c2911b27882b1bc3c4"
     ]
    },
    "id": "12c6c8a0-5c51-4670-8d12-cfae1237a141",
    "outputId": "e1cb58d4-9040-49d7-a8cb-208bb5b1f98e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "# Tokenization, POS tagging, and NER\n",
    "def process_text(text):\n",
    "    global counter\n",
    "    counter = counter+1\n",
    "    print(\"REACHED THE FUNCTION, sentence: \", counter)\n",
    "    doc = nlp(text)\n",
    "    print(\"REACHED tokens\")\n",
    "    tokens = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]\n",
    "    print(\"REACHED entities\")\n",
    "    entities = [(ent.text, ent.type) for ent in doc.entities]\n",
    "    print(\"REACHED return\")\n",
    "    return tokens, entities\n",
    "\n",
    "print(\"START\")\n",
    "df_train_shuffled['tokens_pos'], df_train_shuffled['entities'] = zip(*df_train_shuffled['Sentence'].apply(process_text))\n",
    "print(\"START VALID\")\n",
    "counter = 0\n",
    "df_valid_shuffled['tokens_pos'], df_valid_shuffled['entities'] = zip(*df_valid_shuffled['Sentence'].apply(process_text))\n",
    "print(\"FINISH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d1fda-3b06-491a-9197-fcfed02c7432",
   "metadata": {
    "id": "193d1fda-3b06-491a-9197-fcfed02c7432"
   },
   "source": [
    "## Step 2: Word Sense Disambiguation (WSD) using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5520b3-e184-4f7e-a5c9-11e550da51f4",
   "metadata": {
    "id": "4b5520b3-e184-4f7e-a5c9-11e550da51f4"
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_sense(tokens):\n",
    "    senses = []\n",
    "    for token, pos in tokens:\n",
    "        sense = lesk([token], token)\n",
    "        senses.append((token, sense))\n",
    "    return senses\n",
    "\n",
    "df_train_shuffled['senses'] = df_train_shuffled['tokens_pos'].apply(get_sense)\n",
    "df_valid_shuffled['senses'] = df_valid_shuffled['tokens_pos'].apply(get_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c1923-27c4-4044-9266-96712fa73b06",
   "metadata": {
    "id": "db0c1923-27c4-4044-9266-96712fa73b06"
   },
   "source": [
    "## Step 3: Dependency Parsing using Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc62c9-0ac2-4b36-b4ad-823d30eff69a",
   "metadata": {
    "id": "0acc62c9-0ac2-4b36-b4ad-823d30eff69a"
   },
   "outputs": [],
   "source": [
    "def dependency_parse(text):\n",
    "    doc = nlp(text)\n",
    "    dependencies = [(word.text, word.head, word.deprel) for sent in doc.sentences for word in sent.words]\n",
    "    return dependencies\n",
    "\n",
    "df_train_shuffled['dependencies'] = df_train_shuffled['Sentence'].apply(dependency_parse)\n",
    "df_valid_shuffled['dependencies'] = df_valid_shuffled['Sentence'].apply(dependency_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb37dc3-df8e-4d37-8046-205ad16d0196",
   "metadata": {
    "id": "4bb37dc3-df8e-4d37-8046-205ad16d0196"
   },
   "source": [
    "## Step 4: Polarity Detection using Multiple Sentiment Lexicons - SentiWordNet, AFINN and the Subjectivity Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uG4vc4Gpq8ik",
   "metadata": {
    "id": "uG4vc4Gpq8ik"
   },
   "source": [
    "### 4.1. Using SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1cc7f-88c5-4b1f-aa5b-3df3237be9fc",
   "metadata": {
    "id": "0bf1cc7f-88c5-4b1f-aa5b-3df3237be9fc"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def get_sentiwordnet_score(senses):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for word, sense in senses:\n",
    "        if sense:\n",
    "            swn_synset = swn.senti_synset(sense.name())\n",
    "            pos_score += swn_synset.pos_score()\n",
    "            neg_score += swn_synset.neg_score()\n",
    "    return pos_score, neg_score\n",
    "\n",
    "df_train_shuffled['swn_scores'] = df_train_shuffled['senses'].apply(get_sentiwordnet_score)\n",
    "df_valid_shuffled['swn_scores'] = df_valid_shuffled['senses'].apply(get_sentiwordnet_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pJhXlxbMq_2h",
   "metadata": {
    "id": "pJhXlxbMq_2h"
   },
   "source": [
    "### 4.2. Using AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XF8HlXkgqzmj",
   "metadata": {
    "id": "XF8HlXkgqzmj"
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "def get_afinn_score(text):\n",
    "    return afinn.score(text)\n",
    "\n",
    "df_train_shuffled['afinn_score'] = df_train_shuffled['Sentence'].apply(get_afinn_score)\n",
    "df_valid_shuffled['afinn_score'] = df_valid_shuffled['Sentence'].apply(get_afinn_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6B_angnbrC_X",
   "metadata": {
    "id": "6B_angnbrC_X"
   },
   "source": [
    "### 4.3. Using Subjectivity Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E1hZhEuqrDQl",
   "metadata": {
    "id": "E1hZhEuqrDQl"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "def get_subjectivity_score(tokens):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for token, pos in tokens:\n",
    "        if token.lower() in opinion_lexicon.positive():\n",
    "            pos_score += 1\n",
    "        elif token.lower() in opinion_lexicon.negative():\n",
    "            neg_score += 1\n",
    "    return pos_score, neg_score\n",
    "\n",
    "df_train_shuffled['subj_scores'] = df_train_shuffled['tokens_pos'].apply(get_subjectivity_score)\n",
    "df_valid_shuffled['subj_scores'] = df_valid_shuffled['tokens_pos'].apply(get_subjectivity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l15A8TKhrRQb",
   "metadata": {
    "id": "l15A8TKhrRQb"
   },
   "source": [
    "## Step 5: Majority Voting for Final Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LenrjnMarQoX",
   "metadata": {
    "id": "LenrjnMarQoX"
   },
   "outputs": [],
   "source": [
    "def majority_vote(row):\n",
    "    pos_score = row['swn_scores'][0] + row['afinn_score'] + row['subj_scores'][0]\n",
    "    neg_score = row['swn_scores'][1] + abs(row['afinn_score']) if row['afinn_score'] < 0 else 0 + row['subj_scores'][1]\n",
    "\n",
    "    if pos_score > neg_score:\n",
    "        return \"positive\"\n",
    "    elif neg_score > pos_score:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df_train_shuffled['final_sentiment'] = df_train_shuffled.apply(majority_vote, axis=1)\n",
    "df_valid_shuffled['final_sentiment'] = df_valid_shuffled.apply(majority_vote, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63116db7-e277-4418-8f93-ec9c8dca637b",
   "metadata": {
    "id": "63116db7-e277-4418-8f93-ec9c8dca637b"
   },
   "source": [
    "## ? Step 6: Negation Handling ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac17a9-ab15-40f5-8aa7-18839e8a9592",
   "metadata": {
    "id": "26ac17a9-ab15-40f5-8aa7-18839e8a9592"
   },
   "outputs": [],
   "source": [
    "def handle_negation(dependencies):\n",
    "    negations = [word for word, head, deprel in dependencies if deprel == 'neg']\n",
    "    # Вообще не так. Если not отдельно, то ('not', ?, 'advmod'). А если не not?? есть еще n’t ('n’t', ?, 'advmod')\n",
    "    return negations\n",
    "\n",
    "df_train_shuffled['negations'] = df_train_shuffled['dependencies'].apply(handle_negation)\n",
    "df_valid_shuffled['negations'] = df_valid_shuffled['dependencies'].apply(handle_negation)\n",
    "\n",
    "def adjust_sentiment_for_negation(row):\n",
    "    if row['negations']:\n",
    "        if row['final_sentiment'] == \"positive\":\n",
    "            return \"negative\"\n",
    "        elif row['final_sentiment'] == \"negative\":\n",
    "            return \"positive\"\n",
    "    return row['final_sentiment']\n",
    "\n",
    "df_train_shuffled['final_sentiment_adj'] = df_train_shuffled.apply(adjust_sentiment_for_negation, axis=1)\n",
    "df_valid_shuffled['final_sentiment_adj'] = df_valid_shuffled.apply(adjust_sentiment_for_negation, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2513d1-be44-4aa0-84c0-47248ab4bba5",
   "metadata": {},
   "source": [
    "## Checking the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c41a1a-c92a-4413-a397-db790b5e880a",
   "metadata": {
    "id": "b8c41a1a-c92a-4413-a397-db790b5e880a"
   },
   "outputs": [],
   "source": [
    "df_train_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38b83c-f07c-4973-ab0e-dd933c3a0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa5551-aa47-4147-b9e0-2a016bc429e4",
   "metadata": {},
   "source": [
    "## Save the data in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771fda6-8ca5-4d36-8c16-3f5ca6ec0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving df_train_shuffled to a text file with tab separators\n",
    "df_train_shuffled.to_csv('df_train_shuffled.txt', sep='\\t', index=False, header=True)\n",
    "\n",
    "# Saving df_valid_shuffled to a text file with tab separators\n",
    "df_valid_shuffled.to_csv('df_valid_shuffled.txt', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee484548-0f4d-4182-a70f-aef0f0cd5b96",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d02f264-f222-4ffb-be3b-f0097f988b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e45fb0-f9f4-4fc1-b88c-998056fe6acb",
   "metadata": {},
   "source": [
    "# Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1049d0-bd96-4662-988d-0db76fe1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"senses\", \"dependencies\", \"swn_scores\", \"afinn_score\", \"subj_scores\", \"final_sentiment\", \"negations\", \"final_sentiment_adj\"]\n",
    "\n",
    "df_train_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_valid_preprocessed = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "# Remove leading and trailing spaces in the \"Sentence\" column\n",
    "df_train_preprocessed['Sentence'] = df_train_preprocessed['Sentence'].str.strip()\n",
    "df_valid_preprocessed['Sentence'] = df_valid_preprocessed['Sentence'].str.strip()\n",
    "\n",
    "df_train_ready = df_train_preprocessed\n",
    "df_valid_ready = df_valid_preprocessed\n",
    "\n",
    "# Shuffle the data\n",
    "#df_train_ready = df_train_preprocessed.sample(frac=1).reset_index(drop=True)\n",
    "#df_valid_ready = df_valid_preprocessed.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade7978-08a0-4c84-be4c-be6ed20f1d04",
   "metadata": {},
   "source": [
    "# Merging Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaba528-c606-4797-aeee-52437715a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {2: 1, 3: 2, 4: 2}\n",
    "\n",
    "# 0 - neutral, 1 - positive, 2 - negative\n",
    "\n",
    "df_train_ready_merged = df_train_ready\n",
    "df_valid_ready_merged = df_valid_ready\n",
    "\n",
    "# Apply the mapping to the 'Label' column\n",
    "df_train_ready_merged['Label'] = df_train_ready_merged['Label'].replace(label_mapping)\n",
    "df_valid_ready_merged['Label'] = df_valid_ready_merged['Label'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a1cf9-5a57-493a-8c3a-7802f92ffba6",
   "metadata": {},
   "source": [
    "# Turning strings back to lists and tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e79989-3230-4c6b-ba9c-17d47543c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_to_list(dependencies_str):\n",
    "    # Check if it's a string and if it appears to be in the list of tuples format\n",
    "    if isinstance(dependencies_str, str) and dependencies_str.startswith(\"[\") and dependencies_str.endswith(\"]\"):\n",
    "        try:\n",
    "            # Convert string representation of list back to actual list of tuples\n",
    "            return ast.literal_eval(dependencies_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error parsing: {dependencies_str}\")\n",
    "            raise e\n",
    "    elif isinstance(dependencies_str, list):\n",
    "        # If it's already a list, return as is\n",
    "        return dependencies_str\n",
    "    else:\n",
    "        # If it's another unexpected type, return as is or handle appropriately\n",
    "        return dependencies_str\n",
    "\n",
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['dependencies'] = df_train_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_valid_ready_merged['dependencies'] = df_valid_ready_merged['dependencies'].apply(convert_to_list)\n",
    "df_train_ready_merged['tokens_pos'] = df_train_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_valid_ready_merged['tokens_pos'] = df_valid_ready_merged['tokens_pos'].apply(convert_to_list)\n",
    "df_train_ready_merged['entities'] = df_train_ready_merged['entities'].apply(convert_to_list)\n",
    "df_valid_ready_merged['entities'] = df_valid_ready_merged['entities'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35bc274-242f-4433-a829-bc8e62f66d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense(tokens):\n",
    "    #print(tokens)\n",
    "    senses = []\n",
    "    for item in tokens:\n",
    "        #print(item)\n",
    "        if isinstance(item, tuple) and len(item) == 2:\n",
    "            token, pos = item\n",
    "            sense = lesk([token], token)\n",
    "            senses.append((token, sense))\n",
    "        else:\n",
    "            # Handle cases where the token doesn't meet the expected structure\n",
    "            print(f\"Unexpected format: {item}\")\n",
    "            senses.append((item, None))\n",
    "    return senses\n",
    "\n",
    "df_train_ready_merged['senses'] = df_train_ready_merged['tokens_pos'].apply(get_sense)\n",
    "df_valid_ready_merged['senses'] = df_valid_ready_merged['tokens_pos'].apply(get_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5898c-ac22-4229-9a87-3a89b6c3912e",
   "metadata": {},
   "source": [
    "# Following the modified Path Model of Blame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cbbcd-8266-428e-93b7-4efbf1b99019",
   "metadata": {},
   "source": [
    "## Step 1. Event Detection\n",
    "\n",
    "\"We look at the “verb+object” combination as identified using the Stanford dependency parser and take note of the agent of the verb. We use the majority voting mechanism mentioned above for polarity detection. Negatively or positively valenced events are extracted from sentences expressing negative or positive polarity respectively.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9c75dc-5ae1-4504-a84b-30bb8aa66078",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "def get_event_polarity(verb, obj):\n",
    "    # Create a simple context for WSD\n",
    "    context = f\"{verb} {obj}\"\n",
    "    \n",
    "    # Word Sense Disambiguation for the verb and object\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    # Calculate polarity using SentiWordNet\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    # AFINN score\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    # Subjectivity Lexicon score\n",
    "    tokens = context.split()\n",
    "    subj_pos = sum([1 for token in tokens if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in tokens if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    # Determine final polarity\n",
    "    if pos_score > neg_score:\n",
    "        return '1'  # Positive/Praise\n",
    "    elif neg_score > pos_score:\n",
    "        return '2'  # Negative/Blame\n",
    "    else:\n",
    "        return '0'  # Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093c7296-2fa6-497d-9294-7f68d16ae0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events_and_agents(dependencies):\n",
    "    \n",
    "    events = []\n",
    "    verbs = []\n",
    "    \n",
    "    # Step 1: Identify all verbs that are 'root'\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                verbs.append((word, i + 1))  # Save the verb and its index (i+1)\n",
    "    \n",
    "    # Step 2: For each identified verb, find associated subjects and objects\n",
    "    for verb, verb_index in verbs:\n",
    "        subject = None\n",
    "        obj = None\n",
    "\n",
    "        for word, head, deprel in dependencies:\n",
    "            if head == verb_index:  # Compare with the index of the verb, not its head\n",
    "                if deprel in ['nsubj', 'nsubjpass', 'subj']:  # Subject of the verb\n",
    "                    subject = word\n",
    "                if deprel in ['obj', 'dobj']:  # Object of the verb\n",
    "                    obj = word\n",
    "        \n",
    "        if subject and obj:\n",
    "            # Calculate polarity specifically for this verb-object pair\n",
    "            polarity = get_event_polarity(verb, obj)\n",
    "            events.append({\n",
    "                'verb': verb,\n",
    "                'object': obj,\n",
    "                'agent': subject,\n",
    "                'polarity': polarity\n",
    "            })\n",
    "    \n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b090888-6087-4f22-becb-a97fb9e9fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "df_train_ready_merged['events'] = df_train_ready_merged['dependencies'].apply(extract_events_and_agents)\n",
    "df_valid_ready_merged['events'] = df_valid_ready_merged['dependencies'].apply(extract_events_and_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703541e-99e0-422e-923d-7ad187c8ba13",
   "metadata": {},
   "source": [
    "## Label rows without events \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a515fb98-4c09-4fd6-a01c-40bfb30316bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_final_result_column(df):\n",
    "    # Insert 'Final_Result' column after 'Label' with default value None\n",
    "    label_index = df.columns.get_loc('Label')\n",
    "    df.insert(label_index + 1, 'Final_Result', None)\n",
    "    \n",
    "    # Update 'Final_Result' to 0 where 'events' column is empty\n",
    "    df.loc[df['events'].apply(lambda x: not x), 'Final_Result'] = \"others\"\n",
    "\n",
    "# Apply the function to both dataframes\n",
    "add_final_result_column(df_train_ready_merged)\n",
    "add_final_result_column(df_valid_ready_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf515f-3d9d-40f2-b439-701cc625c035",
   "metadata": {},
   "source": [
    "## Step 2. Agent Causality\n",
    "\n",
    "\"Here, one must establish that a moral agent caused an event. We first make use of a popular explicit intra-sentential pattern for causation expression which is “NP verb NP” where NP is a noun phrase (Girju, 2003) and then we identify the agent within the noun phrase. If the intrasentential pattern is not found we consider verbs in the text that belong to the CAUSE class and the CAUSETO semantic relation which are defined in the WordNet. In order for “Agent Causality” taking the value “True”, the agent must be a person entity (including pronouns).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb437b77-e51e-479e-b426-9f423421414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_causative_verb(verb):\n",
    "    # Check if the verb is in the CAUSE class or has CAUSETO relation in WordNet\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a79467e-69f6-435f-b344-e5e8a8154de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_agent_causality(row):\n",
    "    dependencies = row['dependencies']\n",
    "    entities = row['entities']\n",
    "    agent_causality = False\n",
    "    verbs = []\n",
    "\n",
    "    # Step 1: Identify all verbs that are 'root'\n",
    "    for i, dep in enumerate(dependencies):\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep\n",
    "            if deprel == 'root':\n",
    "                verbs.append((word, i + 1))  # Save the verb and its index (i+1)\n",
    "    \n",
    "    # Step 2: For each identified verb, find associated subjects and objects\n",
    "    for verb, verb_index in verbs:\n",
    "        subject = None\n",
    "        obj = None\n",
    "\n",
    "        for word, head, deprel in dependencies:\n",
    "            if head == verb_index:  # Compare with the index of the verb, not its head\n",
    "                if deprel in ['nsubj', 'nsubjpass']:  # Subject of the verb\n",
    "                    subject = word\n",
    "                if deprel in ['obj', 'dobj']:  # Object of the verb\n",
    "                    obj = word\n",
    "        \n",
    "        # Step 3: Validate the agent (subject)\n",
    "        agent_is_valid = False\n",
    "        if subject:\n",
    "            for entity, label in entities:\n",
    "                if entity == subject and label in ['PERSON', 'ORG', 'GPE']:\n",
    "                    agent_is_valid = True\n",
    "                    break\n",
    "            \n",
    "            if not agent_is_valid and 'PRP' in [pos for token, pos in row['tokens_pos'] if token == subject]:\n",
    "                agent_is_valid = True  # It's a pronoun\n",
    "\n",
    "        # Step 4: If both subject and object are found and agent is valid, we have causality\n",
    "        if subject and obj and agent_is_valid:\n",
    "            agent_causality = True\n",
    "            break\n",
    "    \n",
    "    # Step 5: If no \"NP verb NP\" pattern is found, check for causative verbs\n",
    "    if not agent_causality:\n",
    "        for verb, verb_index in verbs:\n",
    "            if check_causative_verb(verb):\n",
    "                agent_causality = True\n",
    "                break\n",
    "    \n",
    "    return agent_causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f0d350-18d2-4bb4-87b7-b9c8b3d732ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_ready_merged['agent_causality'] = df_train_ready_merged.apply(identify_agent_causality, axis=1)\n",
    "df_valid_ready_merged['agent_causality'] = df_valid_ready_merged.apply(identify_agent_causality, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e7abf-087a-4181-a48f-4ec4cdcf2f22",
   "metadata": {},
   "source": [
    "## Label rows without agent causality \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4ae980d-33f4-4f60-9f8e-85ee955684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where agent_causality is False in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['agent_causality'] == False, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where agent_causality is False in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['agent_causality'] == False, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6d564-e230-4694-8844-cf779df7f054",
   "metadata": {},
   "source": [
    "## Step 3. Foreseeability\n",
    "\n",
    "\"Foreseeability. We rely on a set of verbs which indicate foreseeability. These include verbs of communication as suggested in (Mao et al, 2011) and other verb classes which include verbs of creation, verbs of consumption, verbs of competition, verbs of possession and verbs of motion. These classes of verbs are defined in the WordNet7 and can be identified by looking at the WordNet sensekey of the verbs. Example: When I did not speak the truth. In the example above, the communication verb “speak” indicates that the subject “I” had foreknowledge of the event of “speaking the truth”.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defae370-1030-4e1d-82aa-38d92af27075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_foreseeability(row):\n",
    "    dependencies = row['dependencies']\n",
    "    foreseeable = False\n",
    "    \n",
    "    # Identify all verbs in the sentence\n",
    "    verbs = [word for word, head, deprel in dependencies if deprel == 'root']\n",
    "\n",
    "    # Check if any verb indicates foreseeability\n",
    "    for verb in verbs:\n",
    "        if is_foreseeable_verb(verb):\n",
    "            foreseeable = True\n",
    "            break\n",
    "    \n",
    "    return foreseeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fad26a0-54ff-4b1d-96d0-797a14714e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a verb belongs to any foreseeability-related classes\n",
    "def is_foreseeable_verb(verb):\n",
    "    #print(\"NEW\")\n",
    "    #print(verb)\n",
    "    \n",
    "    # Synset categories indicating foreseeability\n",
    "    foreseeability_classes = {\n",
    "        'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'\n",
    "    }\n",
    "\n",
    "    # Get the synsets for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        # Check if the verb belongs to any of the foreseeability classes\n",
    "        lexname = synset.lexname().split('.')[1]\n",
    "        if lexname in foreseeability_classes:\n",
    "            #print(synset.lexname())\n",
    "            #print(synset)\n",
    "            #print(lexname)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5487971-b003-4849-9b54-99a4cfe821fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_ready_merged['foreseeability'] = df_train_ready_merged.apply(determine_foreseeability, axis=1)\n",
    "df_valid_ready_merged['foreseeability'] = df_valid_ready_merged.apply(determine_foreseeability, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb67f423-8435-4508-836b-6e9d6ad80ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foreseeability\n",
       "True     4248\n",
       "False     784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"foreseeability\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f586af-563c-477b-81bc-0afd08b187e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foreseeability\n",
       "True     456\n",
       "False     94\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"foreseeability\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46be908-1ea0-4d86-bf0c-19a3ce5628aa",
   "metadata": {},
   "source": [
    "## Label rows without foreseeability \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b43bbd8-4739-4d9a-9da3-db92cd5cf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where foreseeability is False in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['foreseeability'] == False, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where foreseeability is False in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['foreseeability'] == False, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acc753-293f-40f7-bcd5-33706d86f8e6",
   "metadata": {},
   "source": [
    "## Step 4. Coercion\n",
    "\n",
    "\"To identify coercion, we look at the extension verb classes presented in (Kipper et al, 2006) focusing on verbs in the URGE (13 members), FORCE (46 members) and FORBID (17 members) classes. Example: I was forced to quite the job in the city. In the example above , using word sense disambiguation, the verb “forced” is of sense “to cause to do through pressure or necessity, by physical, moral or intellectual means”. The agent “I” in this case did not willingly quite the job and the sentence does not mention who forced the agent. Thus, the sentence is classified as “Others” (i.e., no blame or praise).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e957c6c-8bc1-4eec-8328-681688346142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_coercion(row):\n",
    "    dependencies = row['dependencies']\n",
    "    coercion = False\n",
    "    \n",
    "    # Identify all verbs in the sentence\n",
    "    verbs = [word for word, head, deprel in dependencies if deprel == 'root']\n",
    "    \n",
    "    # Check if any verb indicates coercion\n",
    "    for verb in verbs:\n",
    "        if is_coercion_verb(verb):\n",
    "            coercion = True\n",
    "            break\n",
    "    \n",
    "    return coercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a66db4-4305-4530-bf76-da83e68682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a verb belongs to any coercion-related VerbNet classes\n",
    "def is_coercion_verb(verb):\n",
    "    coercion_classes = {\n",
    "        'urge-58.1', 'force-59', 'forbid-67'\n",
    "    }\n",
    "    \n",
    "    # Get the VerbNet classes for the verb\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        #print(synset)\n",
    "        lemma = synset.lemmas()[0]\n",
    "        #print(lemma)\n",
    "        vn_classes = lemma.key().split('%')[0]\n",
    "        vn_class_ids = vn.classids(vn_classes)\n",
    "        #print(vn_classes)\n",
    "        #print(vn_class_ids)\n",
    "        \n",
    "        # Check if any VerbNet class matches the coercion classes\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636e8008-81ea-4398-8179-f62dc0c6ee91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to both datasets\n",
    "df_train_ready_merged['coercion'] = df_train_ready_merged.apply(determine_coercion, axis=1)\n",
    "df_valid_ready_merged['coercion'] = df_valid_ready_merged.apply(determine_coercion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce37fd5-c0c7-4803-b730-ba2283e7e6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coercion\n",
       "False    4679\n",
       "True      353\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"coercion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e151c5a-eda9-4e12-8539-ca0d089dec54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coercion\n",
       "False    509\n",
       "True      41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"coercion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271831c0-82c8-45e8-be5d-be8f88d3276d",
   "metadata": {},
   "source": [
    "## Label rows with coercion \"others\" in the column Final_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de490b1b-3186-4ec5-b589-2144cbd5a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Final_Result to 0 where coercion is True in df_train_ready_merged\n",
    "df_train_ready_merged.loc[df_train_ready_merged['coercion'] == True, 'Final_Result'] = \"others\"\n",
    "\n",
    "# Update Final_Result to 0 where coercion is True in df_valid_ready_merged\n",
    "df_valid_ready_merged.loc[df_valid_ready_merged['coercion'] == True, 'Final_Result'] = \"others\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59c24d-5c1c-4e34-bf9e-26df02f7444e",
   "metadata": {},
   "source": [
    "## Negation sucks - redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2dea03-e3b0-4e0d-8281-390515e2fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(dependencies):\n",
    "    negations = [word for word, head, deprel in dependencies if deprel == 'advmod' and (word == 'not' or word == 'n’t')]\n",
    "    return negations\n",
    "\n",
    "df_train_ready_merged['negations'] = df_train_ready_merged['dependencies'].apply(handle_negation)\n",
    "df_valid_ready_merged['negations'] = df_valid_ready_merged['dependencies'].apply(handle_negation)\n",
    "\n",
    "def adjust_sentiment_for_negation(row):\n",
    "    if row['negations']:\n",
    "        if row['final_sentiment'] == \"positive\":\n",
    "            return \"negative\"\n",
    "        elif row['final_sentiment'] == \"negative\":\n",
    "            return \"positive\"\n",
    "    return row['final_sentiment']\n",
    "\n",
    "df_train_ready_merged['final_sentiment_adj'] = df_train_ready_merged.apply(adjust_sentiment_for_negation, axis=1)\n",
    "df_valid_ready_merged['final_sentiment_adj'] = df_valid_ready_merged.apply(adjust_sentiment_for_negation, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307e4da-b2ee-43dc-a32c-9481927a0f13",
   "metadata": {},
   "source": [
    "## Step 5. Final Classification (Blame/Praise/Others)\n",
    "\n",
    "0 - neutral, 1 - positive, 2 - negative - Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "495ef68c-5e2a-42b8-935d-fc387aa052aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>swn_scores</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>subj_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>negations</th>\n",
       "      <th>final_sentiment_adj</th>\n",
       "      <th>events</th>\n",
       "      <th>agent_causality</th>\n",
       "      <th>foreseeability</th>\n",
       "      <th>coercion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "      <td>(0.0, 0.75)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[{'verb': 'supporting', 'object': 'Clinton', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence  Label Final_Result  \\\n",
       "0  George is not supporting Clinton.      2         None   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "\n",
       "                                entities  \\\n",
       "0  [(George, PERSON), (Clinton, PERSON)]   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "\n",
       "                                        dependencies   swn_scores  \\\n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  (0.0, 0.75)   \n",
       "\n",
       "   afinn_score subj_scores final_sentiment negations final_sentiment_adj  \\\n",
       "0          1.0      (1, 0)        positive     [not]            negative   \n",
       "\n",
       "                                              events  agent_causality  \\\n",
       "0  [{'verb': 'supporting', 'object': 'Clinton', '...             True   \n",
       "\n",
       "   foreseeability  coercion  \n",
       "0            True     False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2b082ab-f8d5-4ca3-8423-206847754851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(row):\n",
    "    event_present = row['events'] is not None and len(row['events']) > 0\n",
    "    agent_causality = row['agent_causality']\n",
    "    foreseeability = row['foreseeability']\n",
    "    coercion = row['coercion']\n",
    "    final_sentiment = row['final_sentiment_adj']  # Assuming this is precomputed as positive, negative, or neutral\n",
    "    final_result = row['Final_Result']\n",
    "    \n",
    "    if final_result == None:\n",
    "        if agent_causality and foreseeability and not coercion:\n",
    "            #print(row['events'][0].get('agent'))\n",
    "            if final_sentiment == 'negative':\n",
    "                if row['events'][0].get('agent') == \"I\":\n",
    "                    #print(row['events'][0].get('agent'))\n",
    "                    return \"self-blame\"\n",
    "                else:\n",
    "                    return \"blame-others\"\n",
    "            elif final_sentiment == 'positive':\n",
    "                if row['events'][0].get('agent') == \"I\":\n",
    "                    #print(row['events'][0].get('agent'))\n",
    "                    return \"self-praise\"\n",
    "                else:\n",
    "                    return \"praise-others\"\n",
    "            if final_sentiment == 'neutral':\n",
    "                return \"others\"\n",
    "        else: return \"others\"\n",
    "    else:\n",
    "        return \"others\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2acde65-7380-4e75-96fb-7ce02bc39383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the classification to each row in the DataFrame\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged.apply(classify_sentence, axis=1)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged.apply(classify_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4672b2c0-d9bd-4486-9f1b-47409dba4359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>senses</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>swn_scores</th>\n",
       "      <th>afinn_score</th>\n",
       "      <th>subj_scores</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>negations</th>\n",
       "      <th>final_sentiment_adj</th>\n",
       "      <th>events</th>\n",
       "      <th>agent_causality</th>\n",
       "      <th>foreseeability</th>\n",
       "      <th>coercion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>blame-others</td>\n",
       "      <td>[(George, PROPN), (is, AUX), (not, PART), (sup...</td>\n",
       "      <td>[(George, PERSON), (Clinton, PERSON)]</td>\n",
       "      <td>[(George, Synset('george.n.05')), (is, Synset(...</td>\n",
       "      <td>[(George, 4, nsubj), (is, 4, aux), (not, 4, ad...</td>\n",
       "      <td>(0.0, 0.75)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[{'verb': 'supporting', 'object': 'Clinton', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sentence  Label  Final_Result  \\\n",
       "0  George is not supporting Clinton.      2  blame-others   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [(George, PROPN), (is, AUX), (not, PART), (sup...   \n",
       "\n",
       "                                entities  \\\n",
       "0  [(George, PERSON), (Clinton, PERSON)]   \n",
       "\n",
       "                                              senses  \\\n",
       "0  [(George, Synset('george.n.05')), (is, Synset(...   \n",
       "\n",
       "                                        dependencies   swn_scores  \\\n",
       "0  [(George, 4, nsubj), (is, 4, aux), (not, 4, ad...  (0.0, 0.75)   \n",
       "\n",
       "   afinn_score subj_scores final_sentiment negations final_sentiment_adj  \\\n",
       "0          1.0      (1, 0)        positive     [not]            negative   \n",
       "\n",
       "                                              events  agent_causality  \\\n",
       "0  [{'verb': 'supporting', 'object': 'Clinton', '...             True   \n",
       "\n",
       "   foreseeability  coercion  \n",
       "0            True     False  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a2f83aa-cf27-47a4-bc38-62decfcc66a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George is not supporting Clinton.</td>\n",
       "      <td>2</td>\n",
       "      <td>blame-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan has endorsed Trump and told reporters thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>praise-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John McGraw, 78, was charged with assault and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Filipino fighter unleashed a dazzling comb...</td>\n",
       "      <td>1</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But the Marlins have failed to make the postse...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label   Final_Result\n",
       "0                  George is not supporting Clinton.      2   blame-others\n",
       "1  Ryan has endorsed Trump and told reporters thi...      1  praise-others\n",
       "2  John McGraw, 78, was charged with assault and ...      2         others\n",
       "3  The Filipino fighter unleashed a dazzling comb...      1         others\n",
       "4  But the Marlins have failed to make the postse...      0         others"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[['Sentence', 'Label', 'Final_Result']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "609bc38e-41d9-4aa0-a33f-08753e1c3805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "others           4512\n",
       "blame-others      282\n",
       "praise-others     238\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ready_merged[\"Final_Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07696d4b-4fbc-4936-a593-9262710860fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Final_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actress Patty Duke died on Tuesday at age 69, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who showrunner, Moffat, will give a masterclas...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Patriots keeping Brady on the bench after ...</td>\n",
       "      <td>0</td>\n",
       "      <td>praise-others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia accounted for about 9 percent of Totals...</td>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An Post survey suggests a majority of voters t...</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label   Final_Result\n",
       "0  Actress Patty Duke died on Tuesday at age 69, ...      0         others\n",
       "1  Who showrunner, Moffat, will give a masterclas...      0         others\n",
       "2  The Patriots keeping Brady on the bench after ...      0  praise-others\n",
       "3  Russia accounted for about 9 percent of Totals...      2         others\n",
       "4  An Post survey suggests a majority of voters t...      0         others"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c7f8f0-19fb-4a79-8155-e21eb92ee82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final_Result\n",
       "others           480\n",
       "blame-others      45\n",
       "praise-others     25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_ready_merged[\"Final_Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80232b5-0c37-466e-9c42-8022250cea6e",
   "metadata": {},
   "source": [
    "## Step 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427d92c-2bc0-43d1-9e4a-e7ac7590985c",
   "metadata": {},
   "source": [
    "### Map values in Final_Result column to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d74c33e-c339-4f34-9040-a776c93b3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_16104\\2176863739.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
      "C:\\Users\\Anastasiia Belkina\\AppData\\Local\\Temp\\ipykernel_16104\\2176863739.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "label_mapping = {\"others\": 0, \"blame-others\": 2, \"praise-others\": 1}\n",
    "\n",
    "# 0 - neutral, 1 - praise, 2 - blame\n",
    "\n",
    "# Apply the mapping to the 'Final_Result' column\n",
    "df_train_ready_merged['Final_Result'] = df_train_ready_merged['Final_Result'].replace(label_mapping)\n",
    "df_valid_ready_merged['Final_Result'] = df_valid_ready_merged['Final_Result'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ee14185-795a-4684-92e8-06358fb39bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_eval = df_train_ready_merged[['Sentence', 'Label', 'Final_Result']]\n",
    "df_valid_eval = df_valid_ready_merged[['Sentence', 'Label', 'Final_Result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4565ebbc-4c32-484d-b540-e77af387f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 20 random rows from each DataFrame\n",
    "df_train_sample = df_train_ready_merged.sample(n=20, random_state=1)\n",
    "df_valid_sample = df_valid_ready_merged.sample(n=20, random_state=1)\n",
    "\n",
    "# Save them to an Excel file with different sheets\n",
    "with pd.ExcelWriter('sampled_data.xlsx') as writer:\n",
    "    df_train_sample.to_excel(writer, sheet_name='Train_Sample', index=False)\n",
    "    df_valid_sample.to_excel(writer, sheet_name='Valid_Sample', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e40a5-4319-4f03-921d-cd7c3953eb84",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81a296a3-fa30-49fa-a7f1-701772bc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_train = df_train_eval['Label']\n",
    "y_pred_train = df_train_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29b674ec-f8c7-4c94-a771-73660a1670f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.565382       0.363339          0.475443\n",
      "1  Precision       0.565382       0.537041               NaN\n",
      "2     Recall       0.565382       0.391648               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.94      0.71      2733\n",
      "           1       0.36      0.11      0.17       798\n",
      "           2       0.68      0.13      0.22      1501\n",
      "\n",
      "    accuracy                           0.57      5032\n",
      "   macro avg       0.54      0.39      0.36      5032\n",
      "weighted avg       0.57      0.57      0.48      5032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "f1_macro = f1_score(y_true_train, y_pred_train, average='macro')\n",
    "f1_weighted = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_train, y_pred_train, average='micro')\n",
    "precision_macro = precision_score(y_true_train, y_pred_train, average='macro')\n",
    "recall_micro = recall_score(y_true_train, y_pred_train, average='micro')\n",
    "recall_macro = recall_score(y_true_train, y_pred_train, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46642122-b89b-49fc-9463-56e7b5c447c8",
   "metadata": {},
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8fa5b22-60a0-4dcb-b334-223b84179568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true_valid = df_valid_eval['Label']\n",
    "y_pred_valid = df_valid_eval['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dfaea35-0810-4955-870e-75195f16a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric  Micro-average  Macro-average  Weighted-average\n",
      "0   F1 Score       0.589091       0.400254          0.514723\n",
      "1  Precision       0.589091       0.552778               NaN\n",
      "2     Recall       0.589091       0.412998               NaN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.93      0.72       305\n",
      "           1       0.40      0.13      0.19        78\n",
      "           2       0.67      0.18      0.28       167\n",
      "\n",
      "    accuracy                           0.59       550\n",
      "   macro avg       0.55      0.41      0.40       550\n",
      "weighted avg       0.59      0.59      0.51       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true_valid, y_pred_valid, average='micro')\n",
    "f1_macro = f1_score(y_true_valid, y_pred_valid, average='macro')\n",
    "f1_weighted = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true_valid, y_pred_valid, average='micro')\n",
    "precision_macro = precision_score(y_true_valid, y_pred_valid, average='macro')\n",
    "recall_micro = recall_score(y_true_valid, y_pred_valid, average='micro')\n",
    "recall_macro = recall_score(y_true_valid, y_pred_valid, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2f4fd-558a-477f-8e2f-821ce4185d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ccda0-0a42-49b7-a418-ed2725456ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6e69f9-ebee-4e47-b287-159f060a1bdd",
   "metadata": {},
   "source": [
    "# NEED TO CHANGE LABELS OF TEST DATA FILE AND PREPROCESS IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4a4f8-3eca-4225-bf11-1a320bfd5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

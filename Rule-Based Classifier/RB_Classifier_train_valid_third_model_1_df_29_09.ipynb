{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee484548-0f4d-4182-a70f-aef0f0cd5b96",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d02f264-f222-4ffb-be3b-f0097f988b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import ast\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b106f5-091e-4100-af31-f7677c66f1de",
   "metadata": {},
   "source": [
    "# Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1049d0-bd96-4662-988d-0db76fe1a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "column_names = [\"Sentence\", \"Label\", \"tokens_pos\", \"entities\", \"dependencies\"]\n",
    "df_train_ready = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/datasets_preprocessed/df_train_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_valid_ready = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/datasets_preprocessed/df_valid_shuffled.txt', sep='\\t', names=column_names)\n",
    "df_test_ready = pd.read_csv('C:/Users/Anastasiia Belkina/MANNHEIM/MASTER_THESIS_CODE/Rule-Based Classifier/datasets_preprocessed/df_test_shuffled.txt', sep='\\t', names=column_names)\n",
    "\n",
    "# Unite whole data in one dataframe\n",
    "merged_df = pd.concat([df_train_ready, df_valid_ready, df_test_ready], ignore_index=True)\n",
    "\n",
    "# Shuffle the merged dataframe\n",
    "shuffled_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Remove leading and trailing spaces in the \"Sentence\" column\n",
    "shuffled_df['Sentence'] = shuffled_df['Sentence'].str.strip()\n",
    "\n",
    "# First 100 rows for examples\n",
    "#shuffled_df = shuffled_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32502fac-70ca-4c2c-83be-a0f206d6b765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 2011, of State Hillary Clinton promised the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('In', 'ADP'), ('2011', 'NUM'), (',', 'PUNCT'...</td>\n",
       "      <td>[('2011', 'DATE'), ('State', 'ORG'), ('Hillary...</td>\n",
       "      <td>[('In', 2, 'case'), ('2011', 8, 'obl'), (',', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>’ Today saw the debut of ”,” a new social expe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('’', 'PUNCT'), ('Today', 'NOUN'), ('saw', 'V...</td>\n",
       "      <td>[('Twitch', 'PRODUCT')]</td>\n",
       "      <td>[('’', 3, 'punct'), ('Today', 3, 'nsubj'), ('s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer, actor and activist Harry Belafonte, 88...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Singer', 'NOUN'), (',', 'PUNCT'), ('actor',...</td>\n",
       "      <td>[('Harry Belafonte', 'PERSON'), ('88', 'DATE')...</td>\n",
       "      <td>[('Singer', 12, 'nsubj'), (',', 3, 'punct'), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last week, Roberts said the teams medical staf...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Last', 'ADJ'), ('week', 'NOUN'), (',', 'PUN...</td>\n",
       "      <td>[('Last week', 'DATE'), ('Roberts', 'PERSON'),...</td>\n",
       "      <td>[('Last', 2, 'amod'), ('week', 5, 'obl:tmod'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indeed, I believe that Waitrose does more than...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Indeed', 'ADV'), (',', 'PUNCT'), ('I', 'PRO...</td>\n",
       "      <td>[('Waitrose', 'ORG'), ('UK', 'GPE')]</td>\n",
       "      <td>[('Indeed', 4, 'advmod'), (',', 4, 'punct'), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0  In 2011, of State Hillary Clinton promised the...      0   \n",
       "1  ’ Today saw the debut of ”,” a new social expe...      0   \n",
       "2  Singer, actor and activist Harry Belafonte, 88...      1   \n",
       "3  Last week, Roberts said the teams medical staf...      1   \n",
       "4  Indeed, I believe that Waitrose does more than...      1   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [('In', 'ADP'), ('2011', 'NUM'), (',', 'PUNCT'...   \n",
       "1  [('’', 'PUNCT'), ('Today', 'NOUN'), ('saw', 'V...   \n",
       "2  [('Singer', 'NOUN'), (',', 'PUNCT'), ('actor',...   \n",
       "3  [('Last', 'ADJ'), ('week', 'NOUN'), (',', 'PUN...   \n",
       "4  [('Indeed', 'ADV'), (',', 'PUNCT'), ('I', 'PRO...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [('2011', 'DATE'), ('State', 'ORG'), ('Hillary...   \n",
       "1                            [('Twitch', 'PRODUCT')]   \n",
       "2  [('Harry Belafonte', 'PERSON'), ('88', 'DATE')...   \n",
       "3  [('Last week', 'DATE'), ('Roberts', 'PERSON'),...   \n",
       "4               [('Waitrose', 'ORG'), ('UK', 'GPE')]   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [('In', 2, 'case'), ('2011', 8, 'obl'), (',', ...  \n",
       "1  [('’', 3, 'punct'), ('Today', 3, 'nsubj'), ('s...  \n",
       "2  [('Singer', 12, 'nsubj'), (',', 3, 'punct'), (...  \n",
       "3  [('Last', 2, 'amod'), ('week', 5, 'obl:tmod'),...  \n",
       "4  [('Indeed', 4, 'advmod'), (',', 4, 'punct'), (...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab333b5-5052-4dc7-aa98-b229e3cd04e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6c6add-31b2-4365-82c7-983097d03a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    3390\n",
       "3    1631\n",
       "1     814\n",
       "4     237\n",
       "2     155\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade7978-08a0-4c84-be4c-be6ed20f1d04",
   "metadata": {},
   "source": [
    "# Mapping Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aaba528-c606-4797-aeee-52437715a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary: 0 - neutral, 1 - positive, 2 - negative\n",
    "label_mapping = {2: 1, 3: 2, 4: 2}\n",
    "shuffled_df['Label'] = shuffled_df['Label'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ce2526-47fa-4485-8d66-1e9810305f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>entities</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 2011, of State Hillary Clinton promised the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('In', 'ADP'), ('2011', 'NUM'), (',', 'PUNCT'...</td>\n",
       "      <td>[('2011', 'DATE'), ('State', 'ORG'), ('Hillary...</td>\n",
       "      <td>[('In', 2, 'case'), ('2011', 8, 'obl'), (',', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>’ Today saw the debut of ”,” a new social expe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[('’', 'PUNCT'), ('Today', 'NOUN'), ('saw', 'V...</td>\n",
       "      <td>[('Twitch', 'PRODUCT')]</td>\n",
       "      <td>[('’', 3, 'punct'), ('Today', 3, 'nsubj'), ('s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer, actor and activist Harry Belafonte, 88...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Singer', 'NOUN'), (',', 'PUNCT'), ('actor',...</td>\n",
       "      <td>[('Harry Belafonte', 'PERSON'), ('88', 'DATE')...</td>\n",
       "      <td>[('Singer', 12, 'nsubj'), (',', 3, 'punct'), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last week, Roberts said the teams medical staf...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Last', 'ADJ'), ('week', 'NOUN'), (',', 'PUN...</td>\n",
       "      <td>[('Last week', 'DATE'), ('Roberts', 'PERSON'),...</td>\n",
       "      <td>[('Last', 2, 'amod'), ('week', 5, 'obl:tmod'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indeed, I believe that Waitrose does more than...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('Indeed', 'ADV'), (',', 'PUNCT'), ('I', 'PRO...</td>\n",
       "      <td>[('Waitrose', 'ORG'), ('UK', 'GPE')]</td>\n",
       "      <td>[('Indeed', 4, 'advmod'), (',', 4, 'punct'), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0  In 2011, of State Hillary Clinton promised the...      0   \n",
       "1  ’ Today saw the debut of ”,” a new social expe...      0   \n",
       "2  Singer, actor and activist Harry Belafonte, 88...      1   \n",
       "3  Last week, Roberts said the teams medical staf...      1   \n",
       "4  Indeed, I believe that Waitrose does more than...      1   \n",
       "\n",
       "                                          tokens_pos  \\\n",
       "0  [('In', 'ADP'), ('2011', 'NUM'), (',', 'PUNCT'...   \n",
       "1  [('’', 'PUNCT'), ('Today', 'NOUN'), ('saw', 'V...   \n",
       "2  [('Singer', 'NOUN'), (',', 'PUNCT'), ('actor',...   \n",
       "3  [('Last', 'ADJ'), ('week', 'NOUN'), (',', 'PUN...   \n",
       "4  [('Indeed', 'ADV'), (',', 'PUNCT'), ('I', 'PRO...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [('2011', 'DATE'), ('State', 'ORG'), ('Hillary...   \n",
       "1                            [('Twitch', 'PRODUCT')]   \n",
       "2  [('Harry Belafonte', 'PERSON'), ('88', 'DATE')...   \n",
       "3  [('Last week', 'DATE'), ('Roberts', 'PERSON'),...   \n",
       "4               [('Waitrose', 'ORG'), ('UK', 'GPE')]   \n",
       "\n",
       "                                        dependencies  \n",
       "0  [('In', 2, 'case'), ('2011', 8, 'obl'), (',', ...  \n",
       "1  [('’', 3, 'punct'), ('Today', 3, 'nsubj'), ('s...  \n",
       "2  [('Singer', 12, 'nsubj'), (',', 3, 'punct'), (...  \n",
       "3  [('Last', 2, 'amod'), ('week', 5, 'obl:tmod'),...  \n",
       "4  [('Indeed', 4, 'advmod'), (',', 4, 'punct'), (...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af8d5ed-9357-4ded-970e-eed179b2ebef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e5a722-13c5-4d28-9e66-6713f70950f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    3390\n",
       "2    1868\n",
       "1     969\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a1cf9-5a57-493a-8c3a-7802f92ffba6",
   "metadata": {},
   "source": [
    "# Turning strings back to lists and tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e79989-3230-4c6b-ba9c-17d47543c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(dependencies_str):\n",
    "    # Check if it's a string and if it appears to be in the list of tuples format\n",
    "    if isinstance(dependencies_str, str) and dependencies_str.startswith(\"[\") and dependencies_str.endswith(\"]\"):\n",
    "        try:\n",
    "            # Convert string representation of list back to actual list of tuples\n",
    "            return ast.literal_eval(dependencies_str)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Error parsing: {dependencies_str}\")\n",
    "            raise e\n",
    "    elif isinstance(dependencies_str, list):\n",
    "        # If it's already a list, return as is\n",
    "        return dependencies_str\n",
    "    else:\n",
    "        # If it's another unexpected type, return as is or handle appropriately\n",
    "        return dependencies_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e53857-0db5-4237-b8c7-e742c1fd955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your datasets\n",
    "shuffled_df['dependencies'] = shuffled_df['dependencies'].apply(convert_to_list)\n",
    "shuffled_df['tokens_pos'] = shuffled_df['tokens_pos'].apply(convert_to_list)\n",
    "shuffled_df['entities'] = shuffled_df['entities'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5898c-ac22-4229-9a87-3a89b6c3912e",
   "metadata": {},
   "source": [
    "# Following the Modified Algorithm of Blame/Praise Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "850365df-dec0-4b8f-a92e-6631d0347707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to check if a verb belongs to Foreseeability or Coercion groups\n",
    "\n",
    "def is_foreseeability_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of foreseeability-related verb classes.\n",
    "    foreseeability_classes = {'communication', 'creation', 'consumption', 'competition', 'possession', 'motion'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lexname = synset.lexname().split('.')[1]  # Extracts the lexical category (i.e., type of action)\n",
    "        if lexname in foreseeability_classes:  # Checks if the lexical category is in the foreseeability class\n",
    "            return True  # Returns True if the verb matches any foreseeability category\n",
    "    return False  # If no match is found, returns False\n",
    "\n",
    "\n",
    "def is_coercion_verb(verb):\n",
    "    # This function checks whether a verb belongs to a predefined set of coercion-related VerbNet classes.\n",
    "    coercion_classes = {'urge-58.1', 'force-59', 'forbid-67'}\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)  # Fetches all verb synsets for the word\n",
    "    for synset in synsets:\n",
    "        lemma = synset.lemmas()[0]  # Gets the first lemma for each synset\n",
    "        vn_classes = lemma.key().split('%')[0]  # Extracts the lemma key\n",
    "        vn_class_ids = vn.classids(vn_classes)  # Fetches the VerbNet classes for the lemma\n",
    "        if any(vn_class in coercion_classes for vn_class in vn_class_ids):  # Checks for a match in coercion classes\n",
    "            return True  # If a match is found in coercion classes, return True\n",
    "    return False  # If no match is found, return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c70245b-fa64-4e29-ae20-84f7f8999ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_verb(word, tokens_pos):\n",
    "    \"\"\"\n",
    "    Check if the given word is a verb and passes the foreseeability and coercion checks.\n",
    "    \"\"\"\n",
    "    # Check if the word is a verb using tokens_pos\n",
    "    for token, pos in tokens_pos:\n",
    "        if token == word and 'VERB' in pos:  # Ensure the word is tagged as a verb\n",
    "            # Now check if it passes foreseeability and coercion checks\n",
    "            if is_foreseeability_verb(word) and not is_coercion_verb(word):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_conjunctions(verb_type_verbs, related_word, counter_j, index, dependencies, tokens_pos):\n",
    "    \"\"\"\n",
    "    Helper function to check and append conjunctions for specific verb types (xcomp, ccomp, parataxis, advcl).\n",
    "    It modifies the original verb list (e.g., xcomp_verbs, ccomp_verbs) by adding conjunctions directly.\n",
    "    \"\"\"\n",
    "    counter_x = 0\n",
    "    for conj in dependencies:\n",
    "        if len(conj) == 3:\n",
    "            conj_word, conj_head, conj_rel = conj\n",
    "            counter_x += 1\n",
    "            # Reset counter for punctuation after root - end of the sentence\n",
    "            if conj_rel == 'punct' and (conj_word == \".\" or conj_word == \":\"):\n",
    "                counter_x = 0\n",
    "            # Check if the conj word is a valid verb related to the current relation\n",
    "            if conj_head == counter_j and conj_rel == 'conj' and is_valid_verb(related_word, tokens_pos):\n",
    "                verb_type_verbs.append((conj_word, counter_x, index))  # Append conjunction to the respective verb list\n",
    "\n",
    "\n",
    "def handle_related_verbs(related_rel, related_word, counter_j, index, dependencies, tokens_pos, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs):\n",
    "    \"\"\"\n",
    "    Helper function to handle related verbs (xcomp, ccomp, parataxis, advcl).\n",
    "    Depending on the relation type, it adds the verb to the appropriate list and handles its conjunctions.\n",
    "    \"\"\"\n",
    "    if related_rel == 'xcomp':\n",
    "        xcomp_verbs.append((related_word, counter_j, index))  # xcomp relation to root\n",
    "        check_conjunctions(xcomp_verbs, related_word, counter_j, index, dependencies, tokens_pos)\n",
    "\n",
    "    elif related_rel == 'ccomp':\n",
    "        ccomp_verbs.append((related_word, counter_j, index))  # ccomp relation\n",
    "        check_conjunctions(ccomp_verbs, related_word, counter_j, index, dependencies, tokens_pos)\n",
    "\n",
    "    elif related_rel == 'parataxis':\n",
    "        parataxis_verbs.append((related_word, counter_j, index))  # parataxis relation\n",
    "        check_conjunctions(parataxis_verbs, related_word, counter_j, index, dependencies, tokens_pos)\n",
    "\n",
    "    elif related_rel == 'advcl':\n",
    "        advcl_verbs.append((related_word, counter_j, index))  # advcl relation\n",
    "        check_conjunctions(advcl_verbs, related_word, counter_j, index, dependencies, tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ea980d-d800-4637-a05c-79f56cda0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_one_function(row):\n",
    "    \"\"\"\n",
    "    Main function to find valid verbs (root, xcomp, ccomp, parataxis, advcl, and their conjunctions).\n",
    "    \"\"\"\n",
    "    dependencies = row['dependencies']  # Dependency relations for the sentence\n",
    "    tokens_pos = row['tokens_pos']  # POS-tagged tokens for the sentence\n",
    "\n",
    "    counter_i = 0  # Counter for tracking the index of words in the dependency structure\n",
    "    \n",
    "    # Lists to store categorized verbs\n",
    "    roots = []  # (word, own index, main root), if root is root (not conj) - write its own index\n",
    "    root_verbs = []  # For valid root verbs (that pass foreseeability and coercion checks)\n",
    "    xcomp_verbs = []  # For valid xcomp verbs\n",
    "    ccomp_verbs = []  # For valid ccomp verbs\n",
    "    parataxis_verbs = []  # For valid parataxis verbs\n",
    "    advcl_verbs = []  # For valid advcl verbs\n",
    "\n",
    "    # Iterate through dependencies to identify roots and their related verbs\n",
    "    for dep in dependencies:\n",
    "        if len(dep) == 3:\n",
    "            word, head, deprel = dep  # Unpacking the dependency tuple (word, head, relation)\n",
    "            counter_i += 1  # Increment the index counter for this word\n",
    "\n",
    "            # Reset counter when punctuation is found after root\n",
    "            if roots:\n",
    "                if deprel == 'punct' and (word == \".\" or word == \":\") and head == roots[0][1]:\n",
    "                    counter_i = 0  \n",
    "\n",
    "            # Check if the current word is the root of the sentence\n",
    "            if deprel == 'root':\n",
    "                roots.append((word, counter_i, counter_i))  # Add the root verb and its index\n",
    "                if is_valid_verb(word, tokens_pos):  # Check if the root is a valid verb\n",
    "                    root_verbs.append((word, counter_i, counter_i))  # Append valid root verb\n",
    "\n",
    "                # Looking for related conjunctions\n",
    "                counter_j = 0\n",
    "                for related in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related\n",
    "                        counter_j += 1  # Increment index for related word\n",
    "                        # Reset the counter for punctuation after root\n",
    "                        if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                            counter_j = 0\n",
    "                        # Look for conjunctions attached to the root verb\n",
    "                        if related_head == counter_i and related_rel == 'conj' and is_valid_verb(related_word, tokens_pos):\n",
    "                            roots.append((related_word, counter_j, counter_i))  # Add root conj\n",
    "                            root_verbs.append((related_word, counter_j, counter_i))  # Append valid conj relation\n",
    "\n",
    "    # Find related verbs (xcomp, ccomp, etc.) for root verbs and their conjunctions\n",
    "    for verb in roots:\n",
    "        word, index, head_index = verb\n",
    "        counter_j = 0\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                counter_j += 1\n",
    "                # Reset the counter for punctuation after root\n",
    "                if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                    counter_j = 0\n",
    "                # Handle xcomp, ccomp, parataxis, and advcl relations\n",
    "                if related_head == index and related_rel in ['xcomp', 'ccomp', 'parataxis', 'advcl'] and is_valid_verb(related_word, tokens_pos):\n",
    "                    handle_related_verbs(related_rel, related_word, counter_j, index, dependencies, tokens_pos, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs)\n",
    "\n",
    "    #print()\n",
    "    #print('NEW ROW')\n",
    "    #print('Valid root verbs: ', root_verbs)\n",
    "    #print('Valid xcomp verbs: ', xcomp_verbs)\n",
    "    #print('Valid ccomp verbs: ', ccomp_verbs)\n",
    "    #print('Valid parataxis verbs: ', parataxis_verbs)\n",
    "    #print('Valid advcl verbs: ', advcl_verbs)\n",
    "    \n",
    "    return roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30dbe389-4669-4a18-9135-f9128335f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_agent_validity(related_word, row, tokens_pos):\n",
    "    \"\"\"\n",
    "    Function to check the validity of an agent based on NER tags and additional rules.\n",
    "    \"\"\"\n",
    "    entities = row['entities']\n",
    "    valid_ent_labels = [\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]\n",
    "    valid_additional_words = [\n",
    "        \"person\", \"man\", \"woman\", \"police\", \"administration\", \"immigrants\", \"president\", \"minister\", \"senator\", \n",
    "        \"representative\", \"governor\", \"mayor\", \"council\", \"secretary\", \"ambassador\", \"chancellor\", \"ministry\", \"monarchy\",\n",
    "        \"parliamentary\", \"mr.\", \"ms.\", \"mrs.\"\n",
    "    ]\n",
    "\n",
    "    self = False\n",
    "    agent_is_valid = False\n",
    "    \n",
    "    # Original logic: Check if the related_word is a valid agent based on NER and additional terms\n",
    "    for entity, label in entities: \n",
    "        if related_word in entity and label in valid_ent_labels:  \n",
    "            agent_is_valid = True  # Valid agent based on NER\n",
    "    \n",
    "    # Check if it's a pronoun\n",
    "    if not agent_is_valid and 'PRON' in [pos for token, pos in tokens_pos if token == related_word]: \n",
    "        agent_is_valid = True  \n",
    "        # Logic for handling \"self\" reference (i.e., \"I\" or \"we\") - to be changed\n",
    "        #if related_word.lower() == \"i\" or related_word.lower() == \"we\":\n",
    "            #self = True\n",
    "\n",
    "    # Check if the word is in additional valid agent words\n",
    "    if not agent_is_valid and related_word.lower() in valid_additional_words:\n",
    "        agent_is_valid = True  \n",
    "    #if agent_is_valid:\n",
    "        #print(\"The Agent is valid: \", related_word)\n",
    "    \n",
    "    return agent_is_valid, self\n",
    "\n",
    "\n",
    "def check_causative_verb(verb):\n",
    "    \"\"\"\n",
    "    Function to check if the verb is causative, i.e., if it belongs to the 'cause' or 'CAUSETO' class.\n",
    "    \"\"\"\n",
    "    for synset in wn.synsets(verb, pos=wn.VERB):\n",
    "        if 'cause' in synset.lemma_names():\n",
    "            #print(\"found cause lemma\")\n",
    "            return True\n",
    "        for lemma in synset.lemmas():\n",
    "            for frame in lemma.frame_strings():\n",
    "                if 'CAUSE' in frame or 'CAUSETO' in frame:\n",
    "                    #print(\"found cause and causeto frame strings\")\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def define_polarity(verb, obj):\n",
    "    \"\"\"\n",
    "    Function to define the polarity of the verb + object combination.\n",
    "    Maybe use Word Sense Disambiguation (WSD) here?\n",
    "    \"\"\"\n",
    "    context = f\"{verb} {obj}\"\n",
    "    verb_sense = lesk(context.split(), verb, 'v')\n",
    "    obj_sense = lesk(context.split(), obj, 'n')\n",
    "    \n",
    "    pos_score = neg_score = 0\n",
    "    \n",
    "    if verb_sense:\n",
    "        swn_verb = swn.senti_synset(verb_sense.name())\n",
    "        pos_score += swn_verb.pos_score()\n",
    "        neg_score += swn_verb.neg_score()\n",
    "    \n",
    "    if obj_sense:\n",
    "        swn_obj = swn.senti_synset(obj_sense.name())\n",
    "        pos_score += swn_obj.pos_score()\n",
    "        neg_score += swn_obj.neg_score()\n",
    "\n",
    "    afinn_score = afinn.score(context)\n",
    "    if afinn_score > 0:\n",
    "        pos_score += afinn_score\n",
    "    else:\n",
    "        neg_score += abs(afinn_score)\n",
    "\n",
    "    subj_pos = sum([1 for token in context.split() if token in opinion_lexicon.positive()])\n",
    "    subj_neg = sum([1 for token in context.split() if token in opinion_lexicon.negative()])\n",
    "    \n",
    "    pos_score += subj_pos\n",
    "    neg_score += subj_neg\n",
    "\n",
    "    return 1 if pos_score > neg_score else 2 if neg_score > pos_score else 0\n",
    "\n",
    "\n",
    "def adjust_sentiment_for_negation(row, polarity, verb):\n",
    "    \"\"\"\n",
    "    Function to adjust the sentiment polarity for negation.\n",
    "    \"\"\"\n",
    "    word, index, head_index = verb\n",
    "    dependencies = row['dependencies']\n",
    "\n",
    "    for related in dependencies:\n",
    "        if len(related) == 3:\n",
    "            related_word, related_head, related_rel = related\n",
    "            if related_head == index and related_rel in ['advmod'] and (related_word == 'not' or related_word == 'n’t' or related_word == 'never'):\n",
    "                #print(\"Had found Negation\")\n",
    "                if polarity == 1:\n",
    "                    polarity = 2\n",
    "                    #print(\"Final Polarity: \", polarity)\n",
    "                    return polarity\n",
    "                if polarity == 2:\n",
    "                    polarity = 1\n",
    "                    #print(\"Final Polarity: \", polarity)\n",
    "                    return polarity\n",
    "\n",
    "    #print(\"Final Polarity: \", polarity)\n",
    "    \n",
    "    return polarity\n",
    "\n",
    "\n",
    "\n",
    "def handle_special_cases_for_xcomp_in_ccomp(row, verb, dependencies, tokens_pos, counter_j, related_word):\n",
    "    \"\"\"\n",
    "    Handle special cases for xcomp connected to ccomp, looking for objects connected to xcomp.\n",
    "    \n",
    "    for related_to_xcomp in dependencies:\n",
    "        if len(related_to_xcomp) == 3:\n",
    "            related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "            if related_to_xcomp_head == counter_j and related_to_xcomp_rel in ['obj']:\n",
    "                print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                # Define polarity of the combination xcomp + object\n",
    "                polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                polarity = adjust_sentiment_for_negation(row, polarity, related_to_xcomp)\n",
    "                if polarity != 0:\n",
    "                    return polarity\n",
    "    for related_to_xcomp in dependencies:\n",
    "        if len(related_to_xcomp) == 3:\n",
    "            related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "            if related_to_xcomp_head == counter_j and related_to_xcomp_rel in ['iobj']:\n",
    "                print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                # Define polarity of the combination xcomp + object\n",
    "                polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                polarity = adjust_sentiment_for_negation(row, polarity, related_to_xcomp)\n",
    "                if polarity != 0:\n",
    "                    return polarity\n",
    "    for related_to_xcomp in dependencies:\n",
    "        if len(related_to_xcomp) == 3:\n",
    "            related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "            if related_to_xcomp_head == counter_j and related_to_xcomp_rel in ['obl']:\n",
    "                print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                # Define polarity of the combination xcomp + object\n",
    "                polarity = define_polarity(related_word, related_to_xcomp_word)\n",
    "                polarity = adjust_sentiment_for_negation(row, polarity, related_to_xcomp)\n",
    "                if polarity != 0:\n",
    "                    return polarity\n",
    "\n",
    "    \"\"\"\n",
    "    for i, related in enumerate(dependencies):\n",
    "        if len(related) == 3:\n",
    "            related_word, related_head, related_rel = related\n",
    "            if related_head == verb[1] and related_rel in ['xcomp']:\n",
    "                #print(\"Found xcomp to ccomp: \", i, related_word, related_head, related_rel)\n",
    "                for related_to_xcomp in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "                        # Maintain order of processing 'obj', 'iobj', and 'obl'\n",
    "                        if related_to_xcomp_head == (i+1) and related_to_xcomp_rel in ['obj']:\n",
    "                            polarity = define_polarity(verb[0], related_to_xcomp_word)\n",
    "                            #print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                            return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                for related_to_xcomp in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_word, related_head, related_rel = related_to_xcomp\n",
    "                        if related_to_xcomp_head == (i+1) and related_to_xcomp_rel in ['iobj']:\n",
    "                            polarity = define_polarity(verb[0], related_to_xcomp_word)\n",
    "                            #print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                            return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                for related_to_xcomp in dependencies:\n",
    "                    if len(related) == 3:\n",
    "                        related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "                        if related_to_xcomp_head == (i+1) and related_to_xcomp_rel in ['obl']:\n",
    "                            polarity = define_polarity(verb[0], related_to_xcomp_word)\n",
    "                            #print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                            return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_ccomp_verb(row, verb, dependencies, tokens_pos, roots):\n",
    "    \"\"\"\n",
    "    Process ccomp verbs and handle normal cases and special cases like `obl:agent` and `nsubj:pass`.\n",
    "    \"\"\"\n",
    "    word, index, head_index = verb\n",
    "    agent_is_valid, self = False, False\n",
    "    agent_is_obl = False  # Track if the agent comes from an `obl:agent`\n",
    "\n",
    "    # Find an agent connected to the ccomp verb (normal or obl:agent case)\n",
    "    for related in dependencies:\n",
    "        if len(related) == 3:\n",
    "            related_word, related_head, related_rel = related\n",
    "            # Check for `nsubj` as agent\n",
    "            if related_head == index and related_rel in ['nsubj']:\n",
    "                agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "\n",
    "            # Special case: `obl:agent` becomes the agent\n",
    "            elif related_head == index and related_rel in ['obl:agent', 'obl']:\n",
    "                agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                agent_is_obl = True  # Mark that the agent is an `obl`\n",
    "\n",
    "    # If no valid agent, check for causative verbs\n",
    "    #if not agent_is_valid and check_causative_verb(word):\n",
    "        #agent_is_valid = True\n",
    "\n",
    "    # Object processing priority: `obj`, `iobj`\n",
    "    if agent_is_valid:\n",
    "        counter_j = 0\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                counter_j += 1\n",
    "                # Reset counter for punctuation\n",
    "                if related_rel == 'punct' and (related_word == \".\" or related_word == \":\") and related_head == roots[0][1]:\n",
    "                    counter_j = 0\n",
    "                \n",
    "                # Normal case: Handle objects connected to the ccomp verb\n",
    "                if related_head == index and related_rel in ['obj', 'iobj'] and not agent_is_obl:\n",
    "                    #print(\"The Object is valid: \", related_word)\n",
    "                    polarity = define_polarity(word, related_word)\n",
    "                    polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                    if polarity != 0:\n",
    "                        return f\"self - {polarity}\" if self else polarity\n",
    "\n",
    "                # Special case: When `obl:agent` is present, `nsubj:pass` becomes the object\n",
    "                if agent_is_obl and related_head == index and related_rel == 'nsubj:pass':\n",
    "                    #print(\"The Object is valid: \", related_word)\n",
    "                    polarity = define_polarity(word, related_word)\n",
    "                    polarity = adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                    if polarity != 0:\n",
    "                        return f\"self - {polarity}\" if self else polarity\n",
    "\n",
    "                # Handle xcomp connected to ccomp and check objects within xcomp\n",
    "                polarity = handle_special_cases_for_xcomp_in_ccomp(row, verb, dependencies, tokens_pos, counter_j, related_word)\n",
    "                if polarity != 0:\n",
    "                    return f\"self - {polarity}\" if self else polarity\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def find_object_and_define_polarity(row, verb, agent_is_valid, tokens_pos):\n",
    "    \"\"\"\n",
    "    Helper function to find the object for a given verb and define its polarity.\n",
    "    \"\"\"\n",
    "    if agent_is_valid:\n",
    "        dependencies = row['dependencies']\n",
    "\n",
    "        # xcomp case\n",
    "        for i, related in enumerate(dependencies):\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                if related_head == verb[1] and related_rel in ['xcomp']:\n",
    "                    #print(\"Found xcomp to ccomp: \", i, related_word, related_head, related_rel)\n",
    "                    for related_to_xcomp in dependencies:\n",
    "                        if len(related) == 3:\n",
    "                            related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "                            # Maintain order of processing 'obj', 'iobj', and 'obl'\n",
    "                            if related_to_xcomp_head == (i+1) and related_to_xcomp_rel in ['obj']:\n",
    "                                polarity = define_polarity(verb[0], related_to_xcomp_word)\n",
    "                                #print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                                return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                    for related_to_xcomp in dependencies:\n",
    "                        if len(related) == 3:\n",
    "                            related_word, related_head, related_rel = related_to_xcomp\n",
    "                            if related_to_xcomp_head == (i+1) and related_to_xcomp_rel in ['iobj']:\n",
    "                                polarity = define_polarity(verb[0], related_to_xcomp_word)\n",
    "                                #print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                                return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                    for related_to_xcomp in dependencies:\n",
    "                        if len(related) == 3:\n",
    "                            related_to_xcomp_word, related_to_xcomp_head, related_to_xcomp_rel = related_to_xcomp\n",
    "                            if related_to_xcomp_head == (i+1) and related_to_xcomp_rel in ['obl']:\n",
    "                                polarity = define_polarity(verb[0], related_to_xcomp_word)\n",
    "                                #print(\"The Object is valid: \", related_to_xcomp_word)\n",
    "                                return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "        \n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                # Maintain order of processing 'obj', 'iobj', and 'obl'\n",
    "                if related_head == verb[1] and related_rel in ['obj']:\n",
    "                    polarity = define_polarity(verb[0], related_word)\n",
    "                    #print(\"The Object is valid: \", related_word)\n",
    "                    return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                if related_head == verb[1] and related_rel in ['iobj']:\n",
    "                    polarity = define_polarity(verb[0], related_word)\n",
    "                    #print(\"The Object is valid: \", related_word)\n",
    "                    return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "        for related in dependencies:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                if related_head == verb[1] and related_rel in ['obl']:\n",
    "                    polarity = define_polarity(verb[0], related_word)\n",
    "                    #print(\"The Object is valid: \", related_word)\n",
    "                    return adjust_sentiment_for_negation(row, polarity, verb)\n",
    "                    \n",
    "        # Handle objects connected to xcomp\n",
    "        #polarity = handle_special_cases_for_xcomp_in_ccomp(row, verb, dependencies, tokens_pos, counter_j, related_word)\n",
    "        #if polarity != 0:\n",
    "            #return f\"self - {polarity}\" if self else polarity\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def process_verb_connections(row, verbs, tokens_pos, self=False):\n",
    "    \"\"\"\n",
    "    Generalized function to process verb connections such as root_verbs, xcomp_verbs, etc.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    for verb in verbs:\n",
    "        word, index, head_index = verb\n",
    "        agent_is_valid = False\n",
    "\n",
    "        # Original logic for agent validation\n",
    "        for related in row['dependencies']:\n",
    "            if len(related) == 3:\n",
    "                related_word, related_head, related_rel = related\n",
    "                #if related_head == index and related_rel in ['nsubj', 'nsubj:pass']:\n",
    "                if related_head == index and related_rel in ['nsubj']:\n",
    "                    agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "                else:\n",
    "                    # Checking if the agent of the root of that verb is valid\n",
    "                    if related_head == head_index and related_rel in ['nsubj']: \n",
    "                        agent_is_valid, self = check_agent_validity(related_word, row, tokens_pos)\n",
    "        \n",
    "        # If agent is not valid, check causative verb\n",
    "        #if not agent_is_valid and check_causative_verb(word):\n",
    "            #agent_is_valid = True\n",
    "\n",
    "        # Use original priority order for object detection\n",
    "        polarity = find_object_and_define_polarity(row, verb, agent_is_valid, tokens_pos)\n",
    "        if polarity != 0:\n",
    "            #return f\"self - {polarity}\" if self else polarity, verb\n",
    "            return polarity, verb\n",
    "\n",
    "    return result, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01c3852-ef74-496c-aeb5-95f01c929ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs):\n",
    "    \"\"\"\n",
    "    Main function to decide on Agent Causality, find the object, decide on Polarity, and classify the row.\n",
    "    \"\"\"\n",
    "    tokens_pos = row['tokens_pos']\n",
    "    dependencies = row['dependencies']\n",
    "\n",
    "    # Process root verbs\n",
    "    result, verb = process_verb_connections(row, root_verbs, tokens_pos)\n",
    "    if result:\n",
    "        if result == 1 or result == 2:\n",
    "            return result, verb\n",
    "        return result, None\n",
    "\n",
    "    # Process ccomp verbs with priority handling and special cases\n",
    "    for verb in ccomp_verbs:\n",
    "        result, verb = process_ccomp_verb(row, verb, dependencies, tokens_pos, roots)\n",
    "        if result:\n",
    "            if result == 1 or result == 2:\n",
    "                return result, verb\n",
    "            return result, None\n",
    "\n",
    "    # Process advcl verbs\n",
    "    result, verb = process_verb_connections(row, advcl_verbs, tokens_pos)\n",
    "    if result:\n",
    "        if result == 1 or result == 2:\n",
    "            return result, verb\n",
    "        return result, None\n",
    "        \n",
    "    # Process parataxis verbs\n",
    "    result, verb = process_verb_connections(row, parataxis_verbs, tokens_pos)\n",
    "    if result:\n",
    "        if result == 1 or result == 2:\n",
    "            return result, verb\n",
    "        return result, None\n",
    "    \n",
    "    # Process xcomp verbs\n",
    "    result, verb = process_verb_connections(row, xcomp_verbs, tokens_pos)\n",
    "    if result:\n",
    "        if result == 1 or result == 2:\n",
    "            return result, verb\n",
    "        return result, None\n",
    "\n",
    "    return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e4e0cf-d699-42a2-968b-063783d3a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_the_row(row):\n",
    "    result = 0\n",
    "    # This is the main function to process each row of data and classify the row \n",
    "\n",
    "    # 1 - Find all the related verbs in categories in dependency column 'root', 'xcomp', 'ccomp', 'parataxis', 'advcl', 'conj' (is a verb check - foreseeability check - coercion check)\n",
    "    roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs = step_one_function(row)\n",
    "    \n",
    "    # 2 - If at least one of the lists is not empty - can proceed\n",
    "    if root_verbs or xcomp_verbs or ccomp_verbs or parataxis_verbs or advcl_verbs:\n",
    "        \n",
    "        # 3 - Take a final decision about the label (0 - others, 1 - positive, 2 - negative)\n",
    "        return step_two_function(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs)\n",
    "    \n",
    "    else:\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33375c99-60a7-4f95-a5d1-87d16c49c0e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the function to the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m shuffled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal_Result\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent_Verb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mshuffled_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_the_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m shuffled_df \u001b[38;5;241m=\u001b[39m shuffled_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal_Result\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent_Verb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m shuffled_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal_Result\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent_Verb\u001b[39m\u001b[38;5;124m'\u001b[39m]]]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Rule_Based_Classifier\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mlabel_the_row\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 2 - If at least one of the lists is not empty - can proceed\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m root_verbs \u001b[38;5;129;01mor\u001b[39;00m xcomp_verbs \u001b[38;5;129;01mor\u001b[39;00m ccomp_verbs \u001b[38;5;129;01mor\u001b[39;00m parataxis_verbs \u001b[38;5;129;01mor\u001b[39;00m advcl_verbs:\n\u001b[0;32m     10\u001b[0m     \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# 3 - Take a final decision about the label (0 - others, 1 - positive, 2 - negative)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_two_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_verbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxcomp_verbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mccomp_verbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparataxis_verbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvcl_verbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mstep_two_function\u001b[1;34m(row, roots, root_verbs, xcomp_verbs, ccomp_verbs, parataxis_verbs, advcl_verbs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Process ccomp verbs with priority handling and special cases\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m verb \u001b[38;5;129;01min\u001b[39;00m ccomp_verbs:\n\u001b[1;32m---> 17\u001b[0m     result, verb \u001b[38;5;241m=\u001b[39m process_ccomp_verb(row, verb, dependencies, tokens_pos, roots)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "# Apply the function to the dataset\n",
    "shuffled_df['Final_Result'], shuffled_df['Event_Verb'] = shuffled_df.apply(label_the_row, axis=1)\n",
    "shuffled_df = shuffled_df[['Sentence', 'Label', 'Final_Result', 'Event_Verb'] + [col for col in shuffled_df.columns if col not in ['Sentence', 'Label', 'Final_Result', 'Event_Verb']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19448ab3-75b8-4765-81f4-a7faf19cea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915deb00-52fe-4572-b9e7-5a7f68e35cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df['Final_Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80232b5-0c37-466e-9c42-8022250cea6e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a296a3-fa30-49fa-a7f1-701772bc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels and predicted labels\n",
    "y_true = shuffled_df['Label']\n",
    "y_pred = shuffled_df['Final_Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b674ec-f8c7-4c94-a771-73660a1670f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame with 'Label' as true labels and 'Final_Result' as predicted labels\n",
    "\n",
    "# Calculate F1 Scores\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Calculate Precision and Recall for completeness (optional)\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Micro-average': [f1_micro, precision_micro, recall_micro],\n",
    "    'Macro-average': [f1_macro, precision_macro, recall_macro],\n",
    "    'Weighted-average': [f1_weighted, None, None]  # Weighted average only applicable to F1 score here\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# You can also use classification report to see more detailed metrics\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d673a-c154-41a3-ad5d-226a369954f0",
   "metadata": {},
   "source": [
    "# Export in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b8147-936a-466b-8c3d-22810ed6440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the first 300 rows to an Excel file\n",
    "#shuffled_df.head(100).to_excel('shuffled_df_new_100_rows.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9537f-183b-4efd-90a3-5c31377c9269",
   "metadata": {},
   "source": [
    "# False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444cb0d-b247-4603-8ff4-687c7c724c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3384a-2c8e-460e-b6d7-5e53d0dc53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive: Label is 0, but Final_Result is 1 or 2\n",
    "false_positives = shuffled_df[(shuffled_df['Label'] == 0) & (shuffled_df['Final_Result'].isin([1, 2]))]\n",
    "\n",
    "# Count the number of false positives\n",
    "false_positive_count = false_positives.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321d471-db18-4388-804c-147161e39d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af30fc-adac-4ce5-b631-060dcc338294",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb3022-b7ac-49e0-9391-46da61dfc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_count/shuffled_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850743e1-dac8-48f5-8e1a-e6888ac5a448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6aa51dcb-9bd7-4adc-af2d-90655ed7b59d",
    "5b4b9a5d-f5f1-4b41-89b1-ef0f7ec9269b",
    "XgpcdWkBTA2i"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0489b43fc7e64734882843d7d1dbccce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d91f88d2bd41c2911b27882b1bc3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fb17667761c4591ab2da8e3f71fa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9aa0c7fa734470bfc3feed6592d3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6835ae446b484d3ca602ec2f617aaff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0dc4784cf42446fb83ce5e6f1162d21",
      "placeholder": "​",
      "style": "IPY_MODEL_04d91f88d2bd41c2911b27882b1bc3c4",
      "value": " 386k/? [00:00&lt;00:00, 11.6MB/s]"
     }
    },
    "9ddd39e4df4f422d894bd00d63808d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9aa0c7fa734470bfc3feed6592d3bc",
      "placeholder": "​",
      "style": "IPY_MODEL_d29596beefdc42bea19fafd84f93dadb",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
     }
    },
    "a0dc4784cf42446fb83ce5e6f1162d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae273c49ff2a4099804ec2402c4e2d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee8df7ec2544bd89bdfbdb36a75191f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ddd39e4df4f422d894bd00d63808d90",
       "IPY_MODEL_b74d16cecf6e4c81b3ffd3df6be7845b",
       "IPY_MODEL_6835ae446b484d3ca602ec2f617aaff4"
      ],
      "layout": "IPY_MODEL_ae273c49ff2a4099804ec2402c4e2d9a"
     }
    },
    "b74d16cecf6e4c81b3ffd3df6be7845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0489b43fc7e64734882843d7d1dbccce",
      "max": 47900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fb17667761c4591ab2da8e3f71fa0bd",
      "value": 47900
     }
    },
    "d29596beefdc42bea19fafd84f93dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e978e9-b82c-4e8e-94ed-4817cfa4240a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'directed_sentiment_analysis'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/bywords/directed_sentiment_analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f232c-287f-4b71-97ea-3dcdd4448602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168bd24-c04a-4d26-8407-1749a1ab974f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1d609-19b7-4efc-b091-cf22867ffd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_sentiment_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521c4d0-6d0d-4c55-aab7-35ecc459b3f8",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d252f4e7-e7ed-4e7b-b3f1-809334313152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Transfor_Based_Classifier\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, logging, random, time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from researchers_code.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67587f32-7f46-48e5-8bab-5296f13dc519",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133a1bf8-f213-415e-ab5a-715b4980190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"Sentence\", \"Ent_1\", \"Ent_2\", \"Label\"]\n",
    "\n",
    "# Define file paths\n",
    "test_file = 'datasets/test_with_scobes.txt'\n",
    "valid_file = 'datasets/valid_with_scobes.txt'\n",
    "train_file = 'datasets/train_with_scobes.txt'\n",
    "\n",
    "# Load the datasets\n",
    "test_data = pd.read_csv(test_file, sep='\\t', names=column_names)\n",
    "valid_data = pd.read_csv(valid_file, sep='\\t', names=column_names)\n",
    "train_data = pd.read_csv(train_file, sep='\\t', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d111f8b6-4d05-438d-9756-f7b6dcfdb1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Ent_1</th>\n",
       "      <th>Ent_2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Bush II] — released a statement opposing [Don...</td>\n",
       "      <td>Bush II</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after the [Dallas] murders — by blaming “syste...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>White Americans</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Zappos] CEO [Tony Hsieh] has been praised for...</td>\n",
       "      <td>Zappos</td>\n",
       "      <td>Tony Hsieh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But [Shaffer] and other experts faulted [Clint...</td>\n",
       "      <td>Shaffer</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kulitta, music composition software written by...</td>\n",
       "      <td>Donya Quick</td>\n",
       "      <td>Johann Sebastian Bach</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence        Ent_1  \\\n",
       "0  [Bush II] — released a statement opposing [Don...      Bush II   \n",
       "1  after the [Dallas] murders — by blaming “syste...       Dallas   \n",
       "2  [Zappos] CEO [Tony Hsieh] has been praised for...       Zappos   \n",
       "3  But [Shaffer] and other experts faulted [Clint...      Shaffer   \n",
       "4  Kulitta, music composition software written by...  Donya Quick   \n",
       "\n",
       "                   Ent_2  Label  \n",
       "0           Donald Trump      3  \n",
       "1        White Americans      3  \n",
       "2             Tony Hsieh      1  \n",
       "3                Clinton      3  \n",
       "4  Johann Sebastian Bach      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26378c-cd6d-4ddb-a7e2-4c9951c4ff0f",
   "metadata": {},
   "source": [
    "There are five **labels:**\n",
    "- 0 - neutral\n",
    "- 1 - positive (p → q)\n",
    "- 2 - positive (q → p)\n",
    "- 3 - negative (p → q)\n",
    "- 4 - negative (q → p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24eefb3-d322-423d-8902-39294ed79645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496144a-c346-405b-9a94-5622cafde6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da89f455-b322-4bbe-937c-5f003adc0bc0",
   "metadata": {},
   "source": [
    "# Applying the Model DSE2QA of the Researchers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4355fc-d668-4c51-abba-43e67e2b65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Transfor_Based_Classifier\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Anastasiia Belkina\\anaconda3\\envs\\Transfor_Based_Classifier\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m logger\u001b[38;5;241m.\u001b[39maddHandler(f_handler)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Call the main function with the args object\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m main(args, train_path, logger, exp_id)\n",
      "File \u001b[1;32m~\\MANNHEIM\\MASTER_THESIS_CODE\\Transformer-Based Classifier\\researchers_code\\dse2qa.py:147\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args, train_path, logger, exp_id)\u001b[0m\n\u001b[0;32m    145\u001b[0m train_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnt1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m--> 147\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m proces_line(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSent\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], args\u001b[38;5;241m.\u001b[39mpretrain_type, args\u001b[38;5;241m.\u001b[39minput_type)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label_idx, (\u001b[38;5;28minput\u001b[39m, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, labels)):\n\u001b[0;32m    149\u001b[0m         train\u001b[38;5;241m.\u001b[39mappend((idx, label_idx, \u001b[38;5;28minput\u001b[39m, label))\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from researchers_code.dse2qa import main  # Correct import for main function\n",
    "\n",
    "# Simulate argparse by creating an Args class\n",
    "class Args:\n",
    "    def __init__(self, input_type, resample, max_epoch, batch_size, random_seed, pretrain_type, temperature):\n",
    "        self.input_type = input_type # Set input_type to (\"T(emplate)\" or \"P(seudo)\")\n",
    "        self.resample = resample # Set resample to (\"none\", \"up\", \"down\")\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.random_seed = random_seed\n",
    "        self.pretrain_type = pretrain_type  # Set pretrain_type to \"roberta\" or \"spanbert\"\n",
    "        self.temperature = temperature # needed otherwise doesnt work\n",
    "\n",
    "# Manually create an args object\n",
    "args = Args(input_type=\"T\", resample=\"none\", max_epoch=3, batch_size=40, random_seed=20180422, pretrain_type=\"roberta\", temperature = 1.0)  \n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(args.random_seed)\n",
    "random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "torch.cuda.manual_seed_all(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define the appropriate train_path based on the resample strategy\n",
    "if args.resample == \"none\":\n",
    "    train_path = \"dataset/train.txt\"\n",
    "elif args.resample == \"up\":\n",
    "    train_path = \"dataset/train_over.txt\"\n",
    "elif args.resample == \"down\":\n",
    "    train_path = \"dataset/train_under.txt\"\n",
    "else:\n",
    "    raise TypeError(\"Invalid resample option\")\n",
    "\n",
    "# Create a unique experiment ID based on parameters\n",
    "exp_id = f\"DSE2QA_{args.input_type}_{args.resample}_{args.max_epoch}_{args.batch_size}_{args.random_seed}\"\n",
    "\n",
    "# Ensure the 'out' directory exists\n",
    "os.makedirs(\"out\", exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(exp_id)\n",
    "f_handler = logging.FileHandler(f\"out/{exp_id}.txt\", mode=\"w\")\n",
    "f_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(f_handler)\n",
    "\n",
    "# Call the main function with the args object\n",
    "main(args, train_path, logger, exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b8993-b130-488e-b537-a10c9e7ffa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410500da-aab5-4abe-99b8-0a7400d0e462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ed586-566b-4e56-955b-9b1eb774a00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4797d7-0882-42c3-90b6-432dd798b7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303db45-3cad-4200-beb2-9270dfd9cbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
